
Metrics of MT Quality
Ondřej Bojar
February 20, 2020
NPFL087 Statistical Machine Translation
Charles University
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics unless otherwise stated
Course Outline
1. Metrics of MT Quality.
2. Approaches to MT. SMT, PBMT, NMT, NP-hardness.
3. NMT (Seq2seq, Attention. Transformer). Neural Monkey.
4. Parallel texts. Sentence and word alignment. hunalign, GIZA++.
5. PBMT: Phrase Extraction, Decoding, MERT. Moses.
6. Morphology in MT. Factors or segmenting, data or linguistics.
7. Syntax in SMT (constituency, dependency, deep).
8. Syntax in NMT (soft constraints/multitask, network structure).
9. Towards Understanding: Word and Sentence Representations.
10. Advanced: Multi-Lingual MT. Multi-Task Training. Chef’s Tricks.
11. Project presentations.
1/84
Outline
• Task of MT (formulating a simplified goal).
• Manual evaluation.
• Automatic evaluation.
• Empirical confidence bounds.
• End-to-end vs. component evaluation.
• Summary: Evaluation caveats.
2/84
Importance of Measuring MT Output
You need a metric to be able to check your progress.
An example from the history:
• Manual judgement at Euratom (Ispra) of a Systran system (Russian→English) in
1972 revealed huge differences in judging; (Blanchon et al., 2004):
• 1/5 (D–) for output quality (evaluated by teachers of language),
• 4.5/5 (A+) for usability (evaluated by nuclear physicists).
Metrics can drive the research for the topics they evaluate.
• Some measured improvement required by sponsors: NIST MT Eval,
DARPA, TC-STAR, EuroMatrix+.
• BLEU has lead to a focus on phrase-based MT.
• Other metrics may similarly change the community’s focus.
3/84
Our Goal in MT
We restrict the task of MT to the following conditions.
• No writers’ ambitions, we prefer literal translation.
• No attempt at handling cultural differences.
Expected output quality:
1. Worth reading. (Not speaking the src. lang. I can sort of understand.)
2. Worth editing. (I can edit the MT output to obtain publishable text.)
3. Worth publishing, no editing needed.
In general, we’re aiming at level 1 or 2. Level 3 remains risky.
4/84
Basic Manual Evaluation Decisions
What to Show to the annotators when assessing the candidate?
• REF-based ... only the (human) reference
• SRC-based ... only the source
• SRC&REF-based ... both
Context to Consider:
• Sentence-level ... sentences in random order
• Document-level ... obtain single score per document
• Document-aware ... show whole documents, scores per sentence
What to Ask from annotators (scoring technique):
• Some relative score over several candidates?
• Some absolute score for a single output?
• A more complicated question? 5/84
Scoring Techniques
Black-box: Judging hypotheses produced by MT systems:
• Adequacy and fluency of whole sentences.
Somewhat revisited under the name Direct assessment (DA).
• Relative ranking (RR) of full sentences by several MT systems:
Longer sentences hard to rank. Candidates incomparably poor.
• Ranking of constituents, i.e. parts of sentences:
Tackles the issue of long sentences. Does not evaluate overall coherence.
• Comprehension test: Blind editing+correctness check.
• Task-based: Does MT output help as much as the original?
Do I dress appropriately given a translated weather forecast?
Gray-box: Analyzing errors in systems’ output.
• HMEANT, HUME: Is the core event structure preserved?
• MQM: Multi-dimensional quality metrics.
Glass-box: System-dependent: Does this component work? 6/84
Direct Assessment: Adequacy
Graham et al. (2013) propose a simple continuous scale:
• To what extent MT adequately expresses the meaning of REF?
⊕ After ∼15 judgements, each annotator stabilizes.
⊖ Interpretable by averaging over many judgements of many people.
⊖ 30–70(!)% of participating Turkers unreliable.
⊖ Too few non-English speakers on Amazon Mechanical Turk. 7/84
Direct Assessment: Fluency
DA for fluency:
• To what extent the MT is fluent English?
• The source or reference are not shown at all.
• Fluency used only to break ties in adequacy.
8/84
Recent Result: MT Surpassing Humans: 2018
• WMT 2018 English-to-Czech news translation results: (Bojar et al., 2018)
Ave. % Ave. z System
1 84.4 0.667 CUNI-Transformer
2 79.8 0.521 uedin
78.6 0.483 Professional Translation
4 68.1 0.128 online-B
5 59.4 −0.178 online-A
6 54.1 −0.354 online-G
9/84
Recent Result: MT Surpassing Humans: 2018
• WMT 2018 English-to-Czech news translation results: (Bojar et al., 2018)
Ave. % Ave. z System
1 84.4 0.667 CUNI-Transformer
2 79.8 0.521 uedin
78.6 0.483 Professional Translation
4 68.1 0.128 online-B
5 59.4 −0.178 online-A
6 54.1 −0.354 online-G
Caveats:
• Humans translated whole documents, MT individual segments.
• Evaluation was done for individual segments.
9/84
SRC-Based Doc-Level DA
⊖ Mental overload.
⊖ Too few scores
collected ⇒ Difficult to
get statistical
significance.
10/84
SRC-Based Pseudo-Doc-Aware DA
• Score sentences using DA one by one.
• In the original order (i.e. not shuffled).
⇒ Mentally manageable.
Problems of the first run at WMT19 (Barrault et al., 2019):
• No way to go back to previous sentences.
• All sentences in a row must come from the same MT system.
• No longer independent probes (violating statistical assumptions).
11/84
Recent Results: MT Surpassing Humans: 2019
English→Czech
Ave. Ave. z System
1 91.2 0.642 Professional Translators
2 86.0 0.402 CUNI-DocTransformer-T2T
86.9 0.401 CUNI-Transformer-T2T-2018
85.4 0.388 CUNI-Transformer-T2T-2019
5 81.3 0.223 CUNI-DocTransformer-Marian
80.5 0.206 uedin
7 70.8 −0.156 online-Y
71.4 −0.195 TartuNLP-c
9 67.8 −0.300 online-G
10 68.0 −0.336 online-B
11 60.9 −0.594 online-A
12 59.3 −0.651 online-X
12/84
Recent Results: MT Surpassing Humans: 2019
English→Czech
Ave. Ave. z System
1 91.2 0.642 Professional Translators
2 86.0 0.402 CUNI-DocTransformer-T2T
86.9 0.401 CUNI-Transformer-T2T-2018
85.4 0.388 CUNI-Transformer-T2T-2019
5 81.3 0.223 CUNI-DocTransformer-Marian
80.5 0.206 uedin
7 70.8 −0.156 online-Y
71.4 −0.195 TartuNLP-c
9 67.8 −0.300 online-G
10 68.0 −0.336 online-B
11 60.9 −0.594 online-A
12 59.3 −0.651 online-X
English→German
Ave. Ave. z System
90.3 0.347 Facebook-FAIR
93.0 0.311 Microsoft-WMT19-sent-doc
92.6 0.296 Microsoft-WMT19-doc-level
90.3 0.240 Professional Translation
87.6 0.214 MSRA-MADL
88.7 0.213 UCAM
89.6 0.208 NEU
87.5 0.189 MLLP-UPV
87.5 0.130 eTranslation
86.8 0.119 dfki-nmt
84.2 0.094 online-B
… 10 more systems here …
76.3 −0.400 online-X
43.3 −1.769 en-de-task
12/84
SRC-Based Doc-Aware 10-RankME
Mix of all:
• Two or more systems
considered.
• Whole document shown.
• A section of 10 consecutive
sentences scored in
(1) adequacy, (2) fluency,
(3) overall.
⇒ Combines relative, absolute,
doc-level, sent-level.
⊖ Very time-consuming.
13/84
Relative Ranking of Sentences
14/84
Relative Ranking (Eye-Tracked)
Project suggestion: Analyze the recorded data: path patterns / errors in words.
15/84
Relative Ranking of Constituents
16/84
Interpreting Manual RanksA
B
C
better
D
See also Bojar et al. (2011). 17/84
Interpreting Manual RanksA
B
C
better
D "block"
See also Bojar et al. (2011). 18/84
Interpreting Manual RanksA
B
C
better
D
A
B
C
E
See also Bojar et al. (2011). 19/84
Interpreting Manual RanksA
B
C
better
D
A
B
C
E
Who Wins WMT?
See also Bojar et al. (2011). 20/84
Interpreting Manual RanksA
B
C
better
D
A
B
C
E
[Systems] are ranked
based on how frequently
they were judged to be
better than or equal to
any other system.
See also Bojar et al. (2011). 21/84
Interpreting Manual RanksA
B
C
better
D
A
B
C
E
A
B
C
D
A
B
C
E
"≥ All in Block"
A: 1/2
B: 0/2
C: 0/2
D: 0/1
E: 1/1
See also Bojar et al. (2011). 22/84
Interpreting Manual RanksA
B
C
better
D
A
B
C
E
A
B
C
E
"≥ All in Block"
A: 1/2
B: 0/2
C: 0/2
D: 0/1
E: 1/1
See also Bojar et al. (2011). 23/84
Interpreting Manual RanksA
B
C
better
D
A
B
C
E
Simulated
Pairwise "≥ All in Block"
A: 1/2
B: 0/2
C: 0/2
D: 0/1
E: 1/1
See also Bojar et al. (2011). 24/84
Interpreting Manual RanksA
B
C
better
D
A
B
C
E
Simulated
Pairwise
A
B
C
D
A>B
A>C
A>D
B=C
B>D
C>D
"≥ All in Block"
A: 1/2
B: 0/2
C: 0/2
D: 0/1
E: 1/1
See also Bojar et al. (2011). 25/84
Interpreting Manual RanksA
B
C
better
D
A
B
C
E
A>B
A>C
A>D
B=C
B>D
C>D
A<B
A<C
B=C
A<E
B<E
C<E
Simulated
Pairwise
A
B
C
E
A<B
A<C
B=C
A<E
B<E
C<E
"≥ All in Block"
A: 1/2
B: 0/2
C: 0/2
D: 0/1
E: 1/1
See also Bojar et al. (2011). 26/84
Interpreting Manual RanksA
B
C
better
D
A
B
C
E
A>B
A>C
A>D
B=C
B>D
C>D
A<B
A<C
B=C
A<E
B<E
C<E
Simulated
Pairwise "≥ All in Block"
A: 1/2
B: 0/2
C: 0/2
D: 0/1
E: 1/1
See also Bojar et al. (2011). 27/84
Interpreting Manual RanksA
B
C
better
D
A
B
C
E
"≥ Others"
A: 3/6
B: 4/6
C: 4/6
D: 0/3
E: 3/3
A>B
A>C
A>D
B=C
B>D
C>D
A<B
A<C
B=C
A<E
B<E
C<E
Simulated
Pairwise "≥ All in Block"
A: 1/2
B: 0/2
C: 0/2
D: 0/1
E: 1/1
See also Bojar et al. (2011). 28/84
Interpreting Manual RanksA
B
C
better
D
A
B
C
E
"≥ Others"
A: 3/6
B: 4/6
C: 4/6
D: 0/3
E: 3/3
A>B
A>C
A>D
B=C
B>D
C>D
A<B
A<C
B=C
A<E
B<E
C<E
Simulated
Pairwise "≥ All in Block"
A: 1/2
B: 0/2
C: 0/2
D: 0/1
E: 1/1
See also Bojar et al. (2011). 29/84
Interpreting Manual RanksA
B
C
better
D
A
B
C
E
"≥ Others"
A: 3/6
B: 4/6
C: 4/6
D: 0/3
E: 3/3
A>B
A>C
A>D
B=C
B>D
C>D
A<B
A<C
B=C
A<E
B<E
C<E
Simulated
Pairwise "≥ All in Block"
A: 1/2
B: 0/2
C: 0/2
D: 0/1
E: 1/1
See also Bojar et al. (2011). 30/84
Interpreting Manual RanksA
B
C
better
D
A
B
C
E
"≥ Others"
A: 3/6
B: 4/6
C: 4/6
D: 0/3
E: 3/3
A>B
A>C
A>D
B=C
B>D
C>D
A<B
A<C
B=C
A<E
B<E
C<E
Simulated
Pairwise "≥ All in Block"
A: 1/2
B: 0/2
C: 0/2
D: 0/1
E: 1/1
See also Bojar et al. (2011). 31/84
Interpreting Manual RanksA
B
C
better
D
A
B
C
E
"≥ Others"
A: 3/6
B: 4/6
C: 4/6
D: 0/3
E: 3/3
A>B
A>C
A>D
B=C
B>D
C>D
A<B
A<C
B=C
A<E
B<E
C<E
Simulated
Pairwise "≥ All in Block"
A: 1/2
B: 0/2
C: 0/2
D: 0/1
E: 1/1
A: 3/6
B: 4/6 A: 1/2
B: 0/2
See also Bojar et al. (2011). 32/84
Comprehension 1/2 (Blind Editing)
33/84
Comprehension 2/2 (Judging)
34/84
Quiz-Based Evaluation (1/1)
An approximation of task-based evaluation.
Preparation: English texts and Czech yes/no questions:
• We found English text snippets hopefully by native speakers.
• We equipped each snippet with 3 yes/no questions in Czech.
3 different snippet lengths (1..3 sents.), 4 different topics:
• Meeting: when, where, how often, with whom, …
• Directions: driving/walking instructions, finding buildings, …
• Basic quizes: maths, physics, biology, … simple questions.
• Politics/News: elections chances, affairs, finance news, …
Annotation: Given machine-translated snippet, answer the questions. 35/84
Quiz-Based Evaluation (2/2)
Moses 2007 Google 16.2.2010
Na provoz světla na roundabout, obrátit
levice a projet ballymun. Otočit vlevo
na křižovatce. ballymun / Collins Av-
enue Road Dcu je umístěna na Collins
500m na pravém boku Avenue.
Na semaforech na kruhový objezd,
odbočit doleva a jet přes Ballymun.
Odbočit vlevo na Collins Avenue / Bal-
lymun silniční křižovatky. DCU se
nachází na Collins Avenue 500 m na
pravé straně.
Zaškrtněte pravdivá tvrzení:
1. DCU leží na Collins Avenue.
2. V daném městě mají na kruhových objezdech zřejmě semafory.
3. Při příjezdu budete mít DCU po levé straně.
Original: At the traffic lights on the roundabout, turn left and drive through Ballymun. Turn left at
the Collins Avenue/Ballymun Road crossroads. DCU is located on Collins Avenue 500m on the right
hand side. Correct answer: yyn
36/84
Maturita (GCSE)-Like (Vojtěchová et al., 2019)Manual evalua�on by domain experts, scoring in categories:
1. Language Resources – Spelling and Morphology
2. Vocabulary – Adequacy of Terms Used
3. Vocabulary – Clarity of the Text in Terms of Used Words
4. Syntax and Word Order
5. Coherence and Overall Understanding of the Text
0.5 1 1.5 2 2.5 3 3.5 4
Best SystemReference
plo�ed as average rank for be�er comparibility be�er ⟷ worse
en-cs
37/84
Superhuman MT Translating Agreements?
Supplement No. 1 to the agreement on the sublease the apartment, of 13th May 2016
On the day, month and year written below Marta Burešová, pers. no. 695604/3017
Address: Radimova 8, Prague 6, 169 00 as the tenant on the one hand (Hereinafter
referred to as ”the tenant”) and Karolína Černá, pers. no. 136205/891 Address:
Alfrédova 13, Praha 4, 142 00 As a lessee on the other (Hereinafter referred to as ”the
lessee”) collectively also referred to as ”the Contracting parties” have agreed on this
Supplement No. 1 to the Agreement on the sublease the apartment, of 13th May 2016
(hereinafter referred to as the ”Supplement No. 1”)
I. Introductory Provisions
On 13th May 2016, the tenant and the lessee closed the Agreement on the sublease of
the apartment, under which the tenant let the lessee use the apartment No. 4 (area 49
m²) of size 1+1/L in the ground floor of the house in Prague 4, Alfrédova 13, …
(Vojtěchová et al., 2019)
38/84
Superhuman MT Translating Agreements?
Dodatek č. 1 ke smlouvě o podnájmu bytu ze dne 13. května 2016
V den, měsíc a rok níže napsané Marta Burešová, pers. no.
695604/3017 Adresa: Radimova 8, Praha 6, 169 00 jako nájemce na
jedné straně (dále jen „nájemce“) a Karolína Černá, pers. no.
136205/891 Adresa: Alfrédova 13, Praha 4, 142 00 jako nájemce na
straně druhé (dále jen „nájemce“) společně označované také jako
„smluvní strany“ se dohodly na tomto dodatku č. 1 ke smlouvě o
podnájmu, dále jen „nájemní smlouva“, dále jen „13. května 2016“).
I. Úvodní ustanovení
Dne 13. května 2016 nájemce a nájemce uzavřeli smlouvu o dalším
pronájmu bytu, podle níž nájemce pronajímá nájemci byt č. 4 (plocha
49 m²) o velikosti 1+1/l v přízemí domu v Praze 4, Alfrédova 13, … 39/84
Superhuman MT Translating Agreements?
Dodatek č. 1 ke smlouvě o podnájmu bytu ze dne 13. května 2016
V den, měsíc a rok níže napsané Marta Burešová, pers. no.
695604/3017 Adresa: Radimova 8, Praha 6, 169 00 jako nájemce na
jedné straně (dále jen „nájemce“) a Karolína Černá, pers. no.
136205/891 Adresa: Alfrédova 13, Praha 4, 142 00 jako nájemce na
straně druhé (dále jen „nájemce“) společně označované také jako
„smluvní strany“ se dohodly na tomto dodatku č. 1 ke smlouvě o
podnájmu, dále jen „nájemní smlouva“, dále jen „13. května 2016“).
I. Úvodní ustanovení
Dne 13. května 2016 nájemce a nájemce uzavřeli smlouvu o dalším
pronájmu bytu, podle níž nájemce pronajímá nájemci byt č. 4
(plocha 49 m²) o velikosti 1+1/l v přízemí domu v Praze 4, Alfrédova 39/84
HMEANT (Lo and Wu, 2011)
• Improved evaluation of adequacy compared to BLEU.
• Reduced human labour of HTER (Snover et al., 2006).
Essence: Is the basic event structure understandable?
(Who did what to whom, when, where and why.)
1. Identify semantic frames and roles in ref & hyp.
• Manual (5–15 min of training) or automatic (shallow SRL).
2. Mark match/partial/mismatch of each predicate and each
argument.
• Manual.
3. Calculate prec & rec across all frames in the sentence.
4. Report f-score.
40/84
HMEANT Illustration: Motivation
41/84
HMEANT Illustration: SRL
42/84
HMEANT Illustration: SRL
43/84
HMEANT Illustration: SRL
44/84
HMEANT Illustration: SRL
45/84
HMEANT Illustration: SRL
46/84
HMEANT Illustration: SRL
47/84
HMEANT Illustration: SRL
48/84
HMEANT Illustration: Alignment
49/84
HMEANT Illustration: Alignment
50/84
HMEANT Illustration
51/84
HMEANT Illustration
52/84
HUME
HUME (Birch et al., 2016) improves over HMEANT by:
• using semantic trees (UCCA, Abend and Rappoport (2013)),
• using source rather than reference,
• using trees on the source only, not malformed hypothesis.
Two manual stages again:
1. Create UCCA tree for the source (can reuse for more systems!).
2. Label UCCA tree indicating how much was preserved by MT.
53/84
HUME Annotation
• Leafs get R/O/G (traffic lights): bad, mixed, good.
• Structure gets A/B: adequate, bad.
54/84
HMEANT/HUME are Close to FGD
Project suggestion:
Use t-layer tools to:
• Improve UCCA parser, or
• Automate: parse to UCCA
or t-trees, predict R/O/G,
A/B.
55/84
Evaluation by Flagging Errors
Classification of MT errors, following Vilar et al. (2006).punct::Bad Punctuation
unk::Unknown Word missC::Content Word missA::Auxiliary Word
ows::Short Range
ops::Short Range
owl::Long Range
opl::Long Range
lex::Wrong Lexical Choice
disam::Bad Disambiguation
form::Bad Word Form
extra::Extra Word
Error Missing Word
Word Order
Incorrect Words
Word Level
Phrase Level
Bad Word Sense
56/84
Standard MQM (Core)
(Lommel et al., 2014)
57/84
Standard MQM (Overkill)
58/84
MQM Decision Tree (Simplified)MQM annotators guidelines (version 1.4, 2014-11-17) Page 2
Does an unneeded
function word
appear?
Is a needed function
word missing?
Are “function
words” (preposi-
tions, articles,
“helper” verbs, etc.)
incorrect?
Is an incorrect function
word used?
Is the text garbled or
otherwise impossible to
understand?
No
Fluency
(general)*
Grammar
(general)
Function words
(general)
Yes
Extraneous Missing Incorrect
Unintelligible
No
No
No
Is the text grammatically
incorrect?
No
No
Yes No No No
Accuracy
(general)*
No No No No Are words or phrases
translated inappropri-
ately?
Mistranslation
Yes
Are terms translated
incorrectly for the do-
main or contrary to any
terminology resources?
Terminology
Yes
Is there text in the
source language that
should have been
translated?
Untranslated
Yes
Is source content
inappropriately omitted
from the target?
Omission
Yes
Yes YesYes
Yes
Has unneeded content
been added to the
target text?
Addition
Is typography, other than
misspelling or capitaliza-
tion, used incorrectly?
Are one or more words
misspelled/capitalized
incorrectly?
Typography
Spelling
No
No
Yes
Yes
YesDo words appear in
the wrong order? Word order
Yes
Yes Accuracy
Fluency
Grammar
Is the wrong form
of a word used?
Is the part of speech
incorrect?
Do two or more words
not agree for person,
number, or gender?
Is a wrong verb form or
tense used?
Word form
(general)
Part of speech
Yes No No No
Yes
Tense/mood/aspect
Yes
Agreement
Yes Word form
Function words
Note: For any question, if the answer is unclear, select “No”
Is the issue related to the fact
that the text is a translation
(e.g., the target text does not
mean what the source text
does)?
No
MQM Annotation Decision Tree
59/84
MQM Decision Tree (Full)Start here →
Verity
Internation-
alization
DesignFluency
Subtypes of Inter-
nationalization are
currently undened.
A1
Has content present in the source
been inappropriately omitted from
the target?
Yes Go to A2
No Go to A3
A2
Is a variable omitted from the target
content?
Yes Omitted variable
No Omission
A3
Has content not present in the
source been inappropriately added
to the source?
Yes Addition
No Go to A4
A4
Has content been left in the source
language that should have been
translated?
Yes Go to A5
No Go to A6
A5
Is the untranslated content in a
graphic?
Yes Untranslated graphic
No Untranslated
A6
Are words or phrases translated
incorrectly?
Yes Go to A7
No Accuracy (general)
A7
Is a domain- or organization-
speci$c word or phrase translated
incorrectly?
Yes Go to A8
No Go to A10
A8
Is the word or phrase translated
contrary to company-speci$c
terminology guidelines?
Yes Company terminology
No Go to A9
A9
Is the word or phrase translated
contrary to guidelines established in
a normative document (e.g., law or
standard)?
Yes Normative terminology
No Terminology
A10
Is the translation overly literal?
Yes Overly literal
No Go to A11
A11
Is the translated content a “false
friend” (faux ami)?
Yes False friend
No Go to A12
A12
Is a named entity (such as the name
of a person, place, or organization)
translated incorrectly?
Yes Entity
No Go to A13
A13
Was content translated that should
not have been translated?
Yes Should not have been translated
No A14
A14
Was a date or time translated
incorrectly?
Yes Date/time
No A15
A15
Were units (e.g., for measurement or
currency) translated incorrectly?
Yes Unit conversion
No A16
A16
Were numbers translated
incorrectly?
Yes Number
No A17
A17
Is the translation in improper exact
match from translation memory?
F1
Is the content written at a level
of formality inappropriate for the
subject matter, audience, or text
type?
Yes Go to F2
No Go to F3
F2
Does the content use slang or other
unsuitable word variants?
Yes Variants/slang
No Register
F3
Is the content stylistically
inappropriate?
Yes Stylistics
No Go to F4
F4
Is the content inconsistent with
itself?
Yes Go to F5
No Go to F10
F5
Are abbreviations used
inconsistently?
Yes Abbreviations
No Go to F6
F6
Is text inconsistent with graphics?
Yes Image vs. text
No Go to F7
F7
Is the discourse structure of the
content inconsistent?
Yes Discourse
No Go to F8
F8
Is terminology inconsistent within
the content (without being a mis-
translation)?
Yes Terminological inconsistency
No Go to F9
F9
Are cross-references or links
inconsistent in what they point to?
Yes Inconsistent link/cross-reference
No Inconsistency
F10
Does the content use unidiomatic
expressions?
Yes Unidiomatic
No Go to F11
F11
Is content inappropriately
duplicated?
Yes Duplication
No Go to F12
F12
Is the wrong term used? (Generally
assessed for source text only)
Yes Go to F13
No Go to F14
F13
Is the term used contrary to
guidelines established in a
normative document (e.g., law or
standard)?
Yes Monolingual normative terminology
No Monolingual terminology
F14
Is the content ambiguous?
Yes Go to F15
No Go to F16
F15
Is a pronoun or other linguistically
referential structure unclear as to its
reference/antecedent?
Yes Unclear reference
No Ambiguity
F16
Is content spelled incorrectly
(including incorrect capitalization)?
Yes Go to F17
No Go to F19
F17
Is content capitalized incorrectly?
Yes Capitalization
No Go to F18
F18
Are diacritics (e.g., ¨, ´, ˝, ˜) missing or
incorrect?
Yes Diacritics
No Spelling
F19
Does the content violate a formal
style guide (e.g., Chicago Manual of
Style or organization style guide)?
Yes Go to F20
No Go to F22
F20
Is the violation speci$c to a
company/organization’s internal/
house style guide?
Yes Company style
No Go to F21
F21
Is the violation of a third-party
style guide (e.g. Chicago Manual
of Style, American Psychological
Association)?
Yes 3rd-party style
No Style guide
F22
Does the content display problems
with typography (spacing or
punctuation)
Yes Company style
No Go to F26
F23
Are quote marks or brackets
unpaired (i.e., one of a paired set of
punctuation is missing)?
Yes Unpaired quote marks or brackets
No Go to F24
F24
Is punctuation used incorrectly?
Yes Punctuation
No Go to F25
F25
Is whitespace used incorrectly (i.e.,
missing, extra, inconsistent)?
Yes Whitespace
No Typography
F26
Is the content grammatically
incorrect?
Yes Go to F27
No Go to F33
F27
Is an incorrect form of a word used?
Yes Go to F28
No Go to F31
F28
Is the wrong part of speech used?
Yes Part of speech
No Go to F29
F29
Does the content show problems
with agreement (number, gender,
case, etc.)?
Yes Agreement
No Go to F30
F30
Does the content use an incorrect
verbal tense, mood, or aspect?
Yes Tense/mood/aspect
No Word form
F31
Are words in the wrong order?
Yes Word order
No Go to F32
F32
Are functions words (such as articles,
“helper verbs”, or prepositions) used
incorrectly?
Yes Function words
No Grammar
F33
Does the content violate locale-
specic conventions (i.e., it is $ne for
the language, but not for the target
locale)?
Yes Go to F34
No Go to F40
F34
Are dates shown in the wrong
format for the target locale (e.g.,
D-M-Y when Y-M-D is expected)?
Yes Date format
No Go to F35
F35
Are times in the wrong format for
the target locale (e.g., AM/PM when
24-hour time is expected)?
Yes Time format
No Go to F36
F36
Are measurements in the wrong
format for the target locale (e.g.,
metric units used when Imperial are
expected)?
Yes Measurement format
No Go to F37
F37
Are numbers formatted incorrectly
for the target locale (e.g., comma
used as thousands separator when a
dot is expected)?
Yes Number format
No Go to F38
F38
Does the content use the wrong
type of quote mark for the target
locale (e.g., single quotes when
double quotes are expected)?
Yes Quote mark type
No Go to F39
F39
Does the content violate any
relevant national language
standards (e.g., using disallowed
words from another locale)?
Yes National language standard
No Locale convention
F40
Does the content use an incorrect
character encoding?
Yes Character encoding
No Go to F41
F41
Does the content use characters
that are not allowed according to
speci$cations?
Yes Nonallowed characters
No Go to F42
F42
Does the content violate a formal
pattern (e.g., regular expression)
that de$nes what the content may
contain?
Yes Pattern problem
No Go to F43
F43
Is content sorted incorrectly for the
target locale and sorting type?
Yes Sorting
No Go to F44
F44
Is the content inconsistent with a
corpus of known-good content?
(Note: Almost always determined by
a computer program.)
Yes Corpus conformance
No Go to F45
F45
Are links or cross-references broken
or inaccurate?
Yes Go to F46
No Go to F47
F46
Are internal links or cross-references
broken or inaccurate?
Yes Document-internal
No Document-external
F47
Are there problems with an index or
Table of Content (ToC)?
Yes Go to F48
No Go to F51
F48
Are page references in an index or
Table of Content (ToC) incorrect?
Yes Page references
No Go to F49
F49
Is the format of an index or Table of
Content (ToC) incorrect?
F50
Are items missing from an index or
Table of Content (ToC)?
F51
Is content unintelligible (i.e., the
Duency is bad enough that the
nature of the problem cannot be
determined)?
V1
Is the content unsuitable for the
end-user (target audience)?
Yes End-user suitability
No Go to V2
V2
Is the content incomplete or missing
needed information?
Yes Go to V3
No Go to V5
V3
Are lists within the content
incomplete or missing needed
information?
Yes Lists
No Go to V4
V4
Are procedures described within
the content incomplete or missing
needed information?
Yes Procedures
No Completeness
V5
Does the content violate any legal
requirements for the target locale or
intended audience?
Yes Legal requirements
No Go to V6
V6
Does the content inappropriately
include information that does apply
not to the target locale or that is
otherwise inaccurate for it?
Yes Locale-specific content
No Verity
D1
Does the formatting issue apply
globally to the entire document?
Yes Go to D2
No Go to D8
D2
Are colors used incorrectly?
Yes Color
No Go to D3
D3
Is the overall font choice incorrect
Yes Global font choice
No Go to D4
D4
Are footnotes/endnotes formatted
incorrectly?
Yes Footnote/endnote format
No Go to D5
D5
Are margins for the document
incorrect?
Yes Margins
No Go to D6
D6
Are widows/orphans present in the
content?
Yes Widows/orphans
No Go to D7
D7
Are there improper page breaks?
Yes Page break
No Overall design (layout)
D8
Is local formatting (within content)
incorrect?
Yes Go to D9
No Go to D17
D9
Is text aligned incorrectly?
Yes Text alignment
No Go to D10
D10
Are paragraphs indented improperly
or not indented when they should
be?
Yes Paragraph indentation
No Go to D11
D11
Are fonts used incorrectly within
content (rather than globally)?
Yes Go to D12
No Go to D15
D12
Are bold or italic used incorrectly?
Yes Bold/italic
No Go to D13
D13
Is a wrong font size used?
Yes Wrong size
No Go to D14
D14
Are single-width fonts used when
double-width fonts should be used
(or vice versa)?
(Applies to CJK text only.)
Yes Single/double-width
No Font
D15
Is text kerning (space between
letters) incorrect (text too tight/too
loose)?
Yes Kerning
No Go to D16
D16
Is the leading (line spacing of text)
incorrect (e.g., double spacing when
single spacing is expected)?
Yes Leading
No Local formatting
D17
Is translated text missing from the
layout (i.e., it has been translated
but is not visible in the formatted
version)?
Yes Missing text
No Go to D18
D18
Is markup (e.g., formatting codes)
used incorrectly or in a technically
invalid fashion?
Yes Go to D19
No Go to D24
D19
Is markup used inconsistently (e.g.,
<i> is used in some places and
<em> in others)?
Yes Inconsistent markup
No Go to D20
D20
Does markup appear in the wrong
place within content?
Yes Misplaced markup
No Go to D21
D21
Has markup been inappropriately
added to the content?
Yes Added markup
No Go to D22
D22
Is needed markup missing from the
content?
Yes Missing markup
No Go to D23
D23
Does markup appear to be
incorrect? (Note: Generally detected
by computer processes)
Yes Missing markup
No Markup
D24
Are there problems with graphic
and/or tables?
Yes Go to D25
No Go to D28
D25
Are graphics or tables positioned
incorrectly on the page or with
respect to surrounding text?
Yes Position
No Go to D26
D26
Are graphics or tables missing from
the text?
Yes Missing graphic/table
No Go to D27
D27
Are there problems with call-outs or
captions for graphics or tables?
Yes call-outs and captions
No Graphics and tables
D28
Are portions of text invisible due to
text expansion?
Yes Truncation/text expansion
No Go to D29
D29
Is text longer than is allowed (but
remains visible)?
Yes Truncation/text expansion
No Length
Multidimensional Quality Metrics (MQM): Full Decision Tree
e Multidimensional Quality Metrics (MQM) Framework provides a hierarchical categorization of error types that occur in translated or localized products. Based on a detailed analysis of existing translation quality metrics, it provides a #exible typology of issue types
that can be applied to analytic or holistic translation quality evaluation tasks. Although the full MQM issue tree (which, as of November 2014, contains 115 issue types categorized into ,ve major branches) is not intended to be used in its entirety for any particular evalu-
ation task, this overview chart presents a “decision tree” suitable for selecting an issue type from it. In practical terms, however, an individual metric would have a smaller decision tree that covers just the issues contained in that metric.
To use the decision tree start with the ,rst question and follow the appropriate answers until a speci,c issue type is reached.
General 1
Is the issue related to a diHerence in
meaning between the source and
target?
Yes Go to Accuracy
No Go to General 2
General 2
Is the issue related to the linguistic
or mechanical formulation of the
content
Yes Go to Fluency
No Go to General 3
General 3
Is the issue related to the
appropriateness of the content for the
target audience or locale (separate from
whether it is translated correcty)?
Yes Go to Verity
No Go to General 4
General 4
Is the issue related to the
presentational/display aspects of
the content?
Yes Go to Design
No Go to General 5
General 5
Is the issue related to whether or
not the content was set up properly
to support subsequent translation/
adaptation?
Yes Go to Internationalization
No Go to General 6
General 6
Is the issue addressed in the
Compatability branch
Yes Go to Compatability
No Other
Accuracy
1+ 2          
            '
        ,   
   *0*- 2    Other, it may
           
  ,         
3  -
60/84
Error Flagging Example
Annotation rules:
• Mark/suggest as little as necessary.
• Compare to source, not to reference. Literal translation ok.
• Preserve white space. Don’t add or remove word/line breaks.
• Only insert error labels followed by ::.
• For missing words, use _ instead of space, if necessary.
Src Perhaps there are better times ahead.
Ref Možná se tedy blýská na lepší časy.
Možná, že extra::tam jsou lepší disam::krát lex::dopředu.
Možná extra::tam jsou příhodnější časy vpředu.
missC::v_budoucnu Možná form::je lepší časy.
Možná jsou lepší časy lex::vpřed.
61/84
Results on WMT09 Dataset
google cu-bojar pctrans cu-tectomt Total
Automatic: BLEU 13.59 14.24 9.42 7.29 –
Manual: Rank 0.66 0.61 0.67 0.48 –
disam 406 379 569 659 2013
lex 211 208 231 340 990
Total bad word sense 617 587 800 999 3003
missA 84 111 96 138 429
missC 72 199 42 108 421
Total missed words 156 310 138 246 850
form 783 735 762 713 2993
extra 381 313 353 394 1441
unk 51 53 56 97 257
Total serious errors 1988 1998 2109 2449 8544
ows 117 100 157 155 529
punct 115 117 150 192 574
… … … … … …
tokenization 7 12 10 6 35
Total errors 2319 2354 2536 2895 10104 62/84
Contradictions in Manual Evaluation
Results for WMT10:
Evaluation Method Google CU-Bojar PC Translator TectoMT
≥ others (WMT10 official) 70.4 65.6 62.1 60.1
> others 49.1 45.0 49.4 44.1
Edits deemed acceptable [%] 55 40 43 34
Quiz-based evaluation [%] 80.3 75.9 80.0 81.5
Automatic: BLEU 0.16 0.15 0.10 0.12
Automatic: NIST 5.46 5.30 4.44 5.10
Results for WMT19:
• Best systems match humans in GCSE-like scoring.
• They score worse in pseudo-doc-aware DA.
• They are absolutely terrible on agreements.
… each technique provides a different picture. 63/84
Problems of Manual Evaluation
• Expensive in terms of time/money.
• Subjective (some judges are more careful/better at guessing).
• Not quite consistent judgments from different people.
• Not quite consistent judgments from a single person!
• Not reproducible (too easy to solve a task for the second time).
• Experiment design is critical!
• Black-box evaluation important for users/sponsors.
• Gray/Glass-box evaluation important for the developers.
• SRC-based allows to compare with humans.
• Sentence-level no longer relevant for large language pairs.
64/84
Automatic Evaluation
• Comparing MT output to reference translation.
• (Reference-less evaluation is called Quality Estimation.)
• Fast and cheap.
• Deterministic, replicable.
• Allows automatic model optimization (“tuning”, MERT).
• Usually good for checking progress.
• Usually bad for comparing systems of different types.
65/84
BLEU (Papineni et al., 2002)
• Based on geometric mean of 𝑛-gram precision.
≈ ratio of 1- to 4-grams of hypothesis confirmed by a ref. translation
Src The legislators hope that it will be approved in the next few days . Confirmed
Ref Zákonodárci doufají , že bude schválen v příštích několika dnech . 1 2 3 4
Moses Zákonodárci doufají , že bude schválen v nejbližších dnech . 9 7 5 4
TectoMT Zákonodárci doufají , že bude schváleno další páru volna . 6 4 3 2
Google Zákonodárci naději , že bude schválen v několika příštích dnů . 9 4 3 2
PC Tr. Zákonodárci doufají že to bude schválený v nejbližších dnech . 7 2 0 0
n-grams confirmed: none, unigram, bigram, trigram, fourgram
E.g. Moses produced 10 unigrams (9 confirmed), 9 bigrams (7 confirmed), …
BLEU = BP ⋅ exp(1
4 log( 9
10) + 1
4 log(7
9 ) + 1
4 log(5
8) + 1
4 log( 4
7 ))
BP is “brevity penalty”; 1
4 are uniform weights, the “denominator” equivalent for 4
√⋅ in
geometric mean in the log domain. 66/84
BLEU: Avoid Cheating/Gaming the Metric
• Confirmed counts “clipped” to avoid overgeneration.
• “Brevity penalty” applied to avoid too short output:
BP = { 1 if 𝑐 > 𝑟
𝑒1−𝑟/𝑐 if 𝑐 ≤ 𝑟
Ref 1: The cat is on the mat .
Ref 2: There is a cat on the mat .
Candidate: The the the the the the the .
⇒ Clipping: only 3
8 unigrams confirmed.
Candidate: The the .
⇒ 3
3 unigrams confirmed but the output is too short.
⇒ BP = 𝑒1−7/3 = 0.26 strikes.
The candidate length 𝑐 and “effective” ref. length 𝑟 calculated over the whole test set. 67/84
BLEU Properties
• Within the range 0-1, often written as 0 to 100%.
• Human translation against other humans: ~60%
• Google Chinese→English: ~30%, Arabic→English: ~50%.
• BLEU for individual sentences not reliable.
• More so with only 1 reference translation:
Src ” We ’ ve made great progress .
Ref ” Učinili jsme velký pokrok .
Moses ” my jsme udělali velký pokrok .
TectoMT ” Udělali jsme velký pokrok .
Google ” My jsme dosáhli obrovského pokroku .
PC Translator ” udělali jsme velký pokrok .
68/84
Test Set Influence on BLEU
Havlíček (2007) evaluates the influence of:
• number of reference translations,
• translation direction.
on human-produced text (1 human translation against 4 others).
cs→en, Professionals en→cs, Math Students
Refs Indiv. Results Avg Indiv. Results Avg
1 41.15 32.66 34.03 35.95 3.66 8.62 5.79 6.02
2 49.09 49.78 41.26 46.71 9.82 8.26 9.36 9.15
3 52.63 52.63 13.06 13.06
⇒ heavy dependence on the number of references.
More references allow to match more n-grams of MT output.
⇒ heavy dependence on the translation direction and quality. 69/84
Correlation with Human Judgments
BLEU scores vs. human rank, the higher, the better:
6 7 8 9 10 11 12 13 14 15 16 17
−3.5
−3.3
−3.1
−2.9
−2.7
−2.5bbbbbcbcbcbc
WMT08 Results In-domain • Out-of-domain ∘
BLEU Rank BLEU Rank
Factored Moses 15.91 -2.62 11.93 -2.89
PC Translator 8.48 -2.78 8.41 !! -2.60
TectoMT 9.28 -3.29 6.94 -3.26
Vanilla Moses 12.96 -3.33 9.64 -3.26
⇒ PC Translator nearly won Rank but nearly lost in BLEU. 70/84
Dirty Tricks
• PCEDT 1.0 (Čmejrek et al., 2004) contains test set with:
• 1 English original,
• 1 Czech translation,
• 4 English back-translations (via Czech).
• Čmejrek et al. (2003) evaluate cs→en MT using all 5 English
sentences: they include the original source among the references
and report 5-fold average of BLEU (on 4 refs).
• The additional accepted variance in output increases BLEU
compared to BLEU on the 4 back-translations only.
5-fold Avg of 4-BLEU 4 refs only
PBT, no additional LM 34.8±1.3 32.5
PBT, bigger LM 36.4±1.3 34.2
PBT, more parallel texts, bigger LM 38.1±0.8 36.8 71/84
Improving BLEU in cs→en MT
A summary of older experiments. (Bojar et al., 2006; Bojar, 2006)
Deterministic pre- and post-processing
similar tokenization of reference +10.0 !!!
lemmatization for alignment +2.0
handling numbers +0.9
fixing clear BLEU errors +0.5 !
dependency-based corpus expansion +0.3
More parallel or target-side monolingual data
out-of-domain parallel texts, bigger in-domain LM +5.0
bigged in-domain LM +1.7
out-of-domain parallel texts, also in LM +0.4
adding a raw dictionary +0.2
• Complicated methods bring a little.
• Data bring more.
• Huge jumps from superficial properties but just higher BLEU, same MT quality.
72/84
Finding Clear BLEU Losses
Missing bigram = all references contained it but not the hypothesis.
Superfluous bigram = the hypothesis contained it but none of the references.
Top missing bigrams:
19 , " 12 ” said
12 of the 10 Free Europe
10 Radio Free 7 . "
6 L.J. Hooker 6 United States
6 in the 6 the United
6 the strike …
Top superfluous bigrams:
26 , '' 18 '' .
14 ” said 12 , which
11 Svobodná Evropa 8 , when
8 the state 7 , who
7 J. Hooker 7 L. J.
7 company GM …
Four simple rules to improve BLEU by +0.2 to +0.5 on a particular test set:
'' . → . " L. J. Hooker → L.J. Hooker
'' → " the U.S. → the United States
73/84
Technical Problems of BLEU
BLEU scores are not comparable:
• across languages.
• on different test sets.
• with different number of reference translations.
• with different implementations of the evaluation tool.
• There are different definitions of “reference length”:
Papineni et al. (2002) not specific. One can choose the shortest, longest,
average, closest (the smaller or the larger!).
• Very sensitive to tokenization:
Beware esp. of malformed tokenization of Czech by foreign tools.
⇒ Use a fixed implementation, e.g. sacreBLEU (Post, 2018). 74/84
Fundamenal Problems of BLEU
• BLEU overly sensitive to word forms and sequences of tokens.
Confirmed Contains
by Ref Error Flags 1-grams 2-grams 3-grams 4-grams
Yes Yes 6.34% 1.58% 0.55% 0.29%
Yes No 36.93% 13.68% 5.87% 2.69%
No Yes 22.33% 41.83% 54.64% 63.88%
No No 34.40% 42.91% 38.94% 33.14%
Total 𝑛-grams 35 531 33 891 32 251 30 611
30–40% of tokens not confirmed by reference but without errors.
⇒ Enough space for MT systems to differ unnoticed.
⇒ Low BLEU scores correlate even less. 75/84
Fixing Fundamenal Issues of BLEU
Evaluate coarser units:
• Lemmas or deep-lemmas instead of word forms:
• e.g. SemPOS (Kos and Bojar, 2009): bags of t-lemmas.
• Sequences of characters:
• e.g. chrF3 (Popović, 2015): F-score of character 6-grams.
• Use shorter of gappy sequences:
• e.g. BEER (Stanojevic and Sima’an, 2014) uses characters and also pairs
of (not necessarily adjacent) words.
Use better references:
• Using more references alone helps.
• Post-edited references serve better.
• e.g. HTER (Snover et al., 2006): Measuring edit distance to manually
corrected output.
76/84
Post-Edited References Serve Better0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
10 100 1000
Correlation of
BLEU and manual ranks
Test set size
Refs: official 1
Refs: postedited 1
Refs: postedited 6
Refs: postedited 7
Refs: postedited 8
• Refs created by post-editing serve better than independent ones.
• 100 sents with 6–7 postedited refs as good as 3k indep refs. 77/84
Post-Edited Refs Better0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
10 100 1000
Correlation of
BLEU and manual ranks
Test set size
Refs: official 1
Refs: postedited 1
Refs: postedited 6
Refs: postedited 7
Refs: postedited 8
• … but error bars quite wide
⇒ specific sentences important. 78/84
Fundamenal Problem of Correlation468101214161820
sacreBLEU
Correlation with DA
0
-1
1
• Correlation depends on the underlying set of MT systems.
• Often poor correlation when only top-scoring systems are
considered, see Ma et al. (2019). 79/84
Fundamenal Problem of Correlation0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
-2 -1.5 -1 -0.5 0 0.5
SacreBLEU-BLEU
DA
Top 4
Top 6
Top 8
Top 10
Top 12
Top 15
All systems
80/84
Empirical Confidence Intervals
In statistics, confidence intervals indicate how well was a parameter (e.g. the mean) of
a random variable with known/assumed distribution estimated from a set of repeated
measurements.
• We don’t want to assume any distribution!
• How to “repeat” experiments with a deterministic MT system?
Use “bootstrapping” (Koehn, 2004):
1. Obtain 1000 different test sets:
Randomly select sents., repeat some, ignore some, preserving test set size.
2. Sort by the score.
3. Drop top and bottom 2.5% (i.e. 25 out of 1000) results.
⇒ The lowest and highest remaining scores are 95% empirical
confidence interval around the score obtained on the full test set. 81/84
End-to-end vs. Component Eval.
• Similar to black vs. glass box evaluation and translation vs.
task-based evaluation.
• Evaluation of a single component may not correlate with overall
performance of the system.
Pre-processing Symmetrization Alignment Error Rate BLEU
Lemmas + singletons Intersection 14.6 30.8
Lemmas Intersection 15.0 29.8
Lemmas Union 17.2 32.0
Lemmas + singletons Union 17.4 31.9
Baseline (word forms) Union 25.5 29.8
Baseline (word forms) Intersection 27.4 28.2
Data by Bojar et al. (2006). See also e.g. Lopez and Resnik (2006). 82/84
Summary, The Moral of the Story
Metrics drive research:
• Measure the property that “saves money” in your application.
• Design automatic metrics to correlate with humans.
Comparisons of automatic scores trustworthy
only under all the following:
• a single test set was used (of your domain of interest),
• evaluated by a single evaluation tool (hopefully without bugs),
E.g. for BLEU different tools tokenize and define ref. length differently.
• the metric reflects your final objective (AER vs. BLEU),
• confidence intervals are estimated.
83/84
References
Omri Abend and Ari Rappoport. 2013. Universal Conceptual Cognitive Annotation (UCCA). In Proceedings of the 51st
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 228–238, Sofia,
Bulgaria, August. Association for Computational Linguistics.
Loïc Barrault, Ondřej Bojar, Marta R. Costa-jussà, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow,
Matthias Huck, Philipp Koehn, Shervin Malmasi, Christof Monz, Mathias Müller, Santanu Pal, Matt Post, and Marcos
Zampieri. 2019. Findings of the 2019 conference on machine translation (wmt19). In Proceedings of the Fourth
Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 1–61, Florence, Italy, August.
Association for Computational Linguistics.
Alexandra Birch, Omri Abend, Ondřej Bojar, and Barry Haddow. 2016. HUME: Human UCCA-Based Evaluation of
Machine Translation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,
pages 1264–1274, Austin, Texas, November. Association for Computational Linguistics. peer-reviewed.
Hervé Blanchon, Christian Boitet, and Laurent Besacier. 2004. Spoken Dialogue Translation Systems Evaluation:
Results, New Trends, Problems and Proposals. In Proceedings of International Conference on Spoken Language
Processing ICSLP 2004, Jeju Island, Korea, October.
Ondřej Bojar, Evgeny Matusov, and Hermann Ney. 2006. Czech-English Phrase-Based Machine Translation. In FinTAL
2006, volume LNAI 4139, pages 214–224, Turku, Finland, August. Springer.
Ondřej Bojar, Miloš Ercegovčević, Martin Popel, and Omar Zaidan. 2011. A Grain of Salt for the WMT Manual
Evaluation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 1–11, Edinburgh, Scotland,
July. Association for Computational Linguistics.
Ondřej Bojar, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, and
Christof Monz. 2018. Findings of the 2018 Conference on Machine Translation (WMT18). In Proceedings of the Third
Conference on Machine Translation, Volume 2: Shared Task Papers, Brussels, Belgium, October. Association for 84/84


Approaches to MT:
SMT, PBMT, NMT
Ondřej Bojar
March 5, 2020
NPFL087 Statistical Machine Translation
Charles University
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics unless otherwise stated
Outline
• Approaches to MT.
• What makes MT statistical.
• Probability of a sentence, Bayes’ law.
• Log-linear model.
• Phrase-Based MT.
• Features used.
• Training Pipeline.
• Unjustified independence assumptions.
• Neural MT.
• Deep learning summary.
• Representing text
• Encoder-decoder architecture.
1/51
Approaches to Machine Translation
• The deeper analysis, the easier the transfer should be.
• A hypothetical interlingua captures pure meaning.
• Statistical systems learn “automatically” from data.
• Rule-based systems implemented by linguists-programmers.
Until NMT, it was best to combine the approaches.
2/51
Zap through History
• Rule-Based MT.
Linguists/language experts write rules.
Controlled Language: Authors restricted to produce MT-translatable text.
• Example-Based MT.
Given translation memories, find examples similar to input.
• Statistical MT:
1. Word-Based.
2. Phrase-Based.
3. Syntax-Based.
4. Neural.
3/51
Quotes
Warren Weaver (1949):
I have a text in front of me which is written in Russian but I am going to
pretend that it is really written in English and that is has been coded in some
strange symbols. All I need to do is strip off the code in order to retrieve
the information contained in the text.
Noam Chomsky (1969):
…the notion “probability of a sentence” is an entirely useless one, under any
known interpretation of this term.
Frederick Jelinek (80’s; IBM; later JHU and sometimes ÚFAL)
Every time I fire a linguist, the accuracy goes up.
Hermann Ney (RWTH Aachen University):
MT = Linguistic Modelling + Statistical Decision Theory 4/51
The Statistical Approach
(Statistical = Information-theoretic.)
• Specify a probabilistic model.
= How is the probability mass distributed among possible outputs given
observed inputs.
• Specify the training criterion and procedure.
= How to learn free parameters from training data.
Notice:
• Linguistics helpful when designing the models:
• How to divide input into smaller units.
• Which bits of observations are more informative.
5/51
Ultimate Goal of Traditional SMT
Find minimum translation units (MTUs) ∼ graph partitions:
• such that they are frequent across many sentence pairs.
• without imposing (too hard) constraints on reordering.
• (ideally in an unsupervised fashion, no reliance on linguistics).
Available data: Word co-occurrence statistics:
• In large monolingual data (usually up to 109 words).
• In smaller parallel data (up to 107 words per language).
• Optional automatic rich linguistic annotation.
6/51
Statistical MT
Given a source (foreign) language sentence 𝑓𝐽
1 = 𝑓1 … 𝑓𝑗 … 𝑓𝐽 ,
Produce a target language (English) sentence 𝑒𝐼
1 = 𝑒1 … 𝑒𝑗 … 𝑒𝐼 .
Among all possible target language sentences, choose the sentence with
the highest probability:̂
𝑒̂
𝐼
1 = argmax
𝐼,𝑒𝐼
1
𝑝(𝑒𝐼
1|𝑓𝐽
1 ) (1)
We stick to the 𝑒𝐼
1, 𝑓𝐽
1 notation despite translating from English to Czech.
7/51
Brute-Force MT
Translate only sentences listed in a “translation memory” (TM):
Good morning. = Dobré ráno.
How are you? = Jak se máš?
How are you? = Jak se máte?
𝑝(𝑒𝐼
1|𝑓𝐽
1 ) = { 1 if 𝑒𝐼
1 = 𝑓𝐽
1 seen in the TM
0 otherwise (2)
Any problems with the definition?
8/51
Brute-Force MT
Translate only sentences listed in a “translation memory” (TM):
Good morning. = Dobré ráno.
How are you? = Jak se máš?
How are you? = Jak se máte?
𝑝(𝑒𝐼
1|𝑓𝐽
1 ) = { 1 if 𝑒𝐼
1 = 𝑓𝐽
1 seen in the TM
0 otherwise (2)
• Not a probability. There may be 𝑓𝐽
1 , s.t. ∑𝑒𝐼
1
𝑝(𝑒𝐼
1|𝑓𝐽
1 ) > 1.
⇒ Have to normalize, use count(𝑒𝐼
1,𝑓𝐽
1 )
count(𝑓𝐽
1 ) instead of 1.
• Not “smooth”, no generalization:
Good morning. ⇒ Dobré ráno.
Good evening. ⇒ ∅ 8/51
Bayes’ Law
Bayes’ law for conditional probabilities: 𝑝(𝑎|𝑏) = 𝑝(𝑏|𝑎)𝑝(𝑎)
𝑝(𝑏)
So in our case:̂
𝑒̂
𝐼
1 = argmax
𝐼,𝑒𝐼
1
𝑝(𝑒𝐼
1|𝑓𝐽
1 ) Apply Bayes’ law
= argmax
𝐼,𝑒𝐼
1
𝑝(𝑓𝐽
1 |𝑒𝐼
1)𝑝(𝑒𝐼
1)
𝑝(𝑓𝐽
1 )
𝑝(𝑓𝐽
1 ) constant
⇒ irrelevant in maximization
= argmax
𝐼,𝑒𝐼
1
𝑝(𝑓𝐽
1 |𝑒𝐼
1)𝑝(𝑒𝐼
1)
Also called “Noisy Channel” model.
9/51
Motivation for Noisy Channel̂
𝑒̂
𝐼
1 = argmax
𝐼,𝑒𝐼
1
𝑝(𝑓𝐽
1 |𝑒𝐼
1)𝑝(𝑒𝐼
1) (3)
Bayes’ law divided the model into components:
𝑝(𝑓𝐽
1 |𝑒𝐼
1) Translation model (“reversed”, 𝑒𝐼
1 → 𝑓𝐽
1 )
…is it a likely translation?
𝑝(𝑒𝐼
1) Language model (LM)
…is the output a likely sentence of the target language?
• The components can be trained on different sources.
There are far more monolingual data ⇒ language model can be more reliable.
10/51
Without EquationsInput Global Search
for sentence with highest probability Output
Parallel Texts
Translation Model
Monolingual Texts
Language Model
11/51
Summary of Language Models
• 𝑝(𝑒𝐼
1) should report how “good” sentence 𝑒𝐼
1 is.
• We surely want 𝑝(The the the.) < 𝑝(Hello.)
• How about 𝑝(The cat was black.) < 𝑝(Hello.)?
…We don’t really care in MT. We hope to compare synonymic sentences.
LM is usually a 3-gram language model:
𝑝(↱ ↱ The cat was black . ↰ ↰ ) = 𝑝(The| ↱ ↱) 𝑝(cat| ↱ The) 𝑝(was|The cat)
𝑝(black|cat was) 𝑝(.|was black) 𝑝(↰ |black .)
𝑝(↰ |. ↰)
Formally, with 𝑛 = 3:
𝑝LM(𝑒𝐼
1) =
𝐼
∏
𝑖=1
𝑝(𝑒𝑖|𝑒𝑖−1
𝑖−𝑛+1) (4)
12/51
Estimating and Smoothing LM
𝑝(𝑤1) = count(𝑤1)
total words observed Unigram probabilities.
𝑝(𝑤2|𝑤1) = count(𝑤1𝑤2)
count(𝑤1) Bigram probabilities.
𝑝(𝑤3|𝑤2, 𝑤1) = count(𝑤1𝑤2𝑤3)
count(𝑤1𝑤2) Trigram probabilities.
Unseen ngrams (𝑝(ngram) = 0) are a big problem, invalidate whole
sentence: 𝑝LM(𝑒𝐼
1) = ⋯ ⋅ 0 ⋅ ⋯ = 0
⇒ Back-off with shorter ngrams:
𝑝LM(𝑒𝐼
1) = ∏𝐼
𝑖=1( 0.8 ⋅ 𝑝(𝑒𝑖|𝑒𝑖−1, 𝑒𝑖−2)+
0.15 ⋅ 𝑝(𝑒𝑖|𝑒𝑖−1)+
0.049 ⋅ 𝑝(𝑒𝑖)+
0.001 ) ≠ 0
(5)
13/51
From Bayes to Log-Linear Model
Och (2002) discusses some problems of Equation 3:
• Models estimated unreliably ⇒ maybe LM more important:̂
𝑒̂
𝐼
1 = argmax
𝐼,𝑒𝐼
1
𝑝(𝑓𝐽
1 |𝑒𝐼
1)(𝑝(𝑒𝐼
1))2 (6)
• In practice, “direct” translation model equally good:̂
𝑒̂
𝐼
1 = argmax
𝐼,𝑒𝐼
1
𝑝(𝑒𝐼
1|𝑓𝐽
1 )𝑝(𝑒𝐼
1) (7)
• Complicated to correctly introduce other dependencies.
⇒ Use log-linear model instead.
14/51
Log-Linear Model (1)
• 𝑝(𝑒𝐼
1|𝑓𝐽
1 ) is modelled as a weighted combination of models, called
“feature functions”: ℎ1(⋅, ⋅) … ℎ𝑀 (⋅, ⋅)
𝑝(𝑒𝐼
1|𝑓𝐽
1 ) = exp(∑𝑀
𝑚=1 𝜆𝑚ℎ𝑚(𝑒𝐼
1, 𝑓𝐽
1 ))
∑𝑒′𝐼′
1
exp(∑𝑀
𝑚=1 𝜆𝑚ℎ𝑚(𝑒′𝐼′
1 , 𝑓𝐽
1 )) (8)
• Each feature function ℎ𝑚(𝑒, 𝑓) relates source 𝑓 to target 𝑒.
E.g. the feature for 𝑛-gram language model:
ℎLM(𝑓𝐽
1 , 𝑒𝐼
1) = log
𝐼
∏
𝑖=1
𝑝(𝑒𝑖|𝑒𝑖−1
𝑖−𝑛+1) (9)
• Model weights 𝜆𝑀
1 specify the relative importance of features. 15/51
Log-Linear Model (2)
As before, the constant denominator not needed in maximization:̂
𝑒̂
𝐼
1 = argmax𝐼,𝑒𝐼
1
exp(∑𝑀
𝑚=1 𝜆𝑚ℎ𝑚(𝑒𝐼
1, 𝑓𝐽
1 ))
∑𝑒′𝐼′
1
exp(∑𝑀
𝑚=1 𝜆𝑚ℎ𝑚(𝑒′𝐼′
1 , 𝑓𝐽
1 ))
= argmax𝐼,𝑒𝐼
1
exp(∑𝑀
𝑚=1 𝜆𝑚ℎ𝑚(𝑒𝐼
1, 𝑓𝐽
1 ))
(10)
16/51
Relation to Noisy Channel
With equal weights and only two features:
• ℎTM(𝑒𝐼
1, 𝑓𝐽
1 ) = log 𝑝(𝑓𝐽
1 |𝑒𝐼
1) for the translation model,
• ℎLM(𝑒𝐼
1, 𝑓𝐽
1 ) = log 𝑝(𝑒𝐼
1) for the language model,
log-linear model reduces to Noisy Channel:̂
𝑒̂
𝐼
1 = argmax𝐼,𝑒𝐼
1
exp(∑𝑀
𝑚=1 𝜆𝑚ℎ𝑚(𝑒𝐼
1, 𝑓𝐽
1 ))
= argmax𝐼,𝑒𝐼
1
exp(ℎTM(𝑒𝐼
1, 𝑓𝐽
1 ) + ℎLM(𝑒𝐼
1, 𝑓𝐽
1 ))
= argmax𝐼,𝑒𝐼
1
exp(log 𝑝(𝑓𝐽
1 |𝑒𝐼
1) + log 𝑝(𝑒𝐼
1))
= argmax𝐼,𝑒𝐼
1
𝑝(𝑓𝐽
1 |𝑒𝐼
1)𝑝(𝑒𝐼
1)
(11)
17/51
Common Features of PBMT
• Phrase translation probability:
ℎPhr(𝑓𝐽
1 , 𝑒𝐼
1, 𝑠𝐾
1 ) = log ∏𝐾
𝑘=1 𝑝(̃𝑓𝑘|̃𝑒𝑘) where 𝑝(̃𝑓𝑘|̃𝑒𝑘) = count(̃𝑓,̃ 𝑒)
count(̃𝑒)
⇒ Are all used units̃ 𝑓 ↔̃ 𝑒 likely translations?
• Word count/penalty: ℎwp(𝑒𝐼
1, ⋅, ⋅) = 𝐼
⇒ Do we prefer longer or shorter output?
• Phrase count/penalty: ℎpp(⋅, ⋅, 𝑠𝐾
1 ) = 𝐾
⇒ Do we prefer translation in more or fewer less-dependent bits?
• Reordering model: different basic strategies (Lopez, 2009)
⇒ Which source spans can provide continuation at a moment?
• 𝑛-gram LM: ℎLM(⋅, 𝑒𝐼
1, ⋅) = log ∏𝐼
𝑖=1 𝑝(𝑒𝑖|𝑒𝑖−1
𝑖−𝑛+1)
⇒ Is output 𝑛-gram-wise coherent?
18/51
Features: Constructing or Scoring?
Are features used to construct hypotheses or just score them?
• Phrase translation probabilities: ⇒ for construction, see below.
• Counts/penalties: ⇒ for scoring only.
• Language models: ⇒ for scoring only.
But it could be used for construction: predict next word and confirm from
translation.
19/51
Traditional MT “Pipeline”
“Training the Translation Model”
1. Find relevant parallel texts.
2. Align at the level of sentences.
3. Align at the level of words.
4. Extract translation units, with scores (co-oc. stats.).
(Language Model similar, “simple” words co-oc. stats, no alignment.)
“Tuning” (“MERT”) = Actual training in the ML sense
5. Identify TM/LM/other model component weights.
Translation:
6. Decompose input into known units.
7. Search for best combinations of units. 20/51
1: Align Training SentencesNemám žádného psa.
I have no dog.
Viděl kočku.
a cat.He saw
21/51
2: Align WordsNemám žádného psa.
I have no dog.
Viděl kočku.
a cat.He saw
22/51
3: Extract Phrase Pairs (MTUs)Nemám žádného psa.
I have no dog.
Viděl kočku.
a cat.He saw
23/51
4: New InputNemám žádného psa.
I have no dog.
Viděl kočku.
a cat.He saw
New input: kočku.Nemám
24/51
4: New InputNemám žádného psa.
I have no dog.
Viděl kočku.
a cat.He saw
New input: kočku.Nemám
... I don't have cat.
25/51
5: Pick Probable Phrase Pairs (TM)Nemám žádného psa.
I have no dog.
Viděl kočku.
a cat.He saw
New input: kočku.Nemám
... I don't have cat.
New input: Nemám
I have
26/51
6: So That 𝑛-Grams Probable (LM)Nemám žádného psa.
I have no dog.
Viděl kočku.
a cat.He saw
New input: kočku.Nemám
... I don't have cat.
New input: Nemám
I have
kočku.
a cat.
27/51
Meaning Got Reversed!Nemám žádného psa.
I have no dog.
Viděl kočku.
a cat.He saw
New input: kočku.Nemám
... I don't have cat.
New input: Nemám
I have
kočku.
a cat. ✘
28/51
What Went Wrong?̂
𝑒̂
𝐼
1 = argmax
𝐼,𝑒𝐼
1
𝑝(𝑓𝐽
1 |𝑒𝐼
1)𝑝(𝑒𝐼
1) = argmax
𝐼,𝑒𝐼
1
∏
(̂𝑓,̂𝑒)∈phrase pairs of 𝑓𝐽
1 ,𝑒𝐼
1
𝑝(̂𝑓|̂𝑒)𝑝(𝑒𝐼
1) (12)
• Too strong phrase-independence assumption.
• Phrases do depend on each other.
Here “nemám” and “žádného” jointly express one negation.
• Word alignments ignored that dependence.
But adding it would increase data sparseness.
• Language model is a separate unit.
• 𝑝(𝑒𝐼
1) models the target sentence independently of 𝑓𝐽
1 .
29/51
Redefining 𝑝(𝑒𝐼
1|𝑓𝐽
1 )
What if we modelled 𝑝(𝑒𝐼
1|𝑓𝐽
1 ) directly, word by word:
𝑝(𝑒𝐼
1|𝑓𝐽
1 ) = 𝑝(𝑒1, 𝑒2, … 𝑒𝐼 |𝑓𝐽
1 )
= 𝑝(𝑒1|𝑓𝐽
1 ) ⋅ 𝑝(𝑒2|𝑒1, 𝑓𝐽
1 ) ⋅ 𝑝(𝑒3|𝑒2, 𝑒1, 𝑓𝐽
1 ) …
=
𝐼
∏
𝑖=1
𝑝(𝑒𝑖|𝑒1, … 𝑒𝑖−1, 𝑓𝐽
1 )
(13)
…this is “just a cleverer language model:” 𝑝(𝑒𝐼
1) = ∏𝐼
𝑖=1 𝑝(𝑒𝑖|𝑒1, … 𝑒𝑖−1)
Main Benefit: All dependencies available.
But what technical device can learn this?
30/51
NNs: Universal Approximators
• A neural network with a single hidden layer (possibly huge) can
approximate any continuous function to any precision.
• (Nothing claimed about learnability.)
https://www.quora.com/How-can-a-deep-neural-network-with-ReLU-activations-in-its-hidden-layers-approximate-any-function
31/51
Play with playground.tensorflow.org
−0.43𝑥1 − 0.89𝑥2 + 2.0 > 0
and −0.67𝑥1 + 0.89𝑥2 + 2.1 > 0
and 1.4𝑥1 − 0.067𝑥2 + 2.3 > 0 32/51
A DL “Program” Is Just a Computation…
In fact: 1 tanh(−0.43𝑥1−0.89𝑥2 + 2.0)
+1 tanh(−0.67𝑥1+0.89𝑥2 + 2.1)
+1 tanh(1.4𝑥1−0.067𝑥2 + 2.3)−𝜋/2 > 0 33/51
… with Parameters Guessed Automatically
In fact: 1 tanh(−0.43𝑥1−0.89𝑥2 + 2.0)
+1 tanh(−0.67𝑥1+0.89𝑥2 + 2.1)
+1 tanh(1.4𝑥1−0.067𝑥2 + 2.3)−𝜋/2 > 0 34/51
Perfect Features
1𝑥2
1 + 1𝑥2
2 − 1 < 0
35/51
Bad Features & Low Depth
36/51
Too Complex NN Fails to Learn
37/51
Deep NNs for Image Classification
38/51
Processing Text with NNs
• Map each word to a vector of 0s and 1s (“1-hot repr.”):
cat ↦ (0, 0, … , 0, 1, 0, … , 0)
• Sentence is then a matrix: the cat is on the mat
↑ a 0 0 0 0 0 0
about 0 0 0 0 0 0
… … … … … … …
cat 0 1 0 0 0 0
Vocabulary size: … … … … … … …
1.3M English is 0 0 1 0 0 0
2.2M Czech … … … … … … …
the 1 0 0 0 1 0
… … … … … … …
↓ zebra 0 0 0 0 0 0
Main drawback: No relations, all words equally close/far. 39/51
Processing Text with NNs
• Map each word to a vector of 0s and 1s (“1-hot repr.”):
cat ↦ (0, 0, … , 0, 1, 0, … , 0)
• Sentence is then a matrix: the cat is on the mat
↑ a 0 0 0 0 0 0
about 0 0 0 0 0 0
… … … … … … …
cat 0 1 0 0 0 0
Vocabulary size: … … … … … … …
1.3M English is 0 0 1 0 0 0
2.2M Czech … … … … … … …
the 1 0 0 0 1 0
… … … … … … …
↓ zebra 0 0 0 0 0 0
Main drawback: No relations, all words equally close/far. 40/51
Processing Text with NNs
• Map each word to a vector of 0s and 1s (“1-hot repr.”):
cat ↦ (0, 0, … , 0, 1, 0, … , 0)
• Sentence is then a matrix: the cat is on the mat
↑ a 0 0 0 0 0 0
about 0 0 0 0 0 0
… … … … … … …
cat 0 1 0 0 0 0
Vocabulary size: … … … … … … …
1.3M English is 0 0 1 0 0 0
2.2M Czech … … … … … … …
the 1 0 0 0 1 0
… … … … … … …
↓ zebra 0 0 0 0 0 0
Main drawback: No relations, all words equally close/far. 41/51
Solution: Word Embeddings
• Idea: Map each word to a dense vector.
• Result: 300–2000 dimensions instead of 1–2M.
• The dimensions have no clear interpretation.
• The “embedding” is the mapping.
• Technically, the first layer of NNs for NLP is the matrix that maps 1-hot
input to the first layer.
• Embeddings are trained for each particular task.
• Sentence classification (sentiment analysis, etc.)
• Neural language modelling.
• The famous word2vec (Mikolov et al., 2013):
• CBOW: Predict the word from its four neighbours.
• Skip-gram: Predict likely neighbours given the word.
• End-to-end neural MT.
42/51
Further Compression: Sub-Words
• SMT struggled with productive morphology (>1M wordforms).
nejneobhodpodařovávatelnějšími, Donaudampfschifffahrtsgesellschaftskapitän
• NMT can handle only 30–80k dictionaries.
⇒ Resort to sub-word units.
Orig český politik svezl migranty
Syllables čes ký ⊔ po li tik ⊔ sve zl ⊔ mig ran ty
Morphemes česk ý ⊔ politik ⊔ s vez l ⊔ migrant y
Char Pairs če sk ý ⊔ po li ti k ⊔ sv ez l ⊔ mi gr an ty
Chars č e s k ý ⊔ p o l i t i k ⊔ s v e z l ⊔ m i g r a n t y
BPE 30k český politik s@@ vez@@ l mi@@ granty
BPE (Byte-Pair Encoding) uses 𝑛 most common substrings (incl. frequent words).
43/51
Variable-Length Inputs
Variable-length input can be handled by recurrent NNs:
• Reading one input symbol at a time.
• The same (trained) transformation used every time.
• Unroll in time (up to a fixed length limit).
Tricks needed to train (to avoid “vanishing gradients”):
• LSTM, Long Short-Term Memory Cells (Hochreiter and Schmidhuber, 1997).
• GRU, Gated Recurrent Unit Cells (Chung et al., 2014).
44/51
NNs as Translation Model in SMT
Cho et al. (2014) proposed:
• encoder-decoder architecture and
• GRU unit (name given later by Chung et al. (2014))
• to score variable-length phrase pairs in PBMT.x1 x2 xT
yT' y2 y1
c
Decoder
Encoder
45/51
NMT: Sequence to Sequence
Sutskever et al. (2014) use:
• LSTM RNN encoder-decoder
• to consume
and produce variable-length sentences.
First the Encoder:
46/51
Then the Decoder
Remember: 𝑝(𝑒𝐼
1|𝑓𝐽
1 ) = 𝑝(𝑒1|𝑓𝐽
1 ) ⋅ 𝑝(𝑒2|𝑒1, 𝑓𝐽
1 ) ⋅ 𝑝(𝑒3|𝑒2, 𝑒1, 𝑓𝐽
1 ) …
• Again RNN, producing one word at a time.
• The produced word fed back into the network.
• (Word embeddings in the target language used here.)
47/51
Encoder-Decoder Architecture
https://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-2/
48/51
Ultimate Goal of SMT vs. NMT
Goal of “classical” SMT (e.g. PBMT):
Find minimum translation units ∼ graph partitions:
• such that they are frequent across many sentence pairs.
• without imposing (too hard) constraints on reordering.
• in an unsupervised fashion.
Goal of neural MT:
Avoid minimum translation units. Find NN architecture that
• Reads input in as original form as possible.
• Produces output in as final form as possible.
• Can be optimized end-to-end in practice.
49/51
Summary of the Lecture
• Statistical MT chooses the most probable sentence:̂
𝑒̂
𝐼
1 = argmax𝐼,𝑒𝐼
1
𝑝(𝑒𝐼
1|𝑓𝐽
1 )
• The probability modelled in Bayes’ or Log-Linear decomposition:̂
𝑒̂
𝐼
1 = argmax𝐼,𝑒𝐼
1
𝑝(𝑓𝐽
1 |𝑒𝐼
1)𝑝(𝑒𝐼
1)
or̂ 𝑒̂ 𝐼
1 = argmax𝐼,𝑒𝐼
1
exp(∑𝑀
𝑚=1 𝜆𝑚ℎ𝑚(𝑒𝐼
1, 𝑓𝐽
1 ))
• Phrase-Based MT models 𝑝(𝑓𝐽
1 |𝑒𝐼
1) as product of phrase
translation probabilities in a segmentation 𝑠𝐾
1 : ∏𝐾
𝑘=1 𝑝(̃𝑓𝑘|̃𝑒𝑘)
• Other (ling.-motivated) decompositions or features possible.
• Probabilities estimated from data (parallel/monolingual).
• Neural MT predict word by word; “just a clever LM”.
• Sub-word units, word embeddings, encoder-decoder.
50/51
References
Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. 2014. Learning phrase representations using rnn encoder–decoder for statistical machine translation. In
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages
1724–1734, Doha, Qatar, October. Association for Computational Linguistics.
Junyoung Chung, Çaglar Gülçehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated recurrent
neural networks on sequence modeling. CoRR, abs/1412.3555.
Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural Comput., 9(8):1735–1780,
November.
Adam Lopez. 2009. Translation as weighted deduction. In Proceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009), pages 532–540, Athens, Greece, March. Association for Computational Linguistics.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector
space. CoRR, abs/1301.3781.
Franz Joseph Och. 2002. Statistical Machine Translation: From Single-Word Models to Alignment Templates. Ph.D.
thesis, RWTH Aachen University.
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to Sequence Learning with Neural Networks. In
Advances in neural information processing systems, pages 3104–3112.
51/51


Basic
Sequence-to-Sequence
(with Attention)
Ondřej Bojar
March 12, 2020
NPFL087 Statistical Machine Translation
Charles University
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics unless otherwise stated
Outline
• Basic NN building blocks for NMT.
• Processing Text.
• Neural Language Model.
• Vanilla Sequence-to-Sequence.
• Attention.
Many of the slides based on RANLP 2017 tutorial (Helcl and Bojar, 2017).
1/40
Encoder-Decoder Architecture
https://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-2/
2/40
Basic NN Building Blocks
One Fully Connected Layer
• One fully-connected layer converts an input (column) vector 𝑥
to an output (column) vector ℎ:
ℎ = 𝑓(𝑊 𝑥 + 𝑏), (1)
• 𝑊 is a weight matrix of input columns and output rows,
• 𝑏 a bias vector of length of output,
• 𝑓(⋅) is a non-linearity applied usually elementwise.W
x
b
+f )(
3/40
One Layer tanh(𝑊 𝑥 + 𝑏), 2D→2D
Skew:
𝑊
Transpose:
𝑏
Non-lin.:
tanh
Animation by http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/
4/40
Feed-Forward Neural Network
𝑥
↓ ↓
ℎ1 = 𝑓(𝑊1𝑥 + 𝑏1) ￿
↓↑ ↓ ↑
ℎ2 = 𝑓(𝑊2ℎ1 + 𝑏2) ￿
↓↑ ↓ ↑
⋮ ⋮ ￿
↓↑ ↓ ↑
ℎ𝑛 = 𝑓(𝑊𝑛ℎ𝑛−1 + 𝑏𝑛) ￿
↓↑ ↓ ↑
𝑜 = 𝑔(𝑊𝑜ℎ𝑛 + 𝑏𝑜) ∂𝐸
∂𝑊𝑜
= ∂𝐸
∂𝑜 ⋅ ∂𝑜
∂𝑊𝑜
↓ ↓ ↑
𝐸 = 𝑒(𝑜, 𝑡) → ∂𝐸
∂𝑜
blue: Training item (input 𝑥, output 𝑡), red: Trainable parameters (𝑊1, 𝑏1, … ). 5/40
Four Layers, Disentagling Spirals
Animation by http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/ 6/40
Variable-Length Inputs and Outputs
Variable-length input can be handled by recurrent NNs:
• Processing one input symbol at a time.
• Initial state ℎ0 = (0) (or some sentence representation).
• The same (trained) transformation 𝐴 used every time.
ℎ𝑡 = 𝐴(ℎ𝑡−1, 𝑥𝑡) (2)
• Unroll in time (up to a fixed length limit).
7/40
Vanilla RNN
ℎ𝑡 = tanh (𝑊 [ℎ𝑡−1; 𝑥𝑡] + 𝑏) (3)
[ℎ𝑡−1; 𝑥𝑡] is concatenation of ℎ𝑡−1 and 𝑥𝑡
• Vanishing gradient problem.
• Non-linear transformation always applied.
⇒ Type theory: ℎ𝑡 and ℎ𝑡−1 live in different vector spaces.
8/40
LSTM and GRU Cells for RNN
• LSTM, Long Short-Term Memory Cells (Hochreiter and Schmidhuber, 1997).
• GRU, Gated Recurrent Unit Cells (Chung et al., 2014):
𝑧𝑡 = 𝜎 (𝑊𝑧[ℎ𝑡−1; 𝑥𝑡] + 𝑏𝑧) (4)
𝑟𝑡 = 𝜎 (𝑊𝑟[ℎ𝑡−1; 𝑥𝑡] + 𝑏𝑟) (5)̃
ℎ𝑡 = tanh (𝑊 [𝑟𝑡 ⊙ ℎ𝑡−1; 𝑥𝑡]) (6)
ℎ𝑡 = (1 − 𝑧𝑡) ⊙ ℎ𝑡−1 + 𝑧𝑡 ⊙̃ ℎ𝑡 (7)
• Gates control:
• what to use from input 𝑥𝑡 (GRU: everything),
• what to use from hidden state ℎ𝑡−1 (reset gate 𝑟𝑡),
• what to put into output (update gate 𝑧𝑡)
• Linear “information highway” preserved.
⇒ All states ℎ𝑡 belong to the same vector space. 9/40
Processing Text
From Categorical Words to Numbers
• Map each word to a vector of 0s and 1s (“1-hot repr.”):
cat ↦ (0, 0, … , 0, 1, 0, … , 0)
• Sentence is then a matrix: the cat is on the mat
↑ a 0 0 0 0 0 0
about 0 0 0 0 0 0
… … … … … … …
cat 0 1 0 0 0 0
Vocabulary size: … … … … … … …
1.3M English is 0 0 1 0 0 0
2.2M Czech … … … … … … …
on 0 0 0 1 0 0
… … … … … … …
the 1 0 0 0 1 0
… … … … … … …
↓ zebra 0 0 0 0 0 0
10/40
Sub-Words to Reduce Vocabulary Size
• SMT struggles with productive morphology (>1M wordforms).
nejneobhodpodařovávatelnějšími, Donaudampfschifffahrtsgesellschaftskapitän
• NMT can handle only 30–80k dictionaries.
⇒ Resort to sub-word units.
Orig český politik svezl migranty
Syllables čes ký ⊔ po li tik ⊔ sve zl ⊔ mig ran ty
Morphemes česk ý ⊔ politik ⊔ s vez l ⊔ migrant y
Char Pairs če sk ý ⊔ po li ti k ⊔ sv ez l ⊔ mi gr an ty
Chars č e s k ý ⊔ p o l i t i k ⊔ s v e z l ⊔ m i g r a n t y
BPE 30k český politik s@@ vez@@ l mi@@ granty
BPE (Byte-Pair Encoding, (Sennrich et al., 2016)) or Google’s wordpieces (Wu et al., 2016) and
Tensor2Tensor’s SubwordTextEncoder use 𝑛 most common substrings (incl. frequent words).
11/40
Word (Actually Token) Embeddings
• Idea: Map each token to a dense vector in continuous space.
• Result: 300–2000 dimensions instead of 1–2M.
• The dimensions have no clear interpretation.
• The “embedding” is the mapping.
• Technically, the first layer of NNs for NLP is the matrix that maps 1-hot
input to the first layer.
• Embeddings are trained for each particular task.
• Sentence classification (sentiment analysis, etc.)
• Neural language modelling.
• The famous word2vec (Mikolov et al., 2013):
• CBOW: Predict the word from its four neighbours.
• Skip-gram: Predict likely neighbours given the word.
• End-to-end neural MT.
12/40
Output: Softmax over Vocabulary
Outputs of the RNN are:
1. Projected (scaled up) to the size of the vocabulary 𝑉 ,
2. Normalized with softmax.
⇒ Distribution over all possible target tokens.
𝑙(𝑤)𝑡 = 𝑊𝑙ℎ𝑡 + 𝑏𝑙
𝑝(𝑤)𝑡 = exp 𝑙(𝑤)𝑡
∑𝑤′∈𝑉 exp 𝑙(𝑤′)𝑡
• 𝑙(𝑤)𝑡 = logits/energies for word 𝑤 in time 𝑡
• 𝑊𝑙: weight matrix (hidden state × voc. size)
… this is big.
• Softmax normalization: exp ⋅
∑ exp ⋅
… this is costly.
• Tricks what to do with it
(negative sampling, hierarchical softmax)
– not frequently used 13/40
Neural Language Modeling
RNN Language Model
• Train RNN as a classifier for next words (unlimited history):<s> w 1 w 2 w 3 w 4
p(w 1 ) p(w 2 ) p(w 3 ) p(w 4 ) p(w 5 )
• Can be used:
• To estimate sentence probability / perplexity.
• To sample from the distribution:<s>
~w 1 ~w 2 ~w 3 ~w 4 ~w 5
14/40
Two Views on RNN LM
• RNN is a for loop / functional map over sequential data
• all outputs are conditional distributions
→ probabilistic distribution over sequences of words
𝑃 (𝑤1, … , 𝑤𝑛) =
𝑛
∏
𝑖=1
𝑃 (𝑤𝑖|𝑤𝑖−1, … , 𝑤1)
15/40
Bidirectional RNN for Input<s> x 1 x 2 x 3 x 4
h 1h 0 h 2 h 3 h 4
...
16/40
Bidirectional RNN for Input<s> x 1 x 2 x 3 x 4
h 1h 0 h 2 h 3 h 4
...
• read the input sentence from both sides
16/40
Bidirectional RNN for Input<s> x 1 x 2 x 3 x 4
h 1h 0 h 2 h 3 h 4
...
• read the input sentence from both sides
• concatenate hidden states from each direction
• every ℎ𝑖 stores information about the whole sentence
16/40
Encoder-Decoder
Architecture
Encoder-Decoder Architecture
• exploits the conditional LM scheme
Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. “Sequence to sequence learning with neural networks.” Advances in neural information processing systems. 2014.
17/40
Encoder-Decoder Architecture
• exploits the conditional LM scheme
• two networks
Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. “Sequence to sequence learning with neural networks.” Advances in neural information processing systems. 2014.
17/40
Encoder-Decoder Architecture
• exploits the conditional LM scheme
• two networks
1. a network processing the input sentence into a single vector representation
(encoder)
Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. “Sequence to sequence learning with neural networks.” Advances in neural information processing systems. 2014.
17/40
Encoder-Decoder Architecture
• exploits the conditional LM scheme
• two networks
1. a network processing the input sentence into a single vector representation
(encoder)
2. a neural language model initialized with the output of the encoder
(decoder)
Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. “Sequence to sequence learning with neural networks.” Advances in neural information processing systems. 2014.
17/40
Encoder-Decoder Model – Image<s><s> x 1 x 2 x 3 x 4
~y 1 ~y 2 ~y 3 ~y 4 ~y 5
18/40
Encoder-Decoder Model – Image<s><s> x 1 x 2 x 3 x 4
~y 1 ~y 2 ~y 3 ~y 4 ~y 5
source language input + target language LM
18/40
Encoder-Decoder Model – Code
state = np.zeros(sent_repr_size)
for w in input_words:
input_embedding = source_embeddings[w]
state, _ = enc_cell(state, input_embedding)
last_w = "<s>"
while last_w != "</s>":
last_w_embedding = target_embeddings[last_w]
state, dec_output = dec_cell(state, last_w_embedding)
logits = output_projection(dec_output)
last_w = np.argmax(logits)
yield last_w
19/40
Encoder-Decoder Model – Formal Notation
Data
input tokens (source language) x = (𝑥1, … , 𝑥𝑇𝑥 )
output tokens (target language) y = (𝑦1, … , 𝑦𝑇𝑦 )̂̂̂̂
20/40
Encoder-Decoder Model – Formal Notation
Data
input tokens (source language) x = (𝑥1, … , 𝑥𝑇𝑥 )
output tokens (target language) y = (𝑦1, … , 𝑦𝑇𝑦 )
Encoder
initial state ℎ0 ≡ 0
𝑗-th state ℎ𝑗 = RNNenc(ℎ𝑗−1, 𝑥𝑗) = tanh(𝑈𝑒ℎ𝑗−1 + 𝑊𝑒𝐸𝑒𝑥𝑗 + 𝑏𝑒)
final state ℎ𝑇𝑥̂̂̂̂
20/40
Encoder-Decoder Model – Formal Notation
Data
input tokens (source language) x = (𝑥1, … , 𝑥𝑇𝑥 )
output tokens (target language) y = (𝑦1, … , 𝑦𝑇𝑦 )
Encoder
initial state ℎ0 ≡ 0
𝑗-th state ℎ𝑗 = RNNenc(ℎ𝑗−1, 𝑥𝑗) = tanh(𝑈𝑒ℎ𝑗−1 + 𝑊𝑒𝐸𝑒𝑥𝑗 + 𝑏𝑒)
final state ℎ𝑇𝑥
Decoder
initial state 𝑠0 = ℎ𝑇𝑥
𝑖-th decoder state 𝑠𝑖 = RNNdec(𝑠𝑖−1,̂ 𝑦𝑖−1) = tanh(𝑈𝑑𝑠𝑖−1 + 𝑊𝑑𝐸𝑑̂ 𝑦𝑖−1 + 𝑏𝑑)
𝑖-th word score 𝑡𝑖 = tanh(𝑈𝑜𝑠𝑖 + 𝑊𝑜𝐸𝑑̂ 𝑦𝑖−1 + 𝑏𝑜) (“output projection”)
output̂ 𝑦𝑖 = arg max 𝑉𝑜𝑡𝑖
20/40
Encoder-Decoder: Training Objective
For output word 𝑦𝑖 we have:
• estimated conditional distribution̂ 𝑝𝑖 = exp 𝑡𝑖
∑ exp 𝑡𝑗
(softmax function)̂̂̂̂̂
21/40
Encoder-Decoder: Training Objective
For output word 𝑦𝑖 we have:
• estimated conditional distribution̂ 𝑝𝑖 = exp 𝑡𝑖
∑ exp 𝑡𝑗
(softmax function)
• unknown true distribution 𝑝𝑖, we lay 𝑝𝑖 ≡ 1 [𝑦𝑖]̂̂̂̂̂
21/40
Encoder-Decoder: Training Objective
For output word 𝑦𝑖 we have:
• estimated conditional distribution̂ 𝑝𝑖 = exp 𝑡𝑖
∑ exp 𝑡𝑗
(softmax function)
• unknown true distribution 𝑝𝑖, we lay 𝑝𝑖 ≡ 1 [𝑦𝑖]
Cross entropy ≈ distance of̂ 𝑝 and 𝑝:
ℒ = 𝐻(̂𝑝, 𝑝) = E𝑝 (− loĝ 𝑝)̂̂
21/40
Encoder-Decoder: Training Objective
For output word 𝑦𝑖 we have:
• estimated conditional distribution̂ 𝑝𝑖 = exp 𝑡𝑖
∑ exp 𝑡𝑗
(softmax function)
• unknown true distribution 𝑝𝑖, we lay 𝑝𝑖 ≡ 1 [𝑦𝑖]
Cross entropy ≈ distance of̂ 𝑝 and 𝑝:
ℒ = 𝐻(̂𝑝, 𝑝) = E𝑝 (− loĝ 𝑝) = − ∑
𝑣∈𝑉
𝑝(𝑣) loĝ 𝑝(𝑣) = − loĝ 𝑝(𝑦𝑖)
21/40
Encoder-Decoder: Training Objective
For output word 𝑦𝑖 we have:
• estimated conditional distribution̂ 𝑝𝑖 = exp 𝑡𝑖
∑ exp 𝑡𝑗
(softmax function)
• unknown true distribution 𝑝𝑖, we lay 𝑝𝑖 ≡ 1 [𝑦𝑖]
Cross entropy ≈ distance of̂ 𝑝 and 𝑝:
ℒ = 𝐻(̂𝑝, 𝑝) = E𝑝 (− loĝ 𝑝) = − ∑
𝑣∈𝑉
𝑝(𝑣) loĝ 𝑝(𝑣) = − loĝ 𝑝(𝑦𝑖)
…computing ∂ℒ
∂𝑡𝑖
is quite simple
See https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/
21/40
Encoder-Decoder: Training Objective
For output word 𝑦𝑖 we have:
• estimated conditional distribution̂ 𝑝𝑖 = exp 𝑡𝑖
∑ exp 𝑡𝑗
(softmax function)
• unknown true distribution 𝑝𝑖, we lay 𝑝𝑖 ≡ 1 [𝑦𝑖]
Cross entropy ≈ distance of̂ 𝑝 and 𝑝:
ℒ = 𝐻(̂𝑝, 𝑝) = E𝑝 (− loĝ 𝑝) = − ∑
𝑣∈𝑉
𝑝(𝑣) loĝ 𝑝(𝑣) = − loĝ 𝑝(𝑦𝑖)
…computing ∂ℒ
∂𝑡𝑖
is quite simple
See https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/
…but we expect the model to produce
the exact word at the exact position! 21/40
Implementation: Runtime vs. Training
runtime:̂ 𝑦𝑗 (decoded) × training: 𝑦𝑗 (ground truth)<s> x 1 x 2 x 3 x 4
<s>
~y 1 ~y 2 ~y 3 ~y 4 ~y 5
<s> y 1 y 2 y 3 y 4
loss
22/40
Encoder-Decoder Architecture
Decoding
Greedy Decoding
• In each step, the model computes a distribution over the
vocabulary 𝑉 (given source x, the previous outputs ℎ, and the
model parameters 𝜃).
𝑝(𝑦|ℎ) = 𝑔(x, ℎ, 𝜃)
23/40
Greedy Decoding
• In each step, the model computes a distribution over the
vocabulary 𝑉 (given source x, the previous outputs ℎ, and the
model parameters 𝜃).
𝑝(𝑦|ℎ) = 𝑔(x, ℎ, 𝜃)
• In greedy decoding:
𝑦∗ = argmax
𝑦∈𝑉
𝑝(𝑦|ℎ)
23/40
Greedy Decoding
• In each step, the model computes a distribution over the
vocabulary 𝑉 (given source x, the previous outputs ℎ, and the
model parameters 𝜃).
𝑝(𝑦|ℎ) = 𝑔(x, ℎ, 𝜃)
• In greedy decoding:
𝑦∗ = argmax
𝑦∈𝑉
𝑝(𝑦|ℎ)
• Repeat, until an end-of-sentence symbol (</s>) is decoded.
23/40
Greedy Decoding — cont.
• Pros:
• Fast and memory-efficient
• Gives reasonable results
• Cons:
• We are interested in the most probable sentence:
(𝑦∗)𝑁
𝑖=0 = argmax
(𝑦)𝑁
𝑖=0
𝑝(𝑦0, … , 𝑦𝑁 |ℎ)
• Other methods: better results for the cost of a slow-down.
24/40
Beam Search
• Instead of taking the arg max in every, step, keep a list (or beam)
of 𝑘-best scoring hypotheses.
25/40
Beam Search
• Instead of taking the arg max in every, step, keep a list (or beam)
of 𝑘-best scoring hypotheses.
• Hypothesis = partially decoded sentence → score
25/40
Beam Search
• Instead of taking the arg max in every, step, keep a list (or beam)
of 𝑘-best scoring hypotheses.
• Hypothesis = partially decoded sentence → score
• Hypothesis score 𝜓𝑡 = (𝑦1, 𝑦2 … , 𝑦𝑡) is the probability of the
decoded sentence prefix up to 𝑡-th word.
𝑝(𝑦1, … , 𝑦𝑡|ℎ) = 𝑝(𝑦1|ℎ) ⋅ ⋯ ⋅ 𝑝(𝑦𝑡|𝑦1, … , 𝑦𝑡−1|ℎ)
25/40
Beam Search
• Instead of taking the arg max in every, step, keep a list (or beam)
of 𝑘-best scoring hypotheses.
• Hypothesis = partially decoded sentence → score
• Hypothesis score 𝜓𝑡 = (𝑦1, 𝑦2 … , 𝑦𝑡) is the probability of the
decoded sentence prefix up to 𝑡-th word.
𝑝(𝑦1, … , 𝑦𝑡|ℎ) = 𝑝(𝑦1|ℎ) ⋅ ⋯ ⋅ 𝑝(𝑦𝑡|𝑦1, … , 𝑦𝑡−1|ℎ)
• Rule to compute the score of an extended hypothesis 𝜓𝑡:
𝑝(𝜓𝑡, 𝑦𝑡+1|ℎ) = 𝑝(𝜓𝑡|ℎ) ⋅ 𝑝(𝑦𝑡+1|ℎ)
25/40
Beam Search
• Instead of taking the arg max in every, step, keep a list (or beam)
of 𝑘-best scoring hypotheses.
• Hypothesis = partially decoded sentence → score
• Hypothesis score 𝜓𝑡 = (𝑦1, 𝑦2 … , 𝑦𝑡) is the probability of the
decoded sentence prefix up to 𝑡-th word.
𝑝(𝑦1, … , 𝑦𝑡|ℎ) = 𝑝(𝑦1|ℎ) ⋅ ⋯ ⋅ 𝑝(𝑦𝑡|𝑦1, … , 𝑦𝑡−1|ℎ)
• Rule to compute the score of an extended hypothesis 𝜓𝑡:
𝑝(𝜓𝑡, 𝑦𝑡+1|ℎ) = 𝑝(𝜓𝑡|ℎ) ⋅ 𝑝(𝑦𝑡+1|ℎ)
• Prefers shorter hypotheses → normalization necessary.
25/40
Beam Search — Algorithm
1. Begin with a single empty hypothesis in the beam.
26/40
Beam Search — Algorithm
1. Begin with a single empty hypothesis in the beam.
2. In each time step:
26/40
Beam Search — Algorithm
1. Begin with a single empty hypothesis in the beam.
2. In each time step:
2.1 Extend all hypotheses in the beam by 𝑘 most probable words (we call these
candidate hypotheses).
26/40
Beam Search — Algorithm
1. Begin with a single empty hypothesis in the beam.
2. In each time step:
2.1 Extend all hypotheses in the beam by 𝑘 most probable words (we call these
candidate hypotheses).
2.2 Sort the candidate hypotheses by their score.
26/40
Beam Search — Algorithm
1. Begin with a single empty hypothesis in the beam.
2. In each time step:
2.1 Extend all hypotheses in the beam by 𝑘 most probable words (we call these
candidate hypotheses).
2.2 Sort the candidate hypotheses by their score.
2.3 Put the best 𝑘 hypotheses in the new beam.
26/40
Beam Search — Algorithm
1. Begin with a single empty hypothesis in the beam.
2. In each time step:
2.1 Extend all hypotheses in the beam by 𝑘 most probable words (we call these
candidate hypotheses).
2.2 Sort the candidate hypotheses by their score.
2.3 Put the best 𝑘 hypotheses in the new beam.
2.4 If a hypothesis from the beam reaches the end-of-sentence symbol, we
move it to the list of finished hypotheses.
26/40
Beam Search — Algorithm
1. Begin with a single empty hypothesis in the beam.
2. In each time step:
2.1 Extend all hypotheses in the beam by 𝑘 most probable words (we call these
candidate hypotheses).
2.2 Sort the candidate hypotheses by their score.
2.3 Put the best 𝑘 hypotheses in the new beam.
2.4 If a hypothesis from the beam reaches the end-of-sentence symbol, we
move it to the list of finished hypotheses.
3. Finish (1) at the final time step or (2) all 𝑘-best hypotheses end
with </s>.
26/40
Beam Search — Algorithm
1. Begin with a single empty hypothesis in the beam.
2. In each time step:
2.1 Extend all hypotheses in the beam by 𝑘 most probable words (we call these
candidate hypotheses).
2.2 Sort the candidate hypotheses by their score.
2.3 Put the best 𝑘 hypotheses in the new beam.
2.4 If a hypothesis from the beam reaches the end-of-sentence symbol, we
move it to the list of finished hypotheses.
3. Finish (1) at the final time step or (2) all 𝑘-best hypotheses end
with </s>.
4. Sort the hypotheses by their score and output the best one.
26/40
Attentive
Sequence-to-Sequence
Learning
Main Idea
Vanilla sequence-to-sequence was degrading with sentence length.
Goal of attention:
• Do not force the network to catch long-distance dependencies.
• Use decoder state only for:
• target sentence dependencies (=LM) and
• a as query for the source word sentence
27/40
Inspiration: Neural Turing Machine
• general architecture for
learning algorithmic tasks,
finite imitation of Turing
Machine
28/40
Inspiration: Neural Turing Machine
• general architecture for
learning algorithmic tasks,
finite imitation of Turing
Machine
• needs to address memory
somehow – either by position
or by content
28/40
Inspiration: Neural Turing Machine
• general architecture for
learning algorithmic tasks,
finite imitation of Turing
Machine
• needs to address memory
somehow – either by position
or by content
• in fact does not work well
– it hardly manages simple algorithmic tasks
28/40
Inspiration: Neural Turing Machine
• general architecture for
learning algorithmic tasks,
finite imitation of Turing
Machine
• needs to address memory
somehow – either by position
or by content
• in fact does not work well
– it hardly manages simple algorithmic tasks
• content-based addressing → attention 28/40
Attentive Sequence-to-Sequence Learning
Attention Mechanism
Attention Mechanism<s> x 1 x 2 x 3 x 4
~y i ~y i+1
h 1h 0 h 2 h 3 h 4
...
+
×
α 0
×
α 1
×
α 2
×
α 3
×
α 4
s is i-1 s i+1
+
29/40
Attention Mechanism in Equations (1)
Inputs:
decoder state 𝑠𝑖
encoder states ℎ𝑗 = [⃗⃗⃗⃗⃗⃗⃗⃗⃗ℎ𝑗;⃖⃖⃖⃖⃖⃖⃖⃖⃖ ℎ𝑗] ∀𝑖 = 1 … 𝑇𝑥
Attention energies:
𝑒𝑖𝑗 = 𝑣⊤
𝑎 tanh (𝑊𝑎𝑠𝑖−1 + 𝑈𝑎ℎ𝑗 + 𝑏𝑎)
Attention distribution:
𝛼𝑖𝑗 = exp(𝑒𝑖𝑗)
∑𝑇𝑥
𝑘=1 exp(𝑒𝑖𝑘)
Context vector:
𝑐𝑖 = ∑𝑇𝑥
𝑗=1 𝛼𝑖𝑗ℎ𝑗
30/40
Attention Mechanism in Equations (2)
Decoder state:
𝑠𝑖 = tanh(𝑈𝑑𝑠𝑖−1 + 𝑊𝑑𝐸𝑑̂ 𝑦𝑖−1 + 𝐶𝑐𝑖 + 𝑏𝑑)
Output projection:
𝑡𝑖 = tanh (𝑈𝑜𝑠𝑖 + 𝑊𝑜𝐸𝑑̂ 𝑦𝑖−1 + 𝐶𝑜𝑐𝑖 + 𝑏𝑜)
…context vector is mixed with the hidden state
Output distribution:
𝑝 (𝑦𝑖 = 𝑘 |𝑠𝑖, 𝑦𝑖−1, 𝑐𝑖) ∝ exp (𝑊𝑜𝑡𝑖)𝑘 + 𝑏𝑘
31/40
Attention Visualization
32/40
Attentive Sequence-to-Sequence Learning
Attention vs. Alignment
Attention vs. Alignment
Differences between attention model and word alignment used for
phrase table generation:
33/40
Attention vs. Alignment
Differences between attention model and word alignment used for
phrase table generation:
Attention (NMT) Alignment (SMT)
33/40
Attention vs. Alignment
Differences between attention model and word alignment used for
phrase table generation:
Attention (NMT) Alignment (SMT)
Probabilistic Discrete
33/40
Attention vs. Alignment
Differences between attention model and word alignment used for
phrase table generation:
Attention (NMT) Alignment (SMT)
Probabilistic Discrete
Declarative Imperative
33/40
Attention vs. Alignment
Differences between attention model and word alignment used for
phrase table generation:
Attention (NMT) Alignment (SMT)
Probabilistic Discrete
Declarative Imperative
LM generates LM discriminates
33/40
Attention vs. Alignment
Differences between attention model and word alignment used for
phrase table generation:
Attention (NMT) Alignment (SMT)
Probabilistic Discrete
Declarative Imperative
LM generates LM discriminates
Learnt with translation Prerequisite
33/40
Attention Off by Onedas
Verh¨altnis
zwischen
Obama
und
Netanyahu
ist
seit
Jahren
gespannt
.
the
relationship
between
Obama
and
Netanyahu
has
been
stretched
for
years
. 11
47
81
72
87
93
95
38
21
17
16
14
38
19
33
90
32
26
54
77
12
17
• Attention can appear on the neighbouring token.
Philipp Koehn and Rebecca Knowles (2017). Six Challenges for Neural Machine Translation. NMT workshop.
34/40
Attending to Two at Once
• To benefit from PBMT, append its output to NMT input.
• Standard attentional model will learns to follow both.
Jan Niehues, Eunah Cho, Thanh-Le Ha, and Alex Waibel. 2016. Pre-translation for neural machine translation.
35/40
Image Captioning
Attention over CNN for image classification:
Source: Xu, Kelvin, et al. ”Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.” ICML. Vol. 14. 2015.
36/40
Encoder-Decoder vs.
Attentive Models
Sutskever+ (2014) × Bahdanau+ (2014)
Two key papers on NMT in 2014:
• Bahdanau et al. (2015) Attention model,
• Sutskever et al. (2014) Seq2seq impressive empirical results:
• Made researchers believe NMT is the way to go.
• (Used reversed input.)
Evaluation on WMT14 EN → FR test set:
Model BLEU score
vanilla SMT 33.0
tuned SMT 37.0
Sutskever et al.: reversed 30.6
–”–: ensemble + beam search 34.8
–”–: vanilla SMT rescoring 36.5
Bahdanau’s attention 28.5
37/40
Sutskever+ (2014) × Bahdanau+ (2014)
Two key papers on NMT in 2014:
• Bahdanau et al. (2015) Attention model,
• Sutskever et al. (2014) Seq2seq impressive empirical results:
• Made researchers believe NMT is the way to go.
• (Used reversed input.)
Evaluation on WMT14 EN → FR test set:
Model BLEU score
vanilla SMT 33.0
tuned SMT 37.0
Sutskever et al.: reversed 30.6
–”–: ensemble + beam search 34.8
–”–: vanilla SMT rescoring 36.5
Bahdanau’s attention 28.5 ← Why worse?
37/40
Sutskever+ (2014) × Bahdanau+ (2014)
Sutskever et al. Bahdanau et al.
vocabulary 160k enc, 80k dec 30k both
encoder 4× LSTM, 1,000 units bidi GRU, 2,000
decoder 4× LSTM, 1,000 units GRU, 1,000 units
word embeddings 1,000 dimensions 620 dimensions
training time 7.5 epochs 5 epochs
38/40
Sutskever+ (2014) × Bahdanau+ (2014)
Sutskever et al. Bahdanau et al.
vocabulary 160k enc, 80k dec 30k both
encoder 4× LSTM, 1,000 units bidi GRU, 2,000
decoder 4× LSTM, 1,000 units GRU, 1,000 units
word embeddings 1,000 dimensions 620 dimensions
training time 7.5 epochs 5 epochs
Comparison with Bahdanau’s model size:
method BLEU score
encoder-decoder 13.9
attention model 28.5
38/40
Summary
We discussed:
• Basic building blocks of NN for NMT.
• Fully-connected, RNN, LSTM and GRU.
• Output softmax.
• Neural LM.
• Sequence-to-sequence (two RNNs attached).
• Architecture.
• Training.
• Decoding (Greedy vs. Beam)
• Attention (decoder attends to a mix on encoder states).
39/40
References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align
and translate. In Proceedings of ICLR.
Junyoung Chung, Çaglar Gülçehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated recurrent
neural networks on sequence modeling. CoRR, abs/1412.3555.
Jindřich Helcl and Ondřej Bojar. 2017. Deep Learning in MT / NMT. Tutorial at RANLP 2017, August.
Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural Comput., 9(8):1735–1780,
November.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector
space. CoRR, abs/1301.3781.
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword
units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pages 1715–1725, Berlin, Germany, August. Association for Computational Linguistics.
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to Sequence Learning with Neural Networks. In
Advances in neural information processing systems, pages 3104–3112.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun,
Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser,
Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang,
Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean.
2016. Google’s neural machine translation system: Bridging the gap between human and machine translation. CoRR,
abs/1609.08144.
40/40


Alignment
Ondřej Bojar
March 19, 2020
NPFL087 Statistical Machine Translation
Charles University
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics unless otherwise stated
Outline
• CzEng (http://ufal.mff.cuni.cz/czeng)
• Sources of (Czech-English) Parallel Texts.
• Licensing Issues.
• Impact of Data Type on MT Quality Gain.
• Mining the Web.
• Document Alignment.
• Sentence Alignment.
• Word Alignment.
• IBM Model 1 and the Expectation-Maximization Loop.
• Problems of Word Alignment.
• Tectogrammatical Alignment.
1/44
Overview of Phrase-Based MT
This time around = Nyní
they ’re moving = zareagovaly
even = dokonce ještě
… = …
This time around, they ’re moving = Nyní zareagovaly
even faster = dokonce ještě rychleji
… = …
1. Given parallel word-aligned corpus,
2. Extract phrases consistent with word
alignment,
3. Translate by replacing phrases.
…but how to do 1? 2/44
Data Acquisition
Sources of Texts in CzEng 0.7
Legal texts:
• Acquis Communautaire Parallel Corpus
• The European Constitution proposal from the OPUS corpus
• samples from the Official Journal of the European Union
Stories and Commentaries:
• Readers’ Digest stories
• e-books: Project Gutenberg and Palmknihy.cz and a subset of the Kačenka
parallel corpus
• articles from Project Syndicate
User-supplied data: …not always complete sentences
• Czech localization of KDE and GNOME open-source projects
• user-contributed translations from the Navajo project
3/44
Texts in CzEng 0.7 – Data Sizes
Sentences Tokens
Acquis Communautaire 64.1% 69.0%
Readers’ Digest 8.6% 8.6%
Project Syndicate 6.5% 8.9%
KDE Messages 6.2% 1.9%
GNOME Messages 5.7% 1.9%
Kačenka 4.2% 4.9%
Navajo User Translations 2.3% 2.1%
E-Books 1.2% 1.6%
European Constitution 0.8% 0.7%
Samples from European Journal 0.4% 0.5%
Total 1.4 mil. 21 mil.
Community-supplied data in bold.
4/44
Community-Supplied Data (1/2)
The Navajo Project
• Anonymous contributors correct MT output of Wikipedia texts.
• About 2,000 segments used to be generated each month.
• Manual evaluation of 1,000 randomly selected segments:
Translation Quality Proportion in the Sample
precise, flawless 69.0%
not translated 6.8%
incomplete 6.6%
imprecise 5.8%
precise, almost flawless 4.5%
machine-generated 4.4%
vandalism 2.7%
other 0.2%
5/44
Community-Supplied Data (2/2)
KDE and GNOME Localizations
• Two major open-source software projects,
• Contributors not anonymous ⇒ the quality considerably higher
(almost professional)
• Only rarely full sentences, mostly short system messages and user
interface elements e.g. “OK”, “Yes” or “Delete file”
6/44
Licensing Issues
• Much more data are available on the Internet,
• Only a fraction labelled for reuse.
Tokens Available
Source of Texts and Translation cs en cs en
Community Transl. of Proprietary Texts 19.5M 25.3M 37.8% 41.1%
Professional 21.3M 23.9M 41.2% 38.9%
Proprietary 9.6M 10.9M 18.6% 17.7%
Community 1.2M 1.4M 2.4% 2.3%
Total 51.6M 61.5M 100.0% 100.0%
CzEng 0.7 ≈ Professional + Community sources; in bold
7/44
En→Cs Data in 2008P
X
D
C
Training Data Composition
In-domain
Professional Translation
Community-supplied
with proper copyright
Community-supplied,
copyright unclear Out-of-domain
Professional
Translation
8/44
OOV and PBMT Quality In/Out of Domain1
2
3
4
5
6
7
5 6 7 8 9 10 11 12 13 14
OOV Rate
BLEU
Proprietary
comm, no copy: X
in D, prof.
CC
domain-D
out-of-domain-D
9/44
Community Data Out-of-Domain1
2
3
4
5
6
7
5 6 7 8 9 10 11 12 13 14
OOV Rate
BLEU
Proprietary
comm, no copy: X
in D, prof.
CC
domain-D
out-of-domain-D
C
10/44
Community Data Out-of-Domain1
2
3
4
5
6
7
5 6 7 8 9 10 11 12 13 14
OOV Rate
BLEU
Proprietary
comm, no copy: X
in D, prof.
CC
domain-D
out-of-domain-D
C
CX
11/44
Professional Out-of-Domain1
2
3
4
5
6
7
5 6 7 8 9 10 11 12 13 14
OOV Rate
BLEU
Proprietary
comm, no copy: X
in D, prof.
CC
domain-D
out-of-domain-D
C
CX
P
12/44
Everything Out-of-Domain1
2
3
4
5
6
7
5 6 7 8 9 10 11 12 13 14
OOV Rate
BLEU
Proprietary
comm, no copy: X
in D, prof.
CC
domain-D
out-of-domain-D
C
CX
P
CPX
13/44
Similar Volume of in-Domain: Much Better1
2
3
4
5
6
7
5 6 7 8 9 10 11 12 13 14
OOV Rate
BLEU
Proprietary
comm, no copy: X
in D, prof.
CC
domain-D
out-of-domain-D
C
CX
P
CPX
D
14/44
Additional Data Improve Coverage1
2
3
4
5
6
7
5 6 7 8 9 10 11 12 13 14
OOV Rate
BLEU
Proprietary
comm, no copy: X
in D, prof.
CC
domain-D
out-of-domain-D
C
CX
P
CPX
D
DP
15/44
But Out-of-Domain Can Decrease Quality1
2
3
4
5
6
7
5 6 7 8 9 10 11 12 13 14
OOV Rate
BLEU
Proprietary
comm, no copy: X
in D, prof.
CC
domain-D
out-of-domain-D
C
CX
P
CPX
D
DPDCPX
16/44
Applying Out of Domain? Much Worse.1
2
3
4
5
6
7
5 6 7 8 9 10 11 12 13 14
OOV Rate
BLEU
Proprietary
comm, no copy: X
in D, prof.
CC
domain-D
out-of-domain-D
C
CX
P
CPX
D
DPDCPX
C
D
17/44
More Data → Better Coverage1
2
3
4
5
6
7
5 6 7 8 9 10 11 12 13 14
OOV Rate
BLEU
Proprietary
comm, no copy: X
in D, prof.
CC
domain-D
out-of-domain-D
C
CX
P
CPX
D
DPDCPX
C
D
DP
CX
18/44
…But Not Much Better Quality1
2
3
4
5
6
7
5 6 7 8 9 10 11 12 13 14
OOV Rate
BLEU
Proprietary
comm, no copy: X
in D, prof.
CC
domain-D
out-of-domain-D
C
CX
P
CPX
D
DPDCPX
C
D
DP
CX
DCPX
19/44
CzEng Releases 2006–2020
• Reached 180M million sentence pairs:
• 0.6 cs / 0.7 en gigawords of genuine parallel text (61M sentpairs)
• 2.0 cs / 2.3 en gigawords of synthetic text (127M sentpairs)
Ver. S. Pairs Main Focus Details in
0.5 0.9M Sentence alignment, common format Bojar and Žabokrtský (2006)
0.7 1.0M Used in WMT06 and WMT07 Bojar et al. (2008)
0.9 8.0M Automatic annotation up to t-layer Bojar and Žabokrtský (2009)
– – Sentence-level filtering Bojar et al. (2010)
1.0 15.0M Improving monolingual annotation
through parallel data
Bojar et al. (2012)
1.6 62.5M Processing tools dockered Bojar et al. (2016)
1.7 57.1M Block-level filtering –
2.0 188.0M Filtering + Synthetic data –
20/44
Methods
Mining the Web
Goal: Given two language names, find parallel texts.
• Hervé Saint-Amand’s master’s thesis (Saarbrücken).
• Train language identification on Wikipedia.
• Search for pages in English containing the word česky.
• Bitextor: Esplà-Gomis and Forcada (2010)
• PANACEA tools (http://myexperiment.elda.org/workflows/7)
• Students’ project ParaSite: proof of concept, fixes needed.
Quasi-comparable sources (incl. Wikipedia):
• Texts on the same topic but written independently.
• Can hope to find parallel sentences but no longer segments.
• BUCC workshops 2008–2020: https://comparable.limsi.fr/bucc2020/
• “Lightly supervised training” (Schwenk, 2008) = basis of unsupervised MT. 21/44
Document Alignment Attempted Many Times
Goal: Given bag of texts in two languages, find pairs.
• A project at this very seminar at FJFI: (Jahoda et al., 2007)
• A project at MFF: (Klempová et al., 2009)
• Evaluation suggested that the first step is tricky: finding source URLs.
• Václav Novák (ÚFAL, ∼2009): aligning subtitles.
• Proper minimum pairing algorithm.
• Not generic enough: focus on named entities at the beg. and end only.
• ParaSite: probably good, re-evaluation would be useful.
• Problem: Based on libraries with conflicting licenses (GPL 2.0 vs 3.0).
• Parallel Paragraphs from CommonCrawl (Kúdela et al., 2017)
• Recall 63%, precision 94% when re-aligning shuffled CzEng.
• 149TB of CommonCrawl ⇝ 115k en-cs sentpairs from 2k webdomains.
• Targetted re-crawl would be highly desirable (project suggestion).
• paracrawl.eu large but noisy. Aligns documents, not paragraphs. 22/44
Sentence Alignment
Goal: Given a text in two languages, align sentences.
23/44
From Aligned Documents
24/44
We Want Sentence Alignment
25/44
Sentence Alignment
Goal: Given a text in two languages, align sentences.
Assume: Sentences hardly ever reordered.
• Classical algorithm: Gale and Church (1993).
• Based on similar character length of aligned sentences, no words examined.
• Dynamic-programming search for the best alignment.
• Allows 0 to 2 sentences in a group: 0-1, 1-0, 1-1, 2-1, 1-2, 2-2.
• Several algorithms for English-Czech evaluated by Rosen (2005).
• Nearly perfect alignment possible by a combination of aligners.
• The “standard tool”: Hunalign (Varga et al., 2005).
• Another option: Gargantua (Braune and Fraser, 2010).
Illustration: MT Talk #7 (https://youtu.be/_4lnyoC3mtQ)
26/44
Word Alignment
Goal: Given a sentence in two languages, align words (tokens).
State of the art: GIZA++ (Och and Ney, 2000):
• Unsupervised, only sentence-parallel texts needed.
• Word alignments formally restricted to a function:
src token ↦ tgt token or NULL
• A cascade of models refining the probability distribution:
• IBM1: only lexical probabilities: 𝑃 (kočka = cat)
• IBM2: absolute reordering added (not used in practice now)
• IBM3: adds fertility: 1 word generates several others
• IBM4/HMM: to account for relative reordering
• Only many-to-one links created ⇒ used twice, in both directions. 27/44
IBM Model 1
Lexical probabilities:
• Disregard the position of words in sentences.
• Estimated using Expectation-Maximization Loop.
See the slides by Philipp Koehn for:
• Formulas of both expectation and maximization step.
• The trick in expectation step, swapping sum and product by
rearranging the sum.
• Pseudocode.
Illustration: MT Talk #8 (https://youtu.be/mqyMDLu5JPw)
28/44
The Trick Illustrated
Sum of pairs:
Can be rearranged: = =
29/44
EM Loop in IBM1 Illustration from Bojar (2012)white
white
white
white
house
house
house
house
bílý
dům
white
white
white
white
dog
dog
dog
dog
bílý
pes
black
black
black
black
dog
dog
dog
dog
černý
pes
bílý
černý
dům
pes
Iteration 0
black
dog
house
white
1
black
dog
house
white
2
black
dog
house
white
3
black
dog
house
white
4
black
dog
house
white
p( )→↓|
Σ
Σ
Σ
Σ
Σ
Σ
Σ
Σ
Σ
Σ
Σ
Σ
Σ
Σ
Σ
Σ
30/44
Symmetrization
“Symmetrization” of two GIZA++ runs:
• intersection: high precision, too low recall.
• popular: heuristical (something between intersection and union).
• minimum-weight edge cover (Matusov et al., 2004). 31/44
Popular Symmetrization Heuristic
32/44
Troubles with Word Alignment
• Humans have troubles aligning word for word.
• Mismatch in alignments points 9–18%. (Bojar and Prokopová, 2006)
Top Problematic Words Top Problematic Parts of Speech
English Czech English Czech
361 to 319 , 679 IN 1348 N
259 the 271 se 519 DT 1283 V
159 of 146 v 510 NN 661 R
143 a 112 na 386 PRP 505 P
124 , 74 o 361 TO 448 Z
107 be 61 že 327 VB 398 A
99 it 55 . 310 JJ 280 D
95 that 47 a 245 RB 192 J
33/44
Limits of Automatic W.A.
Baseline Improved
Humans GIZA++ en cs en cs
Problems Problems 14.3 15.5 14.3 15.5
Problems OK 0.1 0.1 0.2 0.1
OK Problems 38.6 35.7 25.2 25.0
OK OK 46.9 48.7 60.4 59.4
Percentage of English (en) and Czech (cs) tokens where the alignment was difficult for
humans and/or for GIZA++. (Humans against each other, GIZA++ against merged
humans.)
• Where GIZA++ had problems, humans often disagreed, too.
• Improving automatic alignment keeps the problematic part intact.
34/44
Partial Fix: “Possible” Alignments
Chinese-English from
DeNero and Klein (2010). 35/44
A Czech-English Example
Nemyslím o o o * - - - - - - - -
, - - - - - - - o - - - -
že - - - - - - - o - - - -
by - - - - - - - o - - - -
se - - - - - - - o - - - -
to - - - - - - - - * - - -
jejich - - - - * - - - - - - -
zákazníkům - - - - - * - - - - - -
moc - - - - - - - - - * * -
líbilo - - - - - - - * - - - -
. - - - - - - - - - - - *
I do think would very
n't their like much
customers .
it
• Two papers
independently published
the same work and on
the same dataset.
• Kruijff-Korbayová et
al. (2006)
• Bojar and Prokopová
(2006)
• The both defined
essentially the same
rules.
36/44
T-Layer to the Rescue
• Only content-bearing words have a node.
• Auxiliary words hidden, dropped pronouns added..
.
#
SENT
já
ACT
myslit PROC
PRED
ten
PAT
jeho
APP
zákazník
ACT
moc
EXT
líbit_se PROC
EFF.
.
.
#11
SENT
I
ACT
not
RHEM
think CPL
PRED
he
APP
customer
ACT
like CPL
PAT
it
PAT
much
MANN
very
EXT NIL
(já) Nemyslím , že by se to jejich I do n’t think their
zákazníkům moc líbilo . customers would like it very much .
37/44
Tectogrammatical Alignment
• Mareček et al. (2008) align t-nodes, not words.
⇒ Auxiliary words do not clutter the task.
• Improves human agreement from 91% to 94.7%.
• Application to phrase-based MT: (Mareček, 2009)
• Improved alignment error rate on content words.
• Minor improvements in BLEU when combined with GIZA++.
• Main use: Extraction of t-lemma dictionaries for e.g. TectoMT.
Main disadvantage:
• Language-dependent.
• Heavy use of tools (tagging, parsing, deep parsing).
38/44
Related: Fraser and Marcu (2007)
• A generative story called “LEAF” divides:
• Source words into classes: head, non-head, deleted.
• Target words into classes: head, non-head, spurious.
• Heads connected across languages, non-heads within languages.
• Probabilities in the generative story learnt unsupervised:
• Starting from GIZA++ outputs.
• Greedy local updates of alignments to increase the likelihood of the data.
Project suggestions: (1) Revive LEAF, (2) Your own NN version of LEAF. 39/44
Using Alignment in PBMT
Phrase extraction based on word alignments is wrong:
• From statistical point of view:
• No link to the decoding, i.e. the use of the phrases in MT.
• Wuebker et al. (2010) run “forced” or “constraint” decoding on the
training data to obtain phrasal alignments.
• The overfitting to long phrases is avoided by “leaving-one-out” (Ney et al., 1995).
• From linguistic point of view:
• Fraser and Marcu (2007) allow for M-to-N non-consecutive translation
units.
• DeNero and Klein (2010) train on manual word alignments and handle
“possible” links specifically.
40/44
Better Translation ⇝ Uglier Ali. (1)
The better (more fluent) translation, the harder to align:
to o * - - - - - - - - - -
get - - * - - - - - - - - -
in - - - - - - - @ O O O -
shape - - - - - - - O O O @ -
for - - - * - - - - - - - -
the - - - - - - o - - - - -
1990s - - - - * * * - - - - -
. - - - - - - - - - - - *
, aby do . let co formě
vstoupila v nejlepší
90 .
41/44
Better Translation ⇝ Uglier Ali. (2)
T-layer to no rescue:.
.
.
#
SENT
teď
TWHEN
zdát_se PROC
PRED
&Gen;
ACT
tento
RSTR
trasa
ACT
začít PROC
PAT
fungovat PROC
PAT
&Cor;
ACT
až
RHEM
leden
TWHEN.
.
.
#4
SENT
&Gen;
ACT
now
TWHEN NIL
that
RSTR
route
PAT
not
RHEM
expect CPL
PRED
&Cor;
ACT
begin CPL
EFF
Jan
TTILL
Teď se zdá , že tyto trasy Now , those routes
začnou fungovat až v lednu . are n’t expected to begin until Jan . 42/44
Summary
• Paralel data are vital for MT.
The more and better, the better.
• Several projects for document alignment.
Project suggestion: Targeted re-crawl based on Kúdela et al. (2017).
• Sentence alignment “solved”.
• Word alignment ill-defined but used to be very important.
Plus all the funny heuristics…
• Beyond word alignment:
• Phrase alignment never got wide-spread; too tied to PBMT anyway.
• T-Alignment costly (T-layer needed).
• Project suggestion: NN LEAF.
43/44
References
Ondřej Bojar and Magdalena Prokopová. 2006. Czech-English Word Alignment. In Proceedings of the Fifth
International Conference on Language Resources and Evaluation (LREC 2006), pages 1236–1239. ELRA, May.
Ondřej Bojar and Zdeněk Žabokrtský. 2006. CzEng: Czech-English Parallel Corpus, Release version 0.5. Prague
Bulletin of Mathematical Linguistics, 86:59–62.
Ondřej Bojar and Zdeněk Žabokrtský. 2009. CzEng 0.9: Large Parallel Treebank with Rich Annotation. Prague Bulletin
of Mathematical Linguistics, 92:63–83.
Ondřej Bojar, Miroslav Janíček, Zdeněk Žabokrtský, Pavel Češka, and Peter Beňa. 2008. CzEng 0.7: Parallel Corpus
with Community-Supplied Translations. In Proceedings of the Sixth International Language Resources and Evaluation
(LREC’08), Marrakech, Morocco, May. ELRA.
Ondřej Bojar, Adam Liška, and Zdeněk Žabokrtský. 2010. Evaluating Utility of Data Sources in a Large Parallel
Czech-English Corpus CzEng 0.9. In Proceedings of the Seventh International Language Resources and Evaluation
(LREC’10), pages 447–452, Valletta, Malta, May. ELRA, European Language Resources Association.
Ondřej Bojar, Zdeněk Žabokrtský, Ondřej Dušek, Petra Galuščáková, Martin Majliš, David Mareček, Jiří Maršík, Michal
Novák, Martin Popel, and Aleš Tamchyna. 2012. The Joy of Parallelism with CzEng 1.0. In Proceedings of the Eighth
International Language Resources and Evaluation Conference (LREC’12), pages 3921–3928, Istanbul, Turkey, May.
ELRA, European Language Resources Association.
Ondřej Bojar, Ondřej Dušek, Tom Kocmi, Jindřich Libovický, Michal Novák, Martin Popel, Roman Sudarikov, and
Dušan Variš. 2016. CzEng 1.6: Enlarged Czech-English Parallel Corpus with Processing Tools Dockered. In Petr Sojka,
Aleš Horák, Ivan Kopeček, and Karel Pala, editors,
Text, Speech, and Dialogue: 19th International Conference, TSD 2016, number 9924 in Lecture Notes in Computer
Science, pages 231–238, Cham / Heidelberg / New York / Dordrecht / London. Masaryk University, Springer
International Publishing. 44/44


Phrase-Based MT
Ondřej Bojar
March 26, 2020
NPFL087 Statistical Machine Translation
Charles University
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics unless otherwise stated
Outline
• PBMT Overview.
• Reminder: Log-linear model.
• PBMT Model.
• Features Used.
• Traditional PBMT “Pipeline”
• Translating with PBMT (Decoding)
• Translation Options and Stack-Based Beam Search
• MT is NP-Hard.
• Pruning, Future Cost Estimation.
• Local and Non-Local Features.
• Minimum Error-Rate Training.
• Moses as the implementation.
1/26
Overview: Phrase-Based MT
This time around = Nyní
they ’re moving = zareagovaly
even = dokonce ještě
… = …
This time around, they ’re moving = Nyní zareagovaly
even faster = dokonce ještě rychleji
… = …
Phrase-based MT: choose such segmentation
of input string and such phrase “replacements”
to make the output sequence “coherent”
(3-grams most probable).
2/26
Reminder: Log-Linear Model
• 𝑝(𝑒𝐼
1|𝑓𝐽
1 ) is modelled as a weighted combination of models, called
“feature functions”: ℎ1(⋅, ⋅) … ℎ𝑀 (⋅, ⋅)
𝑝(𝑒𝐼
1|𝑓𝐽
1 ) = exp(∑𝑀
𝑚=1 𝜆𝑚ℎ𝑚(𝑒𝐼
1, 𝑓𝐽
1 ))
∑𝑒′𝐼′
1
exp(∑𝑀
𝑚=1 𝜆𝑚ℎ𝑚(𝑒′𝐼′
1 , 𝑓𝐽
1 )) (1)
• The constant denominator not needed in maximization:̂
𝑒̂
𝐼
1 = argmax𝐼,𝑒𝐼
1
exp(∑𝑀
𝑚=1 𝜆𝑚ℎ𝑚(𝑒𝐼
1, 𝑓𝐽
1 ))
∑𝑒′𝐼′
1
exp(∑𝑀
𝑚=1 𝜆𝑚ℎ𝑚(𝑒′𝐼′
1 , 𝑓𝐽
1 ))
= argmax𝐼,𝑒𝐼
1
exp(∑𝑀
𝑚=1 𝜆𝑚ℎ𝑚(𝑒𝐼
1, 𝑓𝐽
1 ))
(2)
3/26
Phrase-Based Translation Model
• Captures the basic assumption of phrase-based MT:
1. Segment source sentence 𝑓𝐽
1 into 𝐾 phrases̃ 𝑓1 …̃ 𝑓𝐾.
2. Translate each phrase independently:̃ 𝑓𝑘 →̃ 𝑒𝑘.
3. Concatenate translated phrases (with possible reordering 𝑅):̃
𝑒𝑅(1) …̃ 𝑒𝑅(𝐾)
• In theory, the segmentation 𝑠𝐾
1 is a hidden variable in the maximization, we should be
summing over all segmentations: (Note the three args in ℎ𝑚(⋅, ⋅, ⋅) now.)̂
𝑒̂
𝐼
1 = argmax𝐼,𝑒𝐼
1
∑𝑠𝐾
1
exp(∑𝑀
𝑚=1 𝜆𝑚ℎ𝑚(𝑒𝐼
1, 𝑓𝐽
1 , 𝑠𝐾
1 )) (3)
• In practice, the sum is approximated with a max (the biggest element only):̂
𝑒̂
𝐼
1 = argmax𝐼,𝑒𝐼
1
max𝑠𝐾
1 exp(∑𝑀
𝑚=1 𝜆𝑚ℎ𝑚(𝑒𝐼
1, 𝑓𝐽
1 , 𝑠𝐾
1 )) (4)
4/26
Commonly Used Features of PBMT
• Phrase translation probability:
ℎPhr(𝑓𝐽
1 , 𝑒𝐼
1, 𝑠𝐾
1 ) = log ∏𝐾
𝑘=1 𝑝(̃𝑓𝑘|̃𝑒𝑘) where 𝑝(̃𝑓𝑘|̃𝑒𝑘) = count(̃𝑓,̃ 𝑒)
count(̃𝑒)
⇒ Are all used units̃ 𝑓 ↔̃ 𝑒 likely translations?
• Word count/penalty: ℎwp(𝑒𝐼
1, ⋅, ⋅) = 𝐼
⇒ Do we prefer longer or shorter output?
• Phrase count/penalty: ℎpp(⋅, ⋅, 𝑠𝐾
1 ) = 𝐾
⇒ Do we prefer translation in more or fewer less-dependent bits?
• Reordering model: different basic strategies (Lopez, 2009)
⇒ Which source spans can provide continuation at a moment?
• 𝑛-gram LM: ℎLM(⋅, 𝑒𝐼
1, ⋅) = log ∏𝐼
𝑖=1 𝑝(𝑒𝑖|𝑒𝑖−1
𝑖−𝑛+1)
⇒ Is output 𝑛-gram-wise coherent?
5/26
Traditional PBMT “Pipeline”
“Training the Translation Model”
1. Find relevant parallel texts.
2. Align at the level of sentences.
3. Align at the level of words.
4. Extract translation units, with scores (co-oc. stats.).
(Language Model similar, “simple” words co-oc. stats, no alignment.)
“Tuning” (“MERT”) = Actual training in the ML sense
5. Identify TM/LM/other model component weights.
Translation: = Inference in the ML sense
6. Decompose input into known units.
7. Search for best combinations of units. 6/26
Estimating Phrase Translation Probs
The most important feature: phrase-to-phrase translation:
ℎPhr(𝑓𝐽
1 , 𝑒𝐼
1, 𝑠𝐾
1 ) = log
𝐾
∏
𝑘=1
𝑝(̃𝑓𝑘|̃𝑒𝑘) (5)
The conditional probability of phrasẽ 𝑓𝑘 given phrasẽ 𝑒𝑘 is estimated from relative
frequencies:
𝑝(̃𝑓𝑘|̃𝑒𝑘) = count(̃𝑓,̃ 𝑒)
count(̃𝑒) (6)
• count(̃𝑓,̃ 𝑒) is the number of co-occurrences of a phrase pair (̃𝑓,̃ 𝑒) that are consistent with
the word alignment
• count(̃𝑒) is the number of occurrences of the target phrasẽ 𝑒 in the training corpus.
• ℎPhr usually used twice, in both directions: 𝑝(̃𝑓𝑘|̃𝑒𝑘) and 𝑝(̃𝑒𝑘|̃𝑓𝑘) 7/26
“Training” = Phrase Extraction
This time around = Nyní
they ’re moving = zareagovaly
even = dokonce ještě
… = …
This time around, they ’re moving = Nyní zareagovaly
even faster = dokonce ještě rychleji
… = …
Extract all phrases (up to max-phrase-len)
consistent with the word alignment.
• Long and short.
• Overlapping in all ways.
Score them (Eq. 6) ⇒ Phrase Table 8/26
Phrase Table in Moses
Given parallel training corpus, phrases are extracted and scored:
in europa ||| in europe ||| 0.829007 0.207955 0.801493 0.492402 2.718
europas ||| in europe ||| 0.0251019 0.066211 0.0342506 0.0079563 2.718
in eu ||| in europe ||| 0.018451 0.00100126 0.0319584 0.0196869 2.718
The scores are: (𝜙(⋅) = log 𝑝(⋅))
• phrase translation probabilities: 𝜙phr(𝑓|𝑒) and 𝜙phr(𝑒|𝑓)
• lexical weighting: 𝜙lex(𝑓|𝑒) and 𝜙lex(𝑒|𝑓) (Koehn, 2003)
𝜙lex(𝑓|𝑒) = log max
𝑎∈alignments
of (𝑓,𝑒)
|𝑓|
∏
𝑖=1
1
|{𝑗|(𝑖, 𝑗) ∈ 𝑎| ∑
∀(𝑖,𝑗)∈𝑎
𝑝(𝑓𝑖|𝑒𝑗) (7)
• phrase penalty (always 𝑒1 = 2.718)
9/26
Translation with PBMT
Translation with phrase-based model has two main stages:
1. Translation Options Preparation.
• Search the phrase table for all phrases applicable to the input sentence.
2. Decoding (Main Search).
• Gradual hypothesis expansion.
• Output produced left-to-right.
• Input consumed in any order.
10/26
Stage 1: Translation Optionshe
er geht ja nicht nach hause
it
, it
, he
is
are
goes
go
yes
is
, of course
not
do not
does not
is not
after
to
according to
in
house
home
chamber
at home
not
is not
does not
do not
home
under house
return home
do not
it is
he will be
it goes
he goes
is
are
is after all
does
to
following
not after
not to
not
is not
are not
is not a
11/26
Stage 2: Decoding (Beam Search)er geht ja nicht nach hauseconsult phrase translation table for all input phrases
12/26
Stage 2: Decoding (Beam Search)er geht ja nicht nach hauseinitial hypothesis: no input words covered, no output produced
13/26
Stage 2: Decoding (Beam Search)er geht ja nicht nach hause
arepick any translation option, create new hypothesis
14/26
Stage 2: Decoding (Beam Search)er geht ja nicht nach hause
are
it
hecreate hypotheses for all other translation options
15/26
Stage 2: Decoding (Beam Search)er geht ja nicht nach hause
are
it
he goes
does not
yes
go
to
home
homealso create hypotheses from created partial hypothesis
16/26
Stage 2: Decoding (Beam Search)er geht ja nicht nach hause
are
it
he goes
does not
yes
go
to
home
homebacktrack from highest scoring complete hypothesis
17/26
Interlude: MT is NP-Hard (1/2)
• Translation options lead to exponentially many hypotheses.
• Indeed, MT is NP-hard for at least two reasons:
• Finding the best word ordering.
• Covering with multi-word units.
• Remember the NP-hardness proof strategy:
• Use MT as a black box to solve an NP-complete task.
With a 2-gram language model, find-
ing the best word ordering solves the
Hamilton Circuit or Travelling Sales-
man Problem. (Knight, 1999)
18/26
Interlude: MT is NP-Hard (2/2)
Selecting a set of multi-word
translations to cover the whole
sentence solves Minimum Set Cover
Problem. (Knight, 1999)
The sentence is:
(However, she cooked and left.)
19/26
Fighting the Complexity
See slides 17–32 by Barry Haddow.
• Hypothesis Recombination.
• Stack-based Pruning.
• Future Cost Estimation.
20/26
Local and Non-Local FeaturesWord penalty
Peter left for home .
Petr odešel domů .
Bigram log. prob.
1,0 2,0 1,0
Phrase penalty 1,0 1,0 1,0
Phrase log. prob. 0,0 -0,69 -1,39
Total
4,0
3,0
-2,08
-2,50 -3,61 -0,39 -10,59
Weight
-0,5
-1,0
2,0
1,0
Weighted
-2,0
-3,0
-4,16
-10,59
Total -19,75
◁
-4,02
◁
-0,08
• Local features decompose along hypothesis construction.
• Phrase- and word-based features.
• Non-local features span the boundaries (e.g. LM). 21/26
Weight Optimization: MERT LoopCurrent weights
1 1 1
Word Translation
Language Model
Phrase Translation
Translate input
Hypothesis \ Weight 1 1 1
Mluvíme nahoru ! 2 0 2
Nahlas ! 0 2 1 3
Mluv nahlas ! 1 1 2 4
Prosím mluvte nahlas . 1 2 3
Word Translation
Language Model
Phrase Translation
Weighter internal score
0
2
1
3
Evaluate candidates using an external score.
External score
2 1
2 0 4 0
0 2 1 7 2
1 1 2 7 1
1 2 8 3
Word Translation
Language Model
Phrase Translation
Weighted internal score
External score
Find new
weights for
a better
match of
external and
internal score.
Weights same?
Stop.
Weights differ?
Loop.
Speak up !
0
0 0
3
0
Minimum Error Rate Training (Och, 2003)
22/26
Effects of Weightsdojít k jinému roz
dojít k jinému rozsud
dojít k jinému rozsudku
dojít k jinému rozsudku , | p
dojít k jinému rozsudku , | poro
dojít k jinému rozsudku , | porovn
dojít k jinému rozsudku , | porovnává
pivovarníci , dvojčata , balírny , Vikingové
dojít k jinému rozsudku , | ventilace , klimati
( od našeho zvláštního zpravodaje ) - | je . . je . .
verdikt | ještě není konečný , | a soud | bude | proj
rozsudek | ještě není konečný | , | soud | s | Tymošen
rozsudek | soudu | , | tak | se | proti | Tymošenkové | .
rozsudek | soudu | , | že | se | proti | Tymošenkové | .
rozsudek | soudu | , | že | se | proti | Tymošenkové | .
i | přesto | , | že | si | v | Tymošenkové | .
i | přesto | , | že | si | v | Tymošenkové | .
i | přesto | , | že | se | proti | Tymošenkové | .
na tom | je | , | že | se | s | Tymošenkovou | .
na tom | je | , | že | se | s | Tymošenkovou | .
na tom | je | , | že | se | s | Tymošenkovou | .1
Language Model Score
-1
verdikt | ještě není konečný , | a soud | ...
verdikt | ještě není | konečný | , | soud | ...
verdikt | ještě není | konečný | , | soud | ...
verdikt | ještě není | konečný | , | soud | ...
verdikt | je | zatím | poslední | ; | soud | ...
verdikt | je | zatím | poslední | ; | soud | ...
verdikt | je | zatím | poslední | ; | soud | ...
jeho | verdikt | je | zatím | poslední | ; | soud | ...
na | verdikt | je | to | ještě | závěrečné | ; | ten | ...
na | verdikt | je | to | ještě | závěrečné | ; | ten | ...
0
1 Phrase Penalty
• Higher phrase penalty chops sentence into more segments.
• Too strong LM weight leads to words dropped.
• Negative LM weight leads to obscure wordings. 23/26
Moses Decoder
• http://www.statmt.org/moses
• Moses Installation Tutorial.
24/26
Phrase-Based MT Summary
Phrase-based MT:
• is a log-linear model
• decomposes sentence into contiguous phrases (=MTU)
• assumes phrases relatively independent of each other
• search has two parts:
• lookup of all relevant translation options
• stack-based beam search, gradually expanding hypotheses
To train a PBMT system:
1. Align words.
2. Extract (and score) phrases consistent with word alignment.
3. Optimize weights (MERT).
Best implementation: Moses Decoder. 25/26
References
Kevin Knight. 1999. Decoding complexity in word-replacement translation models. Comput. Linguist., 25(4):607–615.
Philipp Koehn. 2003. Noun Phrase Translation. Ph.D. thesis, University of Southern California.
Adam Lopez. 2009. Translation as weighted deduction. In Proceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009), pages 532–540, Athens, Greece, March. Association for Computational Linguistics.
Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proc. of the Association
for Computational Linguistics, Sapporo, Japan, July 6-7.
26/26


Morphology in MT
Ondřej Bojar
April 2, 2020
NPFL087 Statistical Machine Translation
Charles University
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics unless otherwise stated
Outline
• Problems caused by rich morphology.
• Morphological richness of Czech.
• Combinatorial explosion of Czech word forms.
• Margin for improvement in BLEU.
• Morphology in PBMT:
• Factored PBMT.
• Reverse self-training.
• Morphology in NMT.
• Subword units, BPE.
1/38
Morphological Richness (in Czech)
Czech English
Rich morphology ≥ 4,000 tags possible 50 used
≥ 2,300 tags seen
Word order free rigid
News Commentary Corpus Czech English
Sentences 55,676
Tokens 1.1M 1.2M
Vocabulary (word forms) 91k 40k
Vocabulary (lemmas) 34k 28k
Czech tagging and lemmatization: Hajič and Hladká (1998)
English tagging (Ratnaparkhi, 1996) and lemmatization (Minnen et al., 2001). 2/38
Morphological Explosion in Czech
MT chooses output words in a form:
• Czech nouns and adjs.: 7 cases, 4 genders, 3 numbers, …
• Czech verbs: gender, number, aspect (im/perfective), …
I saw two green striped cats .
já pila dva zelený pruhovaný kočky .
pily dvě zelená pruhovaná koček
… dvou zelené pruhované kočkám
viděl dvěma zelení pruhovaní kočkách
viděla dvěmi zeleného pruhovaného kočkami
… zelených pruhovaných
uviděl zelenému pruhovanému
uviděla zeleným pruhovaným
… zelenou pruhovanou
viděl jsem zelenými pruhovanými
viděla jsem … …
3/38
Morphological Explosion Elsewhere
Compounding in German:
• Rindfleischetikettierungsüberwachungsaufgabenübertragungs-
gesetz.
“beef labelling supervision duty assignment law”
Agglutination in Hungarian or Finnish:
istua “to sit down” (istun = “I sit down”)
istahtaa “to sit down for a while”
istahdan “I’ll sit down for a while”
istahtaisin “I would sit down for a while”
istahtaisinko “should I sit down for a while?”
istahtaisinkohan “I wonder if I should sit down for a while”
4/38
Margin: Lemmatized BLEU
• Lemmatized BLEU:
• Lemmatized MT output against lemmatized references.
• Does not penalize errors in word forms.
⇒ An indication of achievable BLEU score.
English→Czech phrase-based translation:
PCEDT Project Syndicate
Regular BLEU, lowercase 25.2 ~12
Lemmatized BLEU 33.6 ~20
• Margin for improvement: ~8 points in both experiments.
5/38
Morphology in
Phrase-Based MT
PBMT Main ComponentsInput Global Search
for sentence with highest probability Output
Parallel Texts
Translation Model
Monolingual Texts
Language Model
6/38
LM over Forms Insufficient
Possible translations differring in morphology:
two green striped cats
dvou zelená pruhovaný kočkách ← garbage
dva zelené pruhované kočky ← 3grams ok, 4gram bad
dvě zelené pruhované kočky ← correct nominative/accusative
dvěma zeleným pruhovaným kočkám ← correct dative
• 3-gram LM too weak to ensure agreement.
• 3-gram LM possibly already too sparse!
7/38
Explicit Morphological Target Factor
• Add morphological tag to each output token:
two green striped cats
dvou zelená pruhovaný kočkách ← garbage
fem-loc neut-acc masc-nom-sg fem-loc
dva zelené pruhované kočky ← 3-grams ok, 4-gram bad
masc-nom masc-nom masc-nom
fem-nom fem-nom fem-nom
dvě zelené pruhované kočky ← correct nominative/accusative
fem-nom fem-nom fem-nom fem-nom
fem-acc fem-acc fem-acc fem-acc
dvěma zeleným pruhovaným kočkám ← correct dative
fem-dat fem-dat fem-dat fem-dat
8/38
Advantages of Explicit Morphology for LM
• LM over morphological tags generalizes better.
• p(dvě kočkách) < p(dvě kočky) …surely
But we would need to see all combinations of pruhovaný and kočka!
⇒ Better to ask if p(fem-nom fem-loc) < p(fem-nom fem-nom)
which is trained on any feminine adj+noun.
• But still does not solve everything.
• p(dvě zelené) ≷ p(dva zelené) … bad question anyway!
Not solved by asking if p(fem-nom fem-nom) ≷ p(masc-nom masc-nom).
• Tagset size smaller than vocabulary.
⇒ can afford e.g. 7-grams:
p(masc-nom fem-nom fem-nom) < p(fem-nom fem-nom fem-nom)
9/38
Motivation from Translation Model
Availability of translations of “knee caps” in parallel data:
Case Surface form 50K 500K 5M 50M
nom čéšky • • • •
gen čéšek – • • •
dat čéškám – – • •
acc čéšky ∘ ∘ • •
voc čéšky ∘ ∘ ∘ ∘
loc čéškách – • • •
instr čéškami – – – •
⇒ You need to have 50M parallel sentences to translate:
“What’s wrong with my knee caps?”
“•” … the word was seen in the particular case,
“∘” … the surface form was seen but in a different case.
Reproduced from Huck et al. (2017b). 10/38
Advantages of Explicit Morphology for TM
Main idea:
• separate translation of lemmas and morphology:
J src lemma K ⇒ J tgt lemma K
J src morphology K ⇒ J tgt morphology K
• generate target forms based on the lemma and morphology:
J tgt lemma K⇒ J tgt surface form K
J tgt morphology K
11/38
Factored Phrase-Based MT
• Both input and output words can have more factors.
• Arbitrary number and order of:
Mapping steps (→)
Translate (phrases of) source factors to target factors.
two green → dvě zelené
Generation steps (↓)
src tgt
𝑓1 𝑒1 +LM
𝑓2 𝑒2
Generate target factors from target factors.
dvě → fem-nom; dva → masc-nom
⇒ Ensures “vertical” coherence.
Target-side language models (+LM)
Applicable to various target-side factors.
⇒ Ensures “horizontal” coherence. (Koehn and Hoang, 2007)
12/38
Translation Process in Factored PBMT
Input: (knee caps, knee cap, Adj NN-plur)
1. Translation step: lemma ⇒ lemma:
(?, čéška, ?)
2. Generation step: lemma ⇒ morphological tag
(?, čéška, N1-sg), (?, čéška, N2-sg), (?, čéška, N1-pl), (?, čéška, N2-pl), …
3. Translation step: morphological tag ⇒morphological tag
… This reorders the options, so that plural is more likely:
(?, čéška, N1-pl), (?, čéška, N2-pl), (?, čéška, N1-sg), (?, čéška, N2-sg), …
4. Generation step: lemma, morphological tag ⇒ surface form
(čéšky, čéška, N1-pl), (čéšek, čéška, N2-pl), …, (čéškami, čéška, N7-pl)
This and several following slides reuse slides by Philipp Koehn, MT Marathon 2009. 13/38
Factored PBMT Model
• Extension of the phrase-based model:
1. Phrase Extraction for mapping steps.
2. Extraction for generation steps.
3. Decoding.
• Each step simply brings one or more feature functions:
• Fits nicely into the log-linear model,
• Weights trained by the discriminative training (MERT).
• The order of the operations is defined in configuration.
14/38
Factored Phrase Extraction (1/3)
As in standard phrase-based MT:
1. Run sentence and word alignment,
Extract all phrases consistent with word alignment.natürlich
hat
john
spass
am
spiel
naturally
john
has
fun
with
the
game
⇒ Extracted: natürlich hat john → naturally john has
15/38
Factored Phrase Extraction (2/3)
As in standard phrase-based MT:
1. Run sentence and word alignment,
2. Extract all phrases consistent with word alignment.natürlich
hat
john
spass
am
spiel
naturally
john
has
fun
with
the
game
⇒ Extracted: natürlich hat john → naturally john has
16/38
Factored Phrase Extraction (3/3)
As in standard phrase-based MT:
1. Run sentence and word alignment,
2. Extract same phrases, just another factor from each word.ADV
V
NNP
NN
P
NN
ADV
NNP
V
NN
P
DET
NN
⇒ Extracted: ADV V NNP → ADV NNP V
17/38
The Benefit Illustrated Once Moreold | old | ADJ
man | man | NN
starého | starý | AAMS4
pána | pán | NNMS4
black | black | ADJ
dog | dog | NN
černému | černý | AAMS3
psovi | pes | NNMS3
18/38
The Benefit Illustrated Once Moreold | old | ADJ
man | man | NN
starého | starý | AAMS4
pána | pán | NNMS4
black | black | ADJ
dog | dog | NN
černému | černý | AAMS3
psovi | pes | NNMS3
starého pána
starého
pána
...
černému psovi
černému
psovi
...
old man
old
man
...
black dog
black
dog
...
=
=
=
=
=
=
Instead of one phrase table based on word forms:
18/38
The Benefit Illustrated Once Moreold | old | ADJ
man | man | NN
starého | starý | AAMS4
pána | pán | NNMS4
black | black | ADJ
dog | dog | NN
černému | černý | AAMS3
psovi | pes | NNMS3
starého pána
starého
pána
...
černému psovi
černému
psovi
...
old man
old
man
...
black dog
black
dog
...
=
=
=
=
=
=
Instead of one phrase table based on word forms:
old man
old
man
...
black dog
black
dog
=
=
=
=
=
=
We extract separately a table of lemmas and a table of tags:
starý pán
starý
pán
...
černý pes
černý
pes
18/38
The Benefit Illustrated Once Moreold | old | ADJ
man | man | NN
starého | starý | AAMS4
pána | pán | NNMS4
black | black | ADJ
dog | dog | NN
černému | černý | AAMS3
psovi | pes | NNMS3
starého pána
starého
pána
...
černému psovi
černému
psovi
...
old man
old
man
...
black dog
black
dog
...
=
=
=
=
=
=
Instead of one phrase table based on word forms:
old man
old
man
...
black dog
black
dog
=
=
=
=
=
=
We extract separately a table of lemmas and a table of tags:
starý pán
starý
pán
...
černý pes
černý
pes
AAMS4 NNMS4
AAMS4
NNMS4
...
AAMS3 NNMS3
AAMS3
NNMS3
ADJ NN
ADJ
NN
...
ADJ NN
ADJ
NN
=
=
=
=
=
=
We extract separately a table of lemmas and a table of tags:
18/38
The Benefit Illustrated Once Moreold | old | ADJ
man | man | NN
starého | starý | AAMS4
pána | pán | NNMS4
black | black | ADJ
dog | dog | NN
černému | černý | AAMS3
psovi | pes | NNMS3
starého pána
starého
pána
...
černému psovi
černému
psovi
...
old man
old
man
...
black dog
black
dog
...
=
=
=
=
=
=
Instead of one phrase table based on word forms:
old man
old
man
...
black dog
black
dog
=
=
=
=
=
=
We extract separately a table of lemmas and a table of tags:
starý pán
starý
pán
...
černý pes
černý
pes
AAMS4 NNMS4
AAMS4
NNMS4
...
AAMS3 NNMS3
AAMS3
NNMS3
ADJ NN
ADJ
NN
...
ADJ NN
ADJ
NN
=
=
=
=
=
=
We extract separately a table of lemmas and a table of tags:
black dog=černý pes
AAMS4 NNMS4 ADJ NN=
18/38
Factored Phrase-Based MT
See the following slides by Philipp Koehn (Fri Jan 30, 2009, pp. 49–75):
• Decoding
• Reminder of standard phrase-based decoding
• Factored model decoding
• Experiments
• esp. Alternative decoding paths
19/38
48
Translation optionshe
er geht ja nicht nach hause
it
, it
, he
is
are
goes
go
yes
is
, of course
not
do not
does not
is not
after
to
according to
in
house
home
chamber
at home
not
is not
does not
do not
home
under house
return home
do not
it is
he will be
it goes
he goes
is
are
is after all
does
to
following
not after
not to
not
is not
are not
is not a
• Many translation options to choose from
– in Europarl phrase table: 2727 matching phrase pairs for this sentence
– by pruning to the top 20 per phrase, 202 translation options remain
MT Marathon Winter School, Lecture 5 30 January 2009
49
Translation optionshe
er geht ja nicht nach hause
it
, it
, he
is
are
goes
go
yes
is
, of course
not
do not
does not
is not
after
to
according to
in
house
home
chamber
at home
not
is not
does not
do not
home
under house
return home
do not
it is
he will be
it goes
he goes
is
are
is after all
does
to
following
not after
not to
not
is not
are not
is not a
• The machine translation decoder does not know the right answer
→ Search problem solved by heuristic beam search
MT Marathon Winter School, Lecture 5 30 January 2009
50
Decoding process: precompute translation optionser geht ja nicht nach hause
MT Marathon Winter School, Lecture 5 30 January 2009
51
Decoding process: start with initial hypothesiser geht ja nicht nach hause
MT Marathon Winter School, Lecture 5 30 January 2009
52
Decoding process: hypothesis expansioner geht ja nicht nach hause
are
MT Marathon Winter School, Lecture 5 30 January 2009
53
Decoding process: hypothesis expansioner geht ja nicht nach hause
are
it
he
MT Marathon Winter School, Lecture 5 30 January 2009
54
Decoding process: hypothesis expansioner geht ja nicht nach hause
are
it
he goes
does not
yes
go
to
home
home
MT Marathon Winter School, Lecture 5 30 January 2009
55
Decoding process: find best pather geht ja nicht nach hause
are
it
he goes
does not
yes
go
to
home
home
MT Marathon Winter School, Lecture 5 30 January 2009
56
Factored model decoding
• Factored model decoding introduces additional complexity
• Hypothesis expansion not any more according to simple translation table, but
by executing a number of mapping steps, e.g.:
1. translating of lemma → lemma
2. translating of part-of-speech, morphology → part-of-speech, morphology
3. generation of surface form
• Example: haus|NN|neutral|plural|nominative
→ { houses|house|NN|plural, homes|home|NN|plural,
buildings|building|NN|plural, shells|shell|NN|plural }
• Each time, a hypothesis is expanded, these mapping steps have to applied
MT Marathon Winter School, Lecture 5 30 January 2009
57
Efficient factored model decoding
• Key insight: executing of mapping steps can be pre-computed and stored as
translation options
– apply mapping steps to all input phrases
– store results as translation options
→ decoding algorithm unchanged... haus | NN | neutral | plural | nominative ...
houses|house|NN|plural
homes|home|NN|plural
buildings|building|NN|plural
shells|shell|NN|plural
...
...
...
...
...
...
...
...
...
... ...
...
MT Marathon Winter School, Lecture 5 30 January 2009
58
Efficient factored model decoding
• Problem: Explosion of translation options
– originally limited to 20 per input phrase
– even with simple model, now 1000s of mapping expansions possible
• Solution: Additional pruning of translation options
– keep only the best expanded translation options
– current default 50 per input phrase
– decoding only about 2-3 times slower than with surface model
MT Marathon Winter School, Lecture 5 30 January 2009
59
Factored Translation Models
• Motivation
• Example
• Model and Training
• Decoding
• Experiments
MT Marathon Winter School, Lecture 5 30 January 2009
60
Adding linguistic markup to outputword word
part-of-speech
OutputInput
• Generation of POS tags on the target side
• Use of high order language models over POS (7-gram, 9-gram)
• Motivation: syntactic tags should enforce syntactic sentence structure model
not strong enough to support major restructuring
MT Marathon Winter School, Lecture 5 30 January 2009
61
Some experiments
• English–German, Europarl, 30 million word, test2006
Model BLEU
best published result 18.15
baseline (surface) 18.04
surface + POS 18.15
• German–English, News Commentary data (WMT 2007), 1 million word
Model BLEU
Baseline 18.19
With POS LM 19.05
• Improvements under sparse data conditions
• Similar results with CCG supertags [Birch et al., 2007]
MT Marathon Winter School, Lecture 5 30 January 2009
63
Local agreement (esp. within noun phrases)word word
part-of-speech
OutputInput
morphology
• High order language models over POS and morphology
• Motivation
– DET-sgl NOUN-sgl good sequence
– DET-sgl NOUN-plural bad sequence
MT Marathon Winter School, Lecture 5 30 January 2009
65
Morphological generation modellemma lemma
part-of-speech
OutputInput
morphology
part-of-speech
word word
• Our motivating example
• Translating lemma and morphological information more robust
MT Marathon Winter School, Lecture 5 30 January 2009
66
Initial results
• Results on 1 million word News Commentary corpus (German–English)
System In-doman Out-of-domain
Baseline 18.19 15.01
With POS LM 19.05 15.03
Morphgen model 14.38 11.65
• What went wrong?
– why back-off to lemma, when we know how to translate surface forms?
→ loss of information
MT Marathon Winter School, Lecture 5 30 January 2009
67
Solution: alternative decoding pathslemma lemma
part-of-speech
OutputInput
morphology
part-of-speech
word word
or
• Allow both surface form translation and morphgen model
– prefer surface model for known words
– morphgen model acts as back-off
MT Marathon Winter School, Lecture 5 30 January 2009
68
Results
• Model now beats the baseline:
System In-doman Out-of-domain
Baseline 18.19 15.01
With POS LM 19.05 15.03
Morphgen model 14.38 11.65
Both model paths 19.47 15.23
MT Marathon Winter School, Lecture 5 30 January 2009
69
Adding annotation to the source
• Source words may lack sufficient information to map phrases
– English-German: what case for noun phrases?
– Chinese-English: plural or singular
– pronoun translation: what do they refer to?
• Idea: add additional information to the source that makes the required
information available locally (where it is needed)
• see [Avramidis and Koehn, ACL 2008] for details
MT Marathon Winter School, Lecture 5 30 January 2009
70
Case Information for English–GreekOutputInput
case
word word
subject/object
• Detect in English, if noun phrase is subject/object (using parse tree)
• Map information into case morphology of Greek
• Use case morphology to generate correct word form
MT Marathon Winter School, Lecture 5 30 January 2009
71
Obtaining Case Information
• Use syntactic parse of English input
(method similar to semantic role labeling)
MT Marathon Winter School, Lecture 5 30 January 2009
72
Results English-Greek
• Automatic BLEU scores System devtest test07
baseline 18.13 18.05
enriched 18.21 18.20
• Improvement in verb inflection
System Verb count Errors Missing
baseline 311 19.0% 7.4%
enriched 294 5.4% 2.7%
• Improvement in noun phrase inflection
System NPs Errors Missing
baseline 247 8.1% 3.2%
enriched 239 5.0% 5.0%
• Also successfully applied to English-Czech
MT Marathon Winter School, Lecture 5 30 January 2009
Translation Scenarios for En→Cs
Vanilla Translate+Check (T+C)
English Czech
form form +LM
lemma lemma
morphology morphology
English Czech
form form +LM
lemma lemma
morphology morphology +LM
Translate+2⋅Check (T+C+C) 2⋅Translate+Generate (T+T+G)
English Czech
form form +LM
lemma lemma +LM
morphology morphology +LM
English Czech
form form +LM
lemma lemma +LM
morphology morphology +LM
20/38
Details on Translate+Check
• Drawback: Morphological tags increase target-side complexity:
word form → word form word form → word form
morphological tag
green striped
zelený pruhovaný
zelené pruhované
zelení pruhovaní
zelených pruhovaných
zeleným pruhovaným
green striped
zelenýsg,masc,nom pruhovanýsg,masc,nom
zelenésg,fem,gen pruhovanésg,fem,gen
zelenésg,fem,dat pruhovanésg,fem,dat
zelenépl,fem,nom pruhovanépl,fem,nom
zelenípl,masc,nom pruhovanípl,masc,nom
zelenýchpl,masc,loc pruhovanýchpl,masc,loc
zeleným pruhovaným
• Benefit: more robust LMs, e.g. trained on morphological tags only.
• p(fem,nom masc,loc) < p(fem,nom fem,nom) … observed on all adjectives.
• p(zelené pruhovaných) < p(zelené pruhované) … much sparser.
21/38
Factored Attempts (WMT09)
Sents System BLEU NIST Sent/min
2.2M Vanilla PBMT 14.24 5.175 12.0
2.2M T+C 13.86 5.110 2.6
84k Vanilla PBMT 10.52 4.506 –
84k T+C+C&T+T+G 10.01 4.360 4.0
• In WMT07, T+C worked best.
+ fine-tuned tags helped with small data (Bojar, 2007).
• In WMT08, T+C was worth the effort (Bojar and Hajič, 2008).
• In WMT09, our computers could handle 7-grams of forms.
⇒ No gain from T+C.
• T+T+G too big to fit and explodes the search space.
⇒ Worse than Vanilla trained on the same dataset. 22/38
T+T+G Failure Explained
• Factored models are “synchronous”, i.e. Moses:
1. Generates fully instantiated “translation options”.
2. Appends translation options to extend “partial hypothesis”.
3. Applies LM to see how well the option fits the previous words.
• There are too many possible combinations of lemma+tag.
⇒ Less promising ones must be pruned.
! Pruned before the linear context is available.
• Hieu Hoang wasted a year on trying asynchronous factors.
• Pruning hard to design (no clear comparison for partial translation options).
• In a completely different decoder Bojar and Týnovský (2009) use “delayed
factors”.
• The final value generated only after the full hypothesis is ready.
23/38
Big / Long / Morphological LMs
• Our best setups used four LMs:
LM ID Factor Order # Training Tokens
long word form 7 685M
big word form 4 3903M
morph morph. tag 10 817M
longm morph. tag 15 817M
• … with complementary benefits:
long big long morph big long big morph big long morph all + longm
21.32 22.00 22.01 22.26 22.21 22.48 22.59
24/38
Tentative Summary
• Target-side rich morphology causes data sparseness.
• Factored setups compact the sparseness.
… but the search space is likely to explode at runtime.
• Explosion can be contained by pruning.
… but the pruning happens without linear context
⇒ high risk of search errors.
Two promising techniques for handling sparseness
and avoiding the explosion:
• Two-step translation (Bojar and Kos, 2010).
• Reverse self-training (Bojar and Tamchyna, 2011).
25/38
Two-Step Attempts (WMT10) 1/2
1. English → lemmatized Czech
• meaning-bearing morphology preserved
• max phrase len 10, distortion limit 6
• large target-side (lemmatized LM)
2. Lemmatized Czech → Czech
• trained on much more data
• max phrase len 1, monotone
Src after a sharp drop
Mid po+6 ASA1.prudký NSA-.pokles
Gloss after+voc adj+sg...sharp noun+sg...drop
Out po prudkém poklesu
26/38
Two-Step Attempts (WMT10) 2/2
Training Sents Vanilla Two-Step Diff
Parallel Mono BLEU SemPOS BLEU SemPOS B. S.
126k 126k 10.28±0.40 29.92 10.38±0.38 30.01 ↗↗
27/38
Two-Step Attempts (WMT10) 2/2
Training Sents Vanilla Two-Step Diff
Parallel Mono BLEU SemPOS BLEU SemPOS B. S.
126k 126k 10.28±0.40 29.92 10.38±0.38 30.01 ↗↗
126k 13M 12.50±0.44 31.01 12.29±0.47 31.40 ↘↗
27/38
Two-Step Attempts (WMT10) 2/2
Training Sents Vanilla Two-Step Diff
Parallel Mono BLEU SemPOS BLEU SemPOS B. S.
126k 126k 10.28±0.40 29.92 10.38±0.38 30.01 ↗↗
126k 13M 12.50±0.44 31.01 12.29±0.47 31.40 ↘↗
7.5M 13M 14.17±0.51 33.07 14.06±0.49 32.57 ↘↘
27/38
Two-Step Attempts (WMT10) 2/2
Training Sents Vanilla Two-Step Diff
Parallel Mono BLEU SemPOS BLEU SemPOS B. S.
126k 126k 10.28±0.40 29.92 10.38±0.38 30.01 ↗↗
126k 13M 12.50±0.44 31.01 12.29±0.47 31.40 ↘↗
7.5M 13M 14.17±0.51 33.07 14.06±0.49 32.57 ↘↘
Manual micro-evaluation of ↘↗, i.e. 12.50±0.44 vs. 12.29±0.47:
Two-Step Both Fine Both Wrong Vanilla Total
Two-Step 23 4 8 - 35
Both Fine 7 14 17 5 43
Both Wrong 8 1 28 2 39
Vanilla - 3 7 23 33
Total 38 22 60 30 150
• Each annotator weakly prefers Two-step
• but they don’t agree on individual sentences. 27/38
Reverse Self-Training
Goal: Learn from monolingual data to produce new target-side
word forms in correct contexts.
Source English Target Czech
Small Parallel a cat chased… = kočka honila…
I saw a cat = viděl jsem kočku
Big Monolingual četl jsem o kočce
28/38
Reverse Self-Training
Goal: Learn from monolingual data to produce new target-side
word forms in correct contexts.
Source English Target Czech
Small Parallel a cat chased… = kočka honila…
I saw a cat = viděl jsem kočku
Big Monolingual ? četl jsem o kočce
28/38
Reverse Self-Training
Goal: Learn from monolingual data to produce new target-side
word forms in correct contexts.
Source English Target Czech
Small Parallel a cat chased… = kočka honila…
I saw a cat = viděl jsem kočku
Big Monolingual ? četl jsem o kočce
Use reverse translation
28/38
Reverse Self-Training
Goal: Learn from monolingual data to produce new target-side
word forms in correct contexts.
Source English Target Czech
Small Parallel a cat chased… = kočka honila…
kočka honit… (lem.)
I saw a cat = viděl jsem kočku
vidět být kočka (lem.)
Big Monolingual ? četl jsem o kočce
číst být o kočka (lem.)
Use reverse translation
backed-off by lemmas.
28/38
Reverse Self-Training
Goal: Learn from monolingual data to produce new target-side
word forms in correct contexts.
Source English Target Czech
Small Parallel a cat chased… = kočka honila…
kočka honit… (lem.)
I saw a cat = viděl jsem kočku
vidět být kočka (lem.)
Big Monolingual ? četl jsem o kočce
číst být o kočka (lem.)
Use reverse translation
I read about a cat ← backed-off by lemmas.
28/38
Reverse Self-Training
Goal: Learn from monolingual data to produce new target-side
word forms in correct contexts.
Source English Target Czech
Small Parallel a cat chased… = kočka honila…
kočka honit… (lem.)
I saw a cat = viděl jsem kočku
vidět být kočka (lem.)
Big Monolingual ? četl jsem o kočce
číst být o kočka (lem.)
Use reverse translation
I read about a cat ← backed-off by lemmas.
⇒ A new phrase learned: “about a cat” = “o kočce”.
28/38
The Back-off to Lemmas
• The key distinction from self-training used for domain adaptation
(Bertoldi and Federico, 2009; Ueffing et al., 2007).
• We use simply “alternative decoding paths” in Moses:
Czech English
form form +LM or Czech English
lemma form +LM
• Other languages (e.g. Turkish, German) need different back-off
techniques:
• Split German compounds.
• Separate and allow to ignore Turkish morphology.
29/38
Small Parallel, Increasing Monolingual
1 2 3 4 5
26
28
30
32
Monolingual Data Size (millions of sentences)
Translation Quality (Automatic; BLEU)
LM only
30/38
Small Parallel, Increasing Monolingual
1 2 3 4 5
26
28
30
32
Monolingual Data Size (millions of sentences)
Translation Quality (Automatic; BLEU)
LM only
LM and TM
30/38
Increasing Para, Fixed Mono26
28
30
32
34
36
38
0 0.5 1 1.5 2 2.5 3 3.5 4 4.5
BLEU
%
Parallel data (mils of sents.)
Mono LM and TM
Mono LM
31/38
Morphology in
Neural MT
NMT: “Solved” by Segmentation
• SMT struggled with productive morphology (>1M wordforms).
nejneobhodpodařovávatelnějšími, Donaudampfschifffahrtsgesellschaftskapitän
• NMT can handle only 30–80k dictionaries.
⇒ Resort to sub-word units.
Orig český politik svezl migranty
Syllables čes ký ⊔ po li tik ⊔ sve zl ⊔ mig ran ty
Morphemes česk ý ⊔ politik ⊔ s vez l ⊔ migrant y
Char Pairs če sk ý ⊔ po li ti k ⊔ sv ez l ⊔ mi gr an ty
Chars č e s k ý ⊔ p o l i t i k ⊔ s v e z l ⊔ m i g r a n t y
BPE 30k český politik s@@ vez@@ l mi@@ granty
32/38
Byte Pair Encoding (Sennrich et al., 2016)
• Given a dictionary of token types and frequences.
1. Replace the most frequent pair of characters with a new unit .
(Record this “merge” operation.)
2. Repeat until the desired number of merge operations is reached.
33/38
Byte Pair Encoding (Sennrich et al., 2016)
• Given a dictionary of token types and frequences.
1. Replace the most frequent pair of characters with a new unit .
(Record this “merge” operation.)
2. Repeat until the desired number of merge operations is reached.
Current vocabulary The new merge
lower lowest newer widest
33/38
Byte Pair Encoding (Sennrich et al., 2016)
• Given a dictionary of token types and frequences.
1. Replace the most frequent pair of characters with a new unit .
(Record this “merge” operation.)
2. Repeat until the desired number of merge operations is reached.
Current vocabulary The new merge
lower lowest newer widest we → we
33/38
Byte Pair Encoding (Sennrich et al., 2016)
• Given a dictionary of token types and frequences.
1. Replace the most frequent pair of characters with a new unit .
(Record this “merge” operation.)
2. Repeat until the desired number of merge operations is reached.
Current vocabulary The new merge
lower lowest newer widest we → we
lo we r lo we st ne we r widest
33/38
Byte Pair Encoding (Sennrich et al., 2016)
• Given a dictionary of token types and frequences.
1. Replace the most frequent pair of characters with a new unit .
(Record this “merge” operation.)
2. Repeat until the desired number of merge operations is reached.
Current vocabulary The new merge
lower lowest newer widest we → we
lo we r lo we st ne we r widest we r → we r
33/38
Byte Pair Encoding (Sennrich et al., 2016)
• Given a dictionary of token types and frequences.
1. Replace the most frequent pair of characters with a new unit .
(Record this “merge” operation.)
2. Repeat until the desired number of merge operations is reached.
Current vocabulary The new merge
lower lowest newer widest we → we
lo we r lo we st ne we r widest we r → we r
lo we r lo we st ne we r widest
33/38
Byte Pair Encoding (Sennrich et al., 2016)
• Given a dictionary of token types and frequences.
1. Replace the most frequent pair of characters with a new unit .
(Record this “merge” operation.)
2. Repeat until the desired number of merge operations is reached.
Current vocabulary The new merge
lower lowest newer widest we → we
lo we r lo we st ne we r widest we r → we r
lo we r lo we st ne we r widest st → st
33/38
Byte Pair Encoding (Sennrich et al., 2016)
• Given a dictionary of token types and frequences.
1. Replace the most frequent pair of characters with a new unit .
(Record this “merge” operation.)
2. Repeat until the desired number of merge operations is reached.
Current vocabulary The new merge
lower lowest newer widest we → we
lo we r lo we st ne we r widest we r → we r
lo we r lo we st ne we r widest st → st
• New input: Apply the recorded sequence of merges:
newest → ne we st → ne we st ⇒ n@@ e@@ we@@ st
• Ensures that vocabulary size = alphabet + merge ops.
33/38
Flavours of Subword Units
• Byte Pair Encoding (BPE, Sennrich et al. (2016))
http://github.com/rsennrich/subword-nmt/
• Google Wordpieces (Wu et al., 2016)
Code probably unavailable, used in speech.
• SubwordTextEncoder in Tensor2tensor (Vaswani et al., 2017)
https://github.com/tensorflow/tensor2tensor
STE Blíží_ se_ k_ tobě_ tramvaj _ ._
Z_ tramvaj e_ nevysto upil i_ ._
BPE Blíží se k tobě tramvaj .
Z tramva@@ je nevy@@ stoupili .
BPE underscore Blíží_ se_ k_ tobě_ tramvaj@@ _ ._
Z_ tramvaj@@ e_ nevy@@ stoupili_ ._
The best now is SentencePiece: https://github.com/google/sentencepiece 34/38
Performance of STE and BPE
• German→Czech T2T experiments (Macháček et al., 2018).
• The underscore trick:
• Append “_” to tokens before learning splits.
split underscore BLEU
STE after every token 18.58±0.06
BPE after non-final tokens 18.24±0.08
BPE after every token 13.88±0.18
BPE - 13.69±0.66
• +5(!) BLEU points from the underscore trick.
• If not attached at the end of the sentence.
35/38
Room for Linguistics
• Ataman et al. (2017) use a new Morfessor model Flatcat
(Grönroos et al., 2014) for Turkish.
• Considerably better than BPE.
• Huck et al. (2017a) examine English→German:
• Compound, suffix, prefix and BPE splitting, or a cascade.
• Suffix+BPE or Compound+suffix+BPE best.
• Macháček et al. (2018) for German→Czech:
• Unsupervised (Morfessor) and supervised (DeriNet).
• STE worked best.
36/38
Summary
• Rich morphology causes serious problems to token-based MT.
• Factors in PBMT allow to capture additional info.
• Rich annotation is dangerous when not treated carefully.
Occam’s razor: think twice before adding an attribute.
• Avoid data sparseness, always provide a back-off.
• Avoid complex models:
- They are hard to tune (set parameters).
- They tend to explode at runtime.
• Promising 2-step translation.
• Reverse self-training good for small data.
• NMT with subword units resolves problems with morphology.
• Still room for linguistically-adequate solutions.
37/38
References
Duygu Ataman, Matteo Negri, Marco Turchi, and Marcello Federico. 2017. Linguistically motivated vocabulary
reduction for neural machine translation from turkish to english. The Prague Bulletin of Mathematical Linguistics,
108:331, Jan.
Nicola Bertoldi and Marcello Federico. 2009. Domain adaptation for statistical machine translation with monolingual
resources. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 182–189, Athens, Greece,
March. Association for Computational Linguistics.
Ondřej Bojar and Jan Hajič. 2008. Phrase-Based and Deep Syntactic English-to-Czech Statistical Machine Translation.
In Proceedings of the Third Workshop on Statistical Machine Translation, pages 143–146, Columbus, Ohio, June.
Association for Computational Linguistics.
Ondřej Bojar and Kamil Kos. 2010. 2010 Failures in English-Czech Phrase-Based MT. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and MetricsMATR, pages 60–66, Uppsala, Sweden, July. Association for
Computational Linguistics.
Ondřej Bojar and Aleš Tamchyna. 2011. Improving Translation Model by Monolingual Data. In Proceedings of the
Sixth Workshop on Statistical Machine Translation, pages 330–336, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Ondřej Bojar and Miroslav Týnovský. 2009. Evaluation of Tree Transfer System. Project Euromatrix - Deliverable 3.4,
ÚFAL, Charles University, March.
Ondřej Bojar. 2007. English-to-Czech Factored Machine Translation. In Proceedings of the Second Workshop on
Statistical Machine Translation, pages 232–239, Prague, Czech Republic, June. Association for Computational
Linguistics.
Stig-Arne Grönroos, Sami Virpioja, Peter Smit, and Mikko Kurimo. 2014. Morfessor FlatCat: An HMM-Based Method
for Unsupervised and Semi-Supervised Learning of Morphology. In Proceedings of COLING 2014, the 25th International38/38


Syntax in Pre-Neural
Statistical MT
Ondřej Bojar
April 9, 2020
NPFL087 Statistical Machine Translation
Charles University
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics unless otherwise stated
Outline
• Motivation for grammar in MT.
• Hierarchical Model.
• Proper syntax: Constituency vs. dependency trees.
Constituency Syntax:
• Context Free Grammars.
• MT as parsing.
• Synchronous CFG, LM integration.
• Why real source/target parse trees make it harder.
Dependency Syntax:
• Surface syntax (STSG), problems.
• Deep syntax (t-layer); TectoMT, HMTM.
1/43
Motivation
Simple phrase-based models:
• Don’t check for grammatical coherence.
⇒ 3-grams fluent, overall word salad.
• Don’t allow gaps in phrases:
I do not know… ↔ Je ne sais pas…
“do not” ↔ “ne pas”
• Reordering models have little explicit knowledge:
• No movement of longer blocks.
• No grammar constraints possible.
2/43
Hierarchical Model
Hierarchical Phrase-Based Model
Hierarchical model (Chiang, 2005): rough approximation of syntax.
Hiero addresses only the gaps in phrases:
• Gaps don’t have labels ⇒ any phrase fits.
“do not X” ↔ “ne X pas”
“do not cat” ↔ “ne chat pas”
⇒ Not really a grammar, but not an issue for correct input.
• Phrase extraction similar to phrase-based:
1. Extract all (non-gappy) phrases consistent with alignment.
2. If a subphrase is also extraced, make a synchronous gap.
⇒ Much higher number of rules extracted.
3/43
Hierarchical Phrase Extraction
4/43
Hierarchical Phrase Extraction
4/43
Challenges of Hierarchical Model
• Very high number of extractable rules.
• Limited by ad-hoc constraints:
• Initial phrases ≤ 10 words.
• Rules ≤ 6 symbols.
• At least one aligned terminal required.
• At most two non-terminals, non-adjacent.
• Spurious ambiguity.
= many ways to obtain the same output.
• Pollutes n-best lists.
• LM is a non-local feature.
• Causes serious state splitting ⇒ Search space much larger.
… Plus hierarchical model is not a syntactic model.
• With a special treatment of unaligned words, a regular PBMT system can
reach most of hierarchical hypotheses (Auli et al., 2009).
5/43
Proper Syntax
Constituency vs. Dependency Trees
SPPPP

NP
John
VP
HHH

V
loves
NP
Mary
Constituency trees = syntactic structure of a sentence as “bracketting”:
(𝑆 (𝑁 𝑃 John) (𝑉 𝑃 (𝑉 loves) (𝑁 𝑃 Mary) ) )
6/43
Constituency vs. Dependency Trees
SPPPP

NP
John
VP
HHH

V
loves
NP
Mary
Constituency trees = syntactic structure of a sentence as “bracketting”:
(𝑆 (𝑁 𝑃 John) (𝑉 𝑃 (𝑉 loves) (𝑁 𝑃 Mary) ) )
6/43
Constituency vs. Dependency Trees
SPPPP

NP
John
VP
HHH

V
loves
NP
Mary
loves
bbb
"
""
John Mary
or
John loves Mary
Constituency trees = syntactic structure of a sentence as “bracketting”:
(𝑆 (𝑁 𝑃 John) (𝑉 𝑃 (𝑉 loves) (𝑁 𝑃 Mary) ) )
6/43
Contituency Syntax
See MT Talk #10:
http://youtu.be/y_9SEdG1u3U
Context Free Grammar
Context-free grammar (CFG) describes infinite set of valid trees
using a finite set of rules, e.g.:
S → NP VP
Probabilistic CFG assign weights to rules, e.g.:
VP → {
V 0.1
V NP 0.5
V NP NP 0.4
(1)
Generative model for probabilitic CFG:
𝑝(tree 𝑇 |sentence 𝑠) = ∏
𝑋→𝛼∈𝑇
𝑝(𝛼|𝑋) (2)
7/43
Parsing
Parsing is finding the most probable constituency tree:̂
𝑇 = argmax
𝑇 ∈{trees of sentence 𝑠}
𝑝(𝑇 |𝑠) (3)
CKY (CYK) algorithm for 𝑂(𝑛3) parsing. (“dynamic programming”):
length: 7 S
6 VP
5
4 S
3 VP PP
2 S NP NP
1 NP V, VP Det N P Det N
0she1 1eats2 2a3 3fish4 4with5 6a6 6fork7 8/43
Synchronous CFG
• Synchronous CFG capture the “double generation”.
• Nonterminals must map 1-1.
9/43
Synchronous CFG
• Synchronous CFG capture the “double generation”.
• Nonterminals must map 1-1.
SXXXXX

NP
John
VP``````
V
fell
PP
cc##
P
in
N
love
PP
bbb
"
""
P
with
N
Mary
SPPPP

NP
Jan
VP
HHH

V
miluje
NP
Marii
9/43
Synchronous CFG
• Synchronous CFG capture the “double generation”.
• Nonterminals must map 1-1.
• Synchronous Tree Substitution Grammars (STSG)
• Whole subtrees are attached ⇒ structural changes ok.
SXXXXX

NP
John
VP``````
V
fell
PP
cc##
P
in
N
love
PP
bbb
"
""
P
with
N
Mary
SPPPP

NP
Jan
VP
HHH

V
miluje
NP
Marii
9/43
Synchronous CFG
• Synchronous CFG capture the “double generation”.
• Nonterminals must map 1-1.
• Synchronous Tree Substitution Grammars (STSG)
• Whole subtrees are attached ⇒ structural changes ok.
• In fact equivalent to SCFG.
SXXXXX

NP
John
VP``````
V
fell
PP
cc##
P
in
N
love
PP
bbb
"
""
P
with
N
Mary
SXXXXX

NP
John
VP+𝑃 𝑃 +𝑃 +𝑁XXXXXX

V
fell
in love PP+𝑃 +𝑁
bbb
"
""
with Mary
SPPPP

NP
Jan
VP
HHH

V
miluje
NP
Marii
9/43
MT as Parsing: While Parsing, Translate
Picture from slides by Li et al. (2009).
10/43
State Splitting for LM
11/43
Proper Syntax is Hard
See slides by Chiang (2010):
• The source and target trees constrain too much.
⇒ Too few rules extracted.
⇒ Coverage lost.
• Labelled non-terminals do not always match.
⇒ Some valid translations not admissible.
Relaxation of the hard constraints needed:
• Allow breaking trees into smaller fragments (e.g. binarization).
• Allow attachment of non-matching non-terminals.
12/43
the
DT JJ NN
green witch
la bruja verde
NP
DT NN JJ
P
a
VB NP
VP
bofetada
slap
PP
DTdió
una
NN
NPV
VP
STSG
extraction
1. Phrases
✤ respect word alignments
✤ are syntactic constituents on
both sides
2. Phrase pairs form rules
3. Subtract phrases to form rules
VB NP
VP
bofetada
slap
PP
DTdió
una
NN
NPV
VP
STSG
extraction
1. Phrases
✤ respect word alignments
✤ are syntactic constituents on
both sides
2. Phrase pairs form rules
3. Subtract phrases to form rules
Why is tree-to-tree hard?


13





116
 
 SRLQWV


11
  FKHFN

    116

    11



43





&'
  




,1
  WKDQ


--5
  PRUH
  FKHFN SR


13

    116



43





&'
  




,1
  WKDQ


--5
  PRUH
too few derivations
NP
the
DT JJ NN
green witch
a la bruja verde
too few rules
Why is tree-to-tree hard?
NP
the
DT JJ NN
green witch
a la bruja verde
too few rules



,3





93
  






13



11
  㙫␘




33
  㵀⼉≤ほ㯁㺲



13



15
  㜞㠙
  13
    33
  LQWUDGHEHWZHHQWKHWZRVKRUHV
  13
    11
 
 VXUSOXV
  13
  7DLZDQ·V



,3





93
  






13



11
  㙫␘




33
  㵀⼉≤ほ㯁㺲



13



15
  㜞㠙
  13
    33
  LQWUDGHEHWZHHQWKHWZRVKRUHV
  13
    11
 
 VXUSOXV
  13
  7DLZDQ·V


,3





93
  






13



11
  㙫␘




33
  㵀⼉≤ほ㯁㺲



13



15
  㜞㠙
,3
 ,3
 ,3
 9313


33
  㵀⼉≤ほ㯁㺲


13



15
  㜞㠙
Extracting more rules
binarize head-out


,3





93
  






13



11
  㙫␘




33
  㵀⼉≤ほ㯁㺲



13



15
  㜞㠙


,3





,3





,3





93
  




13



11
  㙫␘



33
  㵀⼉≤ほ㯁㺲


13



15
  㜞㠙
Extracting more rules
Syntax-Augmented Machine
Translation (Venugopal & Zollmann)
Tree-Sequence Substitution Grammar
(M. Zhang et al., 2008)
  
   




11
  㙫␘


,3





93
  






13



13



11
  㙫␘




33



33
  㵀⼉≤ほ㯁㺲



13



13



15
  㜞㠙


,3





93
  




133313



133313





13



11
  㙫␘




33
  㵀⼉≤ほ㯁㺲



13



15
  㜞㠙
Why is tree-to-tree hard?
13
 116
SRLQWV
11
FKHFN

    116

    11



43





&'
  




,1
  WKDQ


--5
  PRUH
  FKHFN  
 SRLQWV


13

    116



43





&'
  




,1
  WKDQ


--5
  PRUH
too few derivations
Allow more derivations

    116

    11



43





&'
  




,1
  WKDQ


--5
  PRUH
  FKHFN  
 SRLQWV


13

    116



43





&'
  




,1
  WKDQ


--5
  PRUH
✤ STSG: allow only matching
substitutions
✤ Hiero-like: allow any
substitutions
✤ Let the model learn to choose:
✤ matching substitutions
✤ mismatching substitutions
✤ monotone phrase-based


13





116
 
 SRLQWV


11
  FKHFN
Summary of Constituency Syntax in MT
• CFG capture syntax of some natural languages.
• Translating while chart parsing.
• SCFG/STSG parse input
and construct syntactically-parallel output.
• Hierarchical model: a simplification.
• Only a single nonterminal used.
• LM integrated by state splitting.
• When real source and/or target parse trees are used:
• Tricks/binarization necessary to extract non-isomorphic trees.
• Fuzzy matching must be allowed during decoding.
13/43
Dependency Syntax in MT
See MT Talk #11:
http://youtu.be/xauhVtfXbDQ
Constituency vs. Dependency Again
Constituency trees (CFG) represent only bracketing:
= which adjacent constituents are glued tighter to each other.
Dependency trees represent which words depend on which.
+ usually, some agreement/conditioning happens along the edge.
Constituency Dependency
John (loves Mary)
John VP(loves Mary) loves
bbb
"
""
John Mary
SPPPP

NP
John
VP
HHH


V
loves
NP
Mary John loves Mary 14/43
What Dependency Trees Tell Us
Input: The grass around your house should be cut soon.
Google: Trávu kolem vašeho domu by se měl snížit brzy.
Long-distance between grass and cut:
• Can “pump” many words in between ⇒ phrase limit exceeded.
Two errors caused by independent translation of grass and cut:
• Bad lexical choice for cut = sekat/snížit/krájet/řezat/…
• Bad case of tráva.
• Depends on the chosen active/passive form:
active⇒accusative passive⇒nominative
trávu … byste se měl posekat tráva … by se měla posekat
tráva … by měla být posekána
Examples by Zdeněk Žabokrtský, Karel Oliva and others.
15/43
Tree vs. Linear Context
The grass around your house should be cut soon
• Tree context (neighbours in the dependency tree):
• is better at predicting lexical choice than 𝑛-grams.
• often equals linear context:
Czech manual trees: 50% of edges link neighbours,
80% of edges fit in a 4-gram.
• Phrase-based MT is a very good approximation.
16/43
Hiero Can Cover Long-Distance Dependencytrávu
the
grass
around
your
house
should
be
cut
soon
brzy
kolem
vašeho
domu
byste
měl
posekat
the grass X1 should be cut = trávu X1 byste měl posekat 17/43
“Crossing Brackets”
• Constituent outside its father’s span causes “crossing brackets.”
• Linguists use “traces” ( 1 ) to represent this.
• Sometimes, this is not visible in the dependency tree:
• There is no “history of bracketing”.
• See Holan et al. (1998) for dependency trees including derivation history.
S’PPPPP

TOPIC
Mary 1
Saaaa
!!!!
NP
John
VP
bb
"
"
V
loves
NP
1
Mary John loves
Despite this shortcoming, CFGs are popular and “the” formal grammar for many. Possibly due to the charm of the
father of linguistics, or due to the abundance of dependency formalisms with no clear winner (Nivre, 2005).
18/43
Non-Projectivity
= a gap in a subtree span, filled by a node higher in the tree.
Ex. Dutch “cross-serial” dependencies, a non-projective tree with one
gap caused by saw within the span of swim.
…dat
…that
Jan
John
kinderen
children
zag
saw
zwemmen
swim
…that John saw children swim.
• 0 gaps ⇒ projective tree ⇒ can be represented in a CFG.
• ≤ 1 gap & “well-nested” ⇒ mildly context sentitive (TAG).
See Kuhlmann and Möhl (2007) and Holan et al. (1998). 19/43
Why Non-Projectivity Matters?
• CFGs cannot handle non-projective constructions:
Think in Dutch that John grass saw being-cut!
20/43
Why Non-Projectivity Matters?
• CFGs cannot handle non-projective constructions:
Think in Dutch that John grass saw being-cut!
• No way to glue these crossing dependencies together:
• Lexical choice:
𝑋 →< grass 𝑋 being-cut, trávu 𝑋 sekat >
• Agreement in gender:
𝑋 →< John 𝑋 saw, Jan 𝑋 viděl >
𝑋 →< Mary 𝑋 saw, Marie 𝑋 viděla >
• Phrasal chunks can memorize fixed sequences containing:
• the non-projective construction
• and all the words in between! (⇒ extreme sparseness)
20/43
Is Non-Projectivity Severe?
In principle:
• Czech allows long gaps as well as many gaps in a subtree.
Proti odmítnutí
Against dismissal
se
aux-refl
zítra
tomorrow
Petr
Peter
v práci
at work
rozhodl
decided
protestovat
to object
Peter decided to object against the dismissal at work tomorrow.
In treebank data:
⊖ 23% of Czech sentences contain a non-projectivity.
⊕ 99.5% of Czech sentences are well nested with ≤ 1 gap.
21/43
Summary of Dependency Trees
• More appropriate for Czech (frequent non-projectivity).
• Exhibit less divergence across languages (Fox, 2002).
• Dependency context more relevant than adjacency context.
So let’s look if we can apply them in MT.
22/43
Idea: Observe a Pair of Trees…
# Asociace uvedla , že domácí poptávka v září stoupla .
# The association said domestic demand grew in September .
23/43
…Decompose into Treelets…
# Asociace uvedla , že domácí poptávka v září stoupla .
# The association said domestic demand grew in September .
24/43
…Collect Dict. of Treelet Pairs
_Pred𝑐𝑠
_Sb𝑐𝑠 uvedla , že _Pred𝑐𝑠
=
_Pred𝑒𝑛
_Sb𝑒𝑛 said _Pred𝑒𝑛
_Sb𝑐𝑠
asociace =
_Sb𝑒𝑛
The association
_Sb𝑐𝑠
_Adj𝑐𝑠 poptávka
=
_Sb𝑒𝑛
_Adj𝑒𝑛 demand
…Synchronous Tree Substitution Grammar,
e.g. Čmejrek (2006). 25/43
Transfer at Various Layers
• My thesis main goal: Transfer at deep-syntactic layer.
• Implementation to be applicable anywhere with dependency trees.
26/43
Deep Syntax
See MT Talk #14:
http://youtu.be/lJwCW2mFk2M
Going Deeper
• Motivation for tectogrammatical layer.
• T-Layer in STSG:
• Complexity of t-layer attributes.
• Factorization inevitable, but how to factorize?
• Empirical evaluation.
• TectoMT Transfer.
• Hidden Markov Tree Model.
27/43
Tectogrammatics: Deep Syntax Culminating
Background: Prague Linguistic Circle (since 1926).
Theory: Sgall (1967), Panevová (1980), Sgall et al. (1986).
Materialized theory — Treebanks:
• Czech: PDT 1.0 (2001), PDT 2.0 (2006)
• Czech-English: PCEDT 1.0 (2004), PCEDT 2.0 (2012)
• Arabic: PADT (2004)
Practice — Tools:
• parsing Czech to surface: McDonald et al. (2005)
• parsing Czech to deep: Klimeš (2006)
• parsing English to surface: well studied (+rules convert to dependency trees)
• parsing English to deep: heuristic rules (manual annotation in progress)
• generating Czech surface from t-layer: Ptáček and Žabokrtský (2006)
28/43
Layers in PDT
29/43
Analytical vs. Tectogrammatical
• hide auxiliary words, add nodes
for “deleted” participants
• resolve e.g. active/passive voice,
analytical verbs etc.
• “full” t-layer resolves much more,
e.g. topic-focus articulation or
anaphora
30/43
Czech and English A-Layer
31/43
Czech and English T-Layer
Predicate-argument structure: changeshould(ACT: someone, PAT: it)
32/43
The Tectogrammatical Hope
Transfer at t-layer should be easier than direct translation:
• Reduced structure size (auxiliary words disappear).
• Long-distance dependencies (non-projectivites) solved at t-layer.
• Word order ignored / interpreted as information structure
(given/new).
• Reduced vocabulary size (Czech morphological complexity).
• Czech and English t-trees structurally more similar
⇒less parallel data might be sufficient (but more monolingual).
• Ready for fancy t-layer features: co-reference.
33/43
Reminder: STSG
# Asociace uvedla , že domácí poptávka v září stoupla .
# The association said domestic demand grew in September .
1. Decompose input tree into treelets.
2. Replace treelets with their translations.
3. Join output treelets.
34/43
Reminder: STSG
# Asociace uvedla , že domácí poptávka v září stoupla .
# The association said domestic demand grew in September .
1. Decompose input tree into treelets.
2. Replace treelets with their translations.
3. Join output treelets.
Real t-nodes have 25 attributes! ⇒ Can’t treat them as atomic.
34/43
Structure vs. Attributes
Factorization = introduction of independence assumptions.
• STSG factorizes along structure (input into treelets).
• T-layer requires factorization along attributes.
Which should go first?
• Treelets of attributes?
• Similar to phrases of factors, synchronous approach.
• Can easily fill up stacks with treelets differing too little.
• Layers of trees?
• Would be hard to ensure matching tree structure.
• Rather use a few attributes to construct structure
and postpone the choice of others until the tree is finished.
35/43
Transfer at Various Layers
Layers \ Language Models no LM 𝑛-gram/binode
epcp, no factors 8.65±0.55 10.90±0.63
eaca, no factors 6.59±0.52 8.75±0.61
etct 2009; 43k - 7.39±0.52
etca, no factors - 6.30±0.57
etct factored, preserving structure 5.31±0.53 5.61±0.50
eact, source factored, output atomic - 3.03±0.32
etct, no factors, all attributes 1.61±0.33 2.56±0.35
etct, no factors, just t-lemmas 0.67±0.19 -
etct 2009: strall + wfwindep. LM rescoring. Formemes (not functors) as frontier labels.
Improved node-to-node alignment (Mareček et al., 2008). New generation pipeline.
t: a: p: (WMT07 DevTest) 36/43
Reasons of STSG Bad Performance
• Cumulation of Errors in annotation pipeline.
• Data Loss due to incompatible structures:
• Error in parses or word-alignment prevents treelet pair extraction.
• Combinatorial Explosion of factored output:
• Abundance of t-node attribute combinations
⇒ e.g. lexically different translation options pushed off the stack
⇒ 𝑛-bestlist varies in unimportant attributes.
• Too Strong Independence Assumtions:
• Should never analyze and factorize phrases seen often enough.
• Complex models hard to tune:
• More models ⇒ Minimum error rate training has harder time.
37/43
“TectoMT Transfer” (1/3)         
   
    
     
    
      
 
 
 
 
  
38/43
“TectoMT Transfer” (2/3)
             
   
     
      
 
 
 
  
 
  
   
    
    !
 "  
     
 
""        
#!
$ 
 
"      
    
"   
 
 " 
   
         
39/43
“TectoMT Transfer” (3/3)
Slides 6–28 by Martin Popel (2009):
• Illustration of TectoMT transfer.
• Hidden Markov Tree Model (HMTM).
40/43
Demo Translation – Analysis
Machine translation should be easy.
machine translation should be easy .
NN NN MD VB JJ .
raw text
m-layer
a-layer
Atr
machine
Sb
translation
Pred
should
Obj
be
AuxK
.
Pnom
easy
Demo Translation – Analysis
Machine translation should be easy.
machine translation should be easy .
NN NN MD VB JJ .
raw text
m-layer
a-layer
Atr
machine
Sb
translation
Pred
should
Obj
be
AuxK
.
Pnom
easy
Mark functional words
Demo Translation – Analysis
Machine translation should be easy.
machine translation should be easy .
NN NN MD VB JJ .
raw text
m-layer
a-layer
Atr
machine
Sb
translation
Pred
should
Obj
be
AuxK
.
Pnom
easy
Mark edges to be contracted
Demo Translation – Analysis
Machine translation should be easy.
machine translation should be easy .
NN NN MD VB JJ .
raw text
m-layer
t-layer
machine
translation
be
easy
Build t-tree (backbone)
Demo Translation – Analysis
Machine translation should be easy.
machine translation should be easy .
NN NN MD VB JJ .
raw text
m-layer
t-layer
machine
translation easy
n:attr n:subj
v:fin
adj:compl
Fill formems
be
Demo Translation – Analysis
Machine translation should be easy.
machine translation should be easy .
NN NN MD VB JJ .
raw text
m-layer
t-layer
machine
translation easy
n:attr n:subj
v:fin
adj:compl
Fill grammatemes tense = simple,
modality, conditional
number = singular degree of comparison
= positive
be
Demo Translation – Transfer
source t-layer
machine
translation
be
easy
n:attr
n:subj
v:fin
adj:compl
Build target t-tree by cloning
target t-layer
machine
translation
be
easy
n:attr
n:subj
v:fin
adj:compl
Demo Translation – Transfer
source t-layer
machine
translation
be
easy
n:attr
n:subj
v:fin
adj:compl
Get translation variants
for lemmas and formems
target t-layer
počítač
stroj
strojový
překlad
převod snadný
jednoduchý
n:2
n:attr
adj:attr
n:1
v:fin
v:inf
adj:compl
n:1
adv:
být
mít
Demo Translation – Transfer
source t-layer
machine
translation
be
easy
n:attr
n:subj
v:fin
adj:compl
Select the best combination
of lemmas and formems
target t-layer
počítač
stroj
strojový
překlad
převod snadný
jednoduchý
n:2
n:attr
adj:attr
n:1
v:fin
v:inf
adj:compl
n:1
adv:
být
mít
Demo Translation – Synthesis
Build target a-layer by cloning
target t-layer
strojový
překlad snadný
adj:attr
n:1
v:fin
adj:compl
být
strojový
překlad snadný
býttarget a-layer
Demo Translation – Synthesis
Fill morphological categories
target t-layer
strojový
překlad snadný
adj:attr
n:1
v:fin
adj:compl
být
strojový
překlad snadný
býttarget a-layer
number = singular
gender = masc. inanim.
case = nominativedegree = positive
degree = positive
Demo Translation – Synthesis
Impose agreement
target t-layer
strojový
překlad snadný
adj:attr
n:1
v:fin
adj:compl
být
strojový
překlad snadný
býttarget a-layer
number = singular
gender = masc. inanim.
case = nominative
number = singular
case = nominative
gender = masc. inanim.
degree = positive
degree = positive
number = singular
case = nominative
gender = masc. inanim.
number = singular
gender = masc. Inanim.
Demo Translation – Synthesis
Add functional words
target t-layer
strojový
překlad snadný
adj:attr
n:1
v:fin
adj:compl
být
strojový
překlad
snadný
míttarget a-layer
býtby
.
Demo Translation – Synthesis
Reorder clitics
target t-layer
strojový
překlad snadný
adj:attr
n:1
v:fin
adj:compl
být
strojový
překlad
snadný
míttarget a-layer
býtby
.
Demo Translation – Synthesis
Generate wordforms
target t-layer
strojový
překlad snadný
adj:attr
n:1
v:fin
adj:compl
být
strojový
překlad
snadný
měltarget a-layer
býtby
.
Demo Translation – Synthesis
Concatenate tokens for output
target t-layer
strojový
překlad snadný
adj:attr
n:1
v:fin
adj:compl
být
strojový
překlad
snadný
měltarget a-layer
býtby
Strojový překlad by měl být snadný.
.
HMTM – Motivation
source t-layer
machine
translation
be
easy
n:attr
n:subj
v:fin
adj:compl
Select the best label
for each node
target t-layer
počítač|n:2,
počítač|n:attr,
strojový|adj:attr, ...
překlad|n:1,
převod|n:1
být|v:fin, být|v:inf,
mít|v:fin, mít|v:inf
snadný|adj:compl,
jednoduchý|adj:compl, ...
Hidden Markov Tree Modelmachine engine
translation arcade
be have
easy simple
strojový
překlad
být
snadný
ROOT
PE (strojový | engine) = 0.5
P E(strojový | machine) = 0.4
PE (překlad | translation) = 0.6
PE (překlad | arcade) = 0.7
1×10 -8
P T(machine | translation) = 0.02
1×10 -8
1×10-10
0.002
0.001
0.01
P E (být | be) = 0.8
P E (být | have) = 0.01
1×10 -8
Source tree (Czech) Target tree (English)
ANALYSIS
TRANSFER
SYNTHESIS
ROOT
Source sentence:
Strojový překlad by měl být snadný.
Target sentence:
Machine translation should be easy.
P(optimal_tree) = P E (strojový | machine) · P T (machine | translation)·
P E (překlad | translation) · P T (translation | be)·
P E (snadný | easy) · P T (easy | be)·
P E (být | be) · P T (be | ROOT)
0.0001
41/43
Summary
• Dependency trees linguistically more promising.
• Tree context vs. linear context. Non-projectivity. T-layer.
• STSG to transfer dependency trees:
• Severe issues of sparseness, i.a. due to missing adjunction.
• TectoMT system with HMTM transfer.
Rich annotation hurts if not backed-off.
• Due to sparser data (incompatible trees).
• Due to cummulation of errors.
• Due to too strong independence assumptions.
• Due to harder optimization problem for MERT.
42/43
References
Michael Auli, Adam Lopez, Hieu Hoang, and Philipp Koehn. 2009. A systematic analysis of translation model search
spaces. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 224–232, Athens, Greece,
March. Association for Computational Linguistics.
David Chiang. 2005. A Hierarchical Phrase-Based Model for Statistical Machine Translation. In Proceedings of the
43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 263–270, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
David Chiang. 2010. Learning to translate with source and target syntax. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics, pages 1443–1452, Uppsala, Sweden, July. Association for
Computational Linguistics.
Martin Čmejrek. 2006. Using Dependency Tree Structure for Czech-English Machine Translation. Ph.D. thesis, ÚFAL,
MFF UK, Prague, Czech Republic.
Heidi J. Fox. 2002. Phrasal cohesion and statistical machine translation. In EMNLP ’02: Proceedings of the ACL-02
conference on Empirical methods in natural language processing, pages 304–3111. Association for Computational
Linguistics.
Tomáš Holan, Vladislav Kuboň, Karel Oliva, and Martin Plátek. 1998. Two Useful Measures of Word Order
Complexity. In A. Polguere and S. Kahane, editors, Proceedings of the Coling ’98 Workshop: Processing of
Dependency-Based Grammars, Montreal. University of Montreal.
Václav Klimeš. 2006. Analytical and Tectogrammatical Analysis of a Natural Language. Ph.D. thesis, ÚFAL, MFF UK,
Prague, Czech Republic.
Marco Kuhlmann and Mathias Möhl. 2007. Mildly context-sensitive dependency languages. In Proceedings of the 45th
Annual Meeting of the Association of Computational Linguistics, pages 160–167, Prague, Czech Republic, June.
Association for Computational Linguistics. 43/43


Transformer and
Syntax in NMT
Ondřej Bojar
April 16, 2020
NPFL087 Statistical Machine Translation
Charles University
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics unless otherwise stated
Overview
• Reminder: Seq2seq with Attention.
• Transformer Architecture.
• Focus on Self-Attention.
• Explicit Syntax in NMT.
• In Network Structure.
• At Each Token.
• In Attention.
Some images due to Jindřich Helcl and/or Jindřich Libovický.
1/43
Reminder: Seq2seq with Attention<s> x 1 x 2 x 3 x 4
~y i ~y i+1
h 1h 0 h 2 h 3 h 4
...
+
×
α 0
×
α 1
×
α 2
×
α 3
×
α 4
s is i-1 s i+1
+
2/43
Attention – Formal Notation
Inputs:
decoder state 𝑠𝑖
encoder states ℎ𝑗 = [⃗⃗⃗⃗⃗⃗⃗⃗⃗ℎ𝑗;⃖⃖⃖⃖⃖⃖⃖⃖⃖ ℎ𝑗] ∀𝑖 = 1 … 𝑇𝑥
where⃗⃗⃗⃗⃗⃗⃗⃗⃗ ℎ𝑗 = RNNenc(ℎ𝑗−1, 𝑥𝑗) = tanh(𝑈𝑒⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗ ℎ𝑗−1 + 𝑊𝑒𝐸𝑒𝑥𝑗 + 𝑏𝑒)
Attention energies: 𝑒𝑖𝑗 = 𝑣⊤
𝑎 tanh (𝑊𝑎𝑠𝑖−1 + 𝑈𝑎ℎ𝑗 + 𝑏𝑎)
Attention distribution: 𝛼𝑖𝑗 = exp(𝑒𝑖𝑗)
∑𝑇𝑥
𝑘=1 exp(𝑒𝑖𝑘)
Context vector: 𝑐𝑖 = ∑𝑇𝑥
𝑗=1 𝛼𝑖𝑗ℎ𝑗
3/43
Attention Mechanism in Equations (2)
Decoder state:
𝑠𝑖 = tanh(𝑈𝑑𝑠𝑖−1 + 𝑊𝑑𝐸𝑑̂ 𝑦𝑖−1 + 𝐶𝑐𝑖 + 𝑏𝑑)
Output projection:
𝑡𝑖 = tanh (𝑈𝑜𝑠𝑖 + 𝑊𝑜𝐸𝑑̂ 𝑦𝑖−1 + 𝐶𝑜𝑐𝑖 + 𝑏𝑜)
…context vector is mixed with the hidden state
Output distribution:
𝑝 (𝑦𝑖 = 𝑘 |𝑠𝑖, 𝑦𝑖−1, 𝑐𝑖) ∝ exp (𝑊𝑜𝑡𝑖)𝑘 + 𝑏𝑘
4/43
Transformer
Attention is All You Need (Vaswani et al., 2017)
5/43
Transformer Detailed Walkthroughs
Transformer Illustrated:
• http://jalammar.github.io/illustrated-transformer/
Amazingly simple description! (I am reusing the pictures.)
Transformer paper annotated with PyTorch code:
• http://nlp.seas.harvard.edu/2018/04/03/attention.html
• PyTorch by examples:
https://github.com/jcjohnson/pytorch-examples
Summary at Medium:
• https://medium.com/@adityathiruvengadam/
transformer-architecture-attention-is-all-you-need-aeccd
6/43
Transformer = 6 Layers Enc + 6 Dec
7/43
Composition of One Layer
8/43
Word Vectors in Encoder
9/43
FF Is Actually Position-Independent
10/43
Positional Encoding
• Encode token position directly in the word vector:
• Positional embedding can be random, or “frequency-like”:
11/43
Self-Attention
Self-Attention Motivation (1/2)
• Sequences of arbitrary length 𝑛 need to be processed.
• RNNs make the (time-unrolled) network as deep as 𝑛.
• CNNs allow to trade kernel size 𝑘 and depth for a target “receptive
field”:
embeddings x = (𝑥1, … , 𝑥𝑁 )𝑥0 =⃗ 0 𝑥𝑛 =⃗ 0
12/43
Self-Attention Motivation (2/2)
• SANs (Self-Attentive Networks) can access any position in
constant time.
Operations Sequential Steps Memory
Recurrent 𝑂(𝑛 ⋅ 𝑑2) 𝑂(𝑛) 𝑂(𝑛 ⋅ 𝑑)
Convolutional 𝑂(𝑘 ⋅ 𝑛 ⋅ 𝑑2) 𝑂(1) 𝑂(𝑛 ⋅ 𝑑)
Self-attentive 𝑂(𝑛2 ⋅ 𝑑) 𝑂(1) 𝑂(𝑛2 ⋅ 𝑑)
• Sequence length 𝑛, state dimensionality 𝑑, kernel size 𝑘.
• Assuming infinitely many GPU cores (or rather ALU),
operations can be run in parallel,
but may depend on each other, needing some Sequential Steps.
13/43
Self-Attention
• Goal: Aggregate arbitary-length input to fixed-size vector.
Goal: Allow data-driven, trainable aggregation.
14/43
Self-Attention
• Goal: Aggregate arbitary-length input to fixed-size vector.
Goal: Allow data-driven, trainable aggregation.
Given the sequence of inputs 𝑥1, … , 𝑥𝑛:
• Create three “views” of
them: queries, keys,
values.
• Using trained matrices
𝑊 𝑄, 𝑊 𝐾, 𝑊 𝑉 .
14/43
Match All Queries with All Keys
15/43
Normalize Scores
16/43
Aggregate Values Accordingly
17/43
Self-Attention as Matrix Calculation
18/43
Multi-Head Attention
19/43
Multi-Head Attention
20/43
Self-Attention Summary
21/43
Self-Attention in Transformer
Three uses of multi-head attention in Transformer
• Encoder-Decoder Attention:
• Q: previous decoder layers; K = V: outputs of encoder
⇒ Decoder positions attend to all positions of the input.
• Encoder Self-Attention:
• Q = K = V: outputs of the previous layer of the encoder
⇒ Encoder positions attend to all positions of previous layer.
• Decoder Self-Attention:
• Q = K = V: outputs of the previous decoder layer.
• Masking used to prevent depending on future outputs.
⇒ Decoder attends to all its previous outputs.
22/43
Self-Attention at Enc Layer #5: 1 Head
23/43
Self-Attention at Enc Layer #5: 2 Heads
24/43
Self-Attention at Enc Layer #5: 8 Heads
25/43
Explicit Linguistic
Information in NMT
Ways of Adding Linguistic Annotation
• Construct network structure along linguistic structure.
∼ What we discussed in Syntax in SMT.
• Tree-LSTMs.
• Graph-Convolutional Networks.
… Source information only.
• Enrich information at each token.
∼ What we discussed in Morphology in SMT.
• Factors on the source side.
• Multi-Task on the target side.
• Improve attention using linguistic annotation.
• Attention calculation respecting syntax.
• Attention forced to reflect syntax in multi-task.
26/43
Linguistics in NN Structure
Tree-LSTMs (Tai et al., 2015)
Memory comes from:
the single predecessor childrenx 1 x 2 x 3 x 4
y 1 y 2 y 3 y 4x 1
x 2
x 4 x 5 x 6
y 1
y 2 y 3
y 4 y 6
plain LSTM Tree-LSTM
• Two flavors:
• Dependency trees: Sum over all children.
• Constituency trees: Up to N children, respecting order. 27/43
Chen et al. (2017a) (1/2)x1 x2 x3 x4 x5 x6
−→
h 1
−→
h 2
−→
h 3
−→
h 4
−→
h 5
−→
h 6
−→
h 1
←−
h 2
←−
h 3
←−
h 4
←−
h 5
←−
h 6
h ↑
7
h↑
8
h↑
9
h↑
10
h ↑
11
Tree-GRU Encoder:
• Constituency syntax of the tree provides additional states. 28/43
Chen et al. (2017a) (2/2)x1 x2 x3 x4 x5 x6
h ↑
1 h↑
2 h ↑
3 h↑
4 h ↑
5 h↑
6h↓
1 h ↓
2 h↓
3 h↓
4 h ↓
5 h ↓
6
h ↑
7 h↓
7
h ↑
8 h ↓
8
h↑
9 h ↓
9
h↑
10 h ↓
10
h ↑
11 h ↓
11
Bidirectional tree encoder.
• Can be seen as many RNNs running from each word up to the root
and back to the word. 29/43
Graph-Convolutional Networks (Bastings et al., 2017)W ( 0)
det
W ( 0)
nsubj
W ( 0)
dobj
W ( 0)
det
W ( 1)
det
W ( 1)
nsubj
W ( 1)
dobj
W ( 1)
det
*PAD* The monkey eats a banana *PAD*
h (0)
h (1)
h (2)
GCNCNN
Figure 2: A 2-layer syntactic GCN on top of a convolutional encoder. Loop connections are depicted
with dashed edges, syntactic ones with solid (dependents to heads) and dotted (heads to dependents)
edges. Gates and some labels are omitted for clarity.
30/43
Linguistics at Each Token
CCGs to Encode Syntax at Each Token
Syntax reflects long-distance dependencies.
• What city is the Taj Mahal in?
• Where is the Taj Mahal ∅?
The need to produce in depends on the What/Where.
• CCG tags for is differ ⇒ dependency highlighted.
• Following CCG tags, the decoder can know if in is needed.
• CCG tags are denser than words ⇒ better generalization.
31/43
CCGs to Encode Syntax at Each Token
Syntax reflects long-distance dependencies.
• What city is the Taj Mahal in?
• Where is the Taj Mahal ∅?
The need to produce in depends on the What/Where.
• CCG tags for is differ ⇒ dependency highlighted.
• Following CCG tags, the decoder can know if in is needed.
• CCG tags are denser than words ⇒ better generalization.
• What(S[wq]/(S[q]/NP))/N city is(S[q]/P P)/NP the Taj Mahal in?
• WhereS[wq]/(S[q]/NP) is(S[q]/NP)/NP the Taj Mahal?
31/43
Factors in NMT
• Source word factors easy to incorporate:
• Concatenate embeddings of the various factors.
• POS tags, morph. features, source dependency labels help en↔de and
en→ro (Sennrich and Haddow, 2016).
• Target word factors:
• Interleave for morphology: (Tamchyna et al., 2017)
Src there are a million different kinds of pizza .
Baseline (BPE) existují miliony druhů piz@@ zy .
Interleave VB3P existovat NNIP1 milion NNIP2 druh NNFS2 pizza Z: .
• Interleave for syntax: (Nadejde et al., 2017)
Src BPE Obama receives Net+ an+ yahu in the capital of USA
Tgt NP Obama ((S[dcl]\NP)/PP)/NP receives NP Net+ an+ yahu PP/NP in NP/N the N cap
• Multiple decoders, each predicting own sequence.
32/43
Predicting Target Syntax
My students Dan Kondratyuk and Ronald Cardenas retried Nadejde et
al. (2017) with:
• sequence-to-sequence model,
• Transformer model.
Predicting target syntax using:
• a secondary decoder
• interleaving.
As tags, they used:
• correct CCG tags, • random tags, • a single dummy tag.
(Kondratyuk et al. Replacing Linguists with Dummies. PBML 2019.)
33/43
Predicting Target Syntax (S2S)Training steps (millions)
Baseline CCG Random Same
0 2 4 6 8 10 12 14 16 18
0
5
10
15
20
25
Interleaved
Seq2seqTraining steps (millions)
Baseline CCG Random Same
Multi-Decoder
0 2 4 6 8 10 12 14 16 18
0
5
10
15
20
25
Seq2seq
34/43
Predicting Target Syntax (Transformer)Training steps (millions)
Baseline CCG Random Same
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28
0
5
10
15
20
25
30
Interleaved
TransformerTraining steps (millions)
Baseline CCG Random Same
Multi-Decoder
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28
0
5
10
15
20
25
Transformer
35/43
Syntax in Word Embeddings Chen et al. (2017b)SrcdepSrcU2=<x3, x1, x4, x7,ε>Dep TuplesU1EncoderVx1VU1h1Vx2VU2h2VxJVUJhJ………DecoderUJCNNCNNCNNCNNx1x2x3x4x5x6x7root…,1i,2i…,i J
• CNN-derived embeddings of nodes’ syntactic neighbourhood
included: (parent, siblings).
• Two mechanisms:
• Concatenated to standard embeddings.
• Separate attention over these word-level annotiations
36/43
Linguistics in Attention
Tree Coverage in Attention Chen et al. (2017a)
Tree coverage model:
• Attention coverage depends on source syntax.
• Without it (left), output is repeated. 37/43
Multi-Task to Request Dependency Tree (1/2)
• Pham et al. (2019) noticed that
attention head could be interpreted as
dependency parse.
• Add secondary objective to require
head #1 to match source dependency
tree.
38/43
Multi-Task to Request Dependency Tree (2/2)
• Czech-to-English translation (BLEU).
• Czech dependency parse from head #1 (UAS).
BLEU UAS
Dev Test Dev Test
Transformer Baseline 37.28 36.66 – –
Parse from layer 0 36.95 36.60 81.39 82.85
Parse from layer 1 38.51 38.01 90.17 90.78
Parse from layer 2 38.50 37.87 91.31 91.18
Parse from layer 3 38.37 37.67 91.43 91.43
Parse from layer 4 37.86 37.60 91.65 91.56
Parse from layer 5 37.63 37.67 91.44 91.46
39/43
Dummy Dependency Tree
I shot an elephant in my pajamas
I
shot
an
elephant
in
my
pajamas
ROOT
I shot
an
elephant
inmy
pajamas
40/43
Multi-Task to Request Dummy Tree
BLEU Precision
Dev Test Dev Test
Transformer Baseline 37.28 36.66 – –
Dummy Parse from layer 0 38.68 38.14 99.97 99.96
Dummy Parse from layer 1 39.11 38.06 99.99 99.99
Dummy Parse from layer 2 37.85 37.85 99.98 99.98
Dummy Parse from layer 3 37.93 37.70 99.97 99.98
Dummy Parse from layer 4 37.68 37.47 99.98 99.96
Dummy Parse from layer 5 37.53 37.54 99.96 99.95
True Parse from layer 1 38.51 38.01 90.17 90.78
41/43
Summary
• Transformer is a great replacement for RNN.
• Constant-time processing.
• (CNNs can be comparable, but Gehring et al. (2016) was kind of missed.)
• Explicit syntax can be useful.
• Many options how to include it.
• Some gains hard to reproduce.
• Dummy information can be equally useful.
• Transformer seems to learn syntax for free.
42/43
References
Joost Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, and Khalil Simaan. 2017. Graph convolutional encoders
for syntax-aware neural machine translation. In Proceedings of the 2017 Conference on Empirical Methods in Natural
Language Processing, pages 1947–1957. Association for Computational Linguistics.
Huadong Chen, Shujian Huang, David Chiang, and Jiajun Chen. 2017a. Improved Neural Machine Translation with a
Syntax-Aware Encoder and Decoder. In Proceedings of the 55th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1936–1945, Vancouver, Canada, July. Association for Computational
Linguistics.
Kehai Chen, Rui Wang, Masao Utiyama, Lemao Liu, Akihiro Tamura, Eiichiro Sumita, and Tiejun Zhao. 2017b. Neural
Machine Translation with Source Dependency Representation. In Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing, pages 2846–2852, Copenhagen, Denmark, September. Association for
Computational Linguistics.
Jonas Gehring, Michael Auli, David Grangier, and Yann N. Dauphin. 2016. A convolutional encoder model for neural
machine translation. CoRR, abs/1611.02344.
Maria Nadejde, Siva Reddy, Rico Sennrich, Tomasz Dwojak, Marcin Junczys-Dowmunt, Philipp Koehn, and Alexandra
Birch. 2017. Predicting target language ccg supertags improves neural machine translation. In Proceedings of the
Second Conference on Machine Translation, Volume 1: Research Paper, pages 68–79, Copenhagen, Denmark,
September. Association for Computational Linguistics.
Thuong-Hai Pham, Dominik Macháček, and Ondřej Bojar. 2019. Promoting the knowledge of source syntax in
transformer nmt is not needed. Computación y Sistemas, 23(3):923–934.
Rico Sennrich and Barry Haddow. 2016. Linguistic input features improve neural machine translation. In Proceedings
of the First Conference on Machine Translation, pages 83–91, Berlin, Germany, August. Association for Computational
Linguistics. 43/43


Does MT Understand?
Word and Sentence Representations
Ondřej Bojar
April 23, 2020
NPFL087 Statistical Machine Translation
Charles University
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics unless otherwise stated
Outline
• Introducing Semiotics.
• Do Current MT Systems Understand?
• Continuous Representations.
• What are Good Representations?
• Continuous Word Representations.
• Continuous Sentence Representations.
• Aspects of Meaning.
• Evaluating Sentence Representations
• How Meaningful is Seq2Seq Representation?
1/86
2/86
Semiotic TriangleThought or Reference
Symbol Referent
Correct symbol symbolises
Adequate thought refers to
True symbol stands for
Semiotic Triangle by Ogden and Richards (1923). 3/86
Semiotic TriangleThought or Reference
Symbol Referent
Correct symbol symbolises
Adequate thought refers to
True symbol stands for
Danny
approached
the chair with
a yellow bag.
Ambiguous sentence… 4/86
Semiotic TriangleThought or Reference
Symbol Referent
Correct symbol symbolises
Adequate thought refers to
True symbol stands for
Danny
approached
the chair with
a yellow bag.
Ambiguous sentence correspond to two situations. LavaCorpus (Berzak et al., 2015) 5/86
Semiotic TriangleThought or Reference
Symbol Referent
Correct symbol symbolises
Adequate thought refers to
True symbol stands for
Danny
approached
the chair with
a yellow bag.
Syntactic “meaning” distinguishes this already. 6/86
Semiotic TriangleThought or Reference
Symbol Referent
Correct symbol symbolises
Adequate thought refers to
True symbol stands for
Danny
approached
the chair with
a yellow bag.
λp.λc.λb.person(p)
∧chair(c)∧bag(b)
∧yellow(b)∧has(p,b)
∧approach(p,c)
λp.λc.λb.person(p)
∧chair(c)∧bag(b)
∧yellow(b)∧has(c,b)
∧approach(p,c)
Lambda calculus makes the difference clear. 7/86
Semiotic TriangleThought or Reference
Symbol Referent
Correct symbol symbolises
Adequate thought refers to
True symbol stands for
Danny
approached
the chair with
a yellow bag.
NN activations when processing the videos will somehow differ, too. 8/86
Why is Meaning Important in MT
Translation = expressing the same meaning in another language.
A meaning-aware translator (human or machine) will:
1. Use context to disambiguate as much as possible.
2. Ask around to learn about and understand the situation described.
3. Ideally warn the audience about unresolved ambiguities.
9/86
Recent Performance of NMT on News
Seg-Level English→Czech 2018
Ave. % Ave. z System
1 84.4 0.667 CUNI-Transformer
2 79.8 0.521 uedin
78.6 0.483 Professional Translation
4 68.1 0.128 online-B
5 59.4 −0.178 online-A
6 54.1 −0.354 online-G
Doc-Aware English→German 2019
Ave. Ave. z System
90.3 0.347 Facebook-FAIR
93.0 0.311 Microsoft-WMT19-sent-doc
92.6 0.296 Microsoft-WMT19-doc-level
90.3 0.240 Professional Translation
87.6 0.214 MSRA-MADL
88.7 0.213 UCAM
89.6 0.208 NEU
87.5 0.189 MLLP-UPV
87.5 0.130 eTranslation
86.8 0.119 dfki-nmt
84.2 0.094 online-B
… 10 more systems here …
76.3 −0.400 online-X
43.3 −1.769 en-de-task
See lecture #1 for all caveats of MT evaluation.
10/86
Do Recent Best Systems Understand?
• NMT systems are trained on millions of documents.
• To read the source and target training data of CUNI-Transformer
you would need 50 years, 8 hours a day, no weekends.
(Only 40% of it was parallel.)
• NNs create internal representations.
… So perhaps these representations are meaningful?
Test it yourself (English↔Czech):
https://lindat.mff.cuni.cz/services/transformer/
11/86
Do Recent Best Systems Understand?
12/86
Do Recent Best Systems Understand?
13/86
Do Recent Best Systems Understand?
14/86
Do Recent Best Systems Understand?
15/86
Do Recent Best Systems Understand?
16/86
Representations
Defining Representations
Given:
• a neural network trained to predict̂ 𝑦𝑖 ∈ 𝒴 given 𝑥𝑖 ∈ 𝒳,
• and a cut 𝐶 of that network
• (a set of neurons s.t. every path from input to output has to intersect it),
a representation is the mapping from 𝒳 to ℋ, where
• ℋ is the vector space of observed activations of neurons in 𝐶
(in some arbitrary fixed order).
17/86
Two Cuts Here: (1) Input, (2) Hidden Layer
18/86
The Learned Representation
Original space allows to:
• plot input data,
• visualize separation boundaries
for the first as well as
subsequent layers.
Hidden space ℋ allows to:
• to linearly separate the classes.
… but is it good for anything
else?
19/86
Good Representations (1/2)
20/86
Good Representations (1/2)
(𝑎, 𝑏, 𝑐) is a good representation
because it separates border from face features
20/86
Good Representations (2/2)
21/86
Good Representations (2/2)
(𝑎, 𝑏, 𝑐) is a good representation
because it resembles a known picture
21/86
Which Representations Are Good?
Ideas for a start:
• Good representations allow to solve some main task.
• … but for this, the NN was trained in the first place.
• Good representations allow to solve some other task.
• Pretrained word embeddings may be useful for other tasks.
• Good representations serve well on task interfaces.
• Divide-and-conquer vs. end-to-end training.
• Must divide training to make use of different data sources
(e.g. spoken language translation needs ASR and MT data).
• Good representations “make sense”.
• The representation of a specific test set resembles something known.
• Attaching a single layer to the representation gives a good accuracy in
something, e.g. part-of-speech tags from sentence embeddings.
22/86
Word Representations
Word Embeddings
• Map each word to a dense vector.
• In practice 300–2000 dimensions are used.
• The dimensions have no clear interpretation.
• Embeddings are trained for each particular task.
• NNs: The matrix that maps 1-hot input to the first layer.
• The famous word2vec (Mikolov et al., 2013):
• CBOW: Predict the word from its four neighbours.
• Skip-gram: Predict likely neighbours given the word.Input layerHidden layerOutput layerx1x2x3xkxVy1y2y3yjyVh1h2hihNWV×N={wki}W'N×V={w'ij}
Right: CBOW with just a single-word context (http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf)
23/86
Emergent Continuous Space of Words
Word2vec embeddings show interesting properties:
𝑣(king) − 𝑣(man) + 𝑣(woman) ≈ 𝑣(queen) (1)
Illustrations from https://www.tensorflow.org/tutorials/word2vec 24/86
Testset by Mikolov et al. (2013)
Question Type Sample Pair
capital-countries Athens – Greece
capital-world Abuja – Nigeria
currency Algeria – dinar
city-in-state Houston – Texas
family boy – girl
adjective-to-adverb calm – calmly
opposite aware – unaware
comparative bad – worse
superlative bad – worst
present-participle code – coding
nationality-adjective Albania – Albanian
past-tense dancing – danced
plural banana – bananas
plural-verbs decrease – decreases
25/86
Caveat on Evaluation (1/2)
Consider word2vec “comprehensive” test set (Mikolov et al., 2013):
• 8.8k “semantic” and 10.6k “syntactic” questions,
• w2v “accuracy is quite good” (eyeballing)
• The authors do mention that exact-match is “only about 60%”).
Kocmi and Bojar (2016) carefully examined the test set:
• “Semantic” questions cover only 3 question types:
• country→city, country→currency, masculine family member→ feminine
• Vylomova et al. (2016) test many other relations, e.g. walk-run,
dog-puppy, bark-dog, cook-eat.
• “Syntactic” questions constructed by combinations:
• starting from only 313 distinct word pairs,
• (leading to only 35 different pairs per question on average),
• And of the 313 pairs, 286 are formed regularly.
26/86
Caveat on Evaluation (2/2)
Test Set by
Accuracy on “Synt Questions” Mikolov et al. Kocmi et al.
word2vec as released 62.5% 43.5%
27/86
Caveat on Evaluation (2/2)
Test Set by
Accuracy on “Synt Questions” Mikolov et al. Kocmi et al.
word2vec as released 62.5% 43.5%
word2vec trained on our data 42.5% 9.7%
SubGram trained on our data 42.3% 22.4%
27/86
Caveat on Evaluation (2/2)
Test Set by
Accuracy on “Synt Questions” Mikolov et al. Kocmi et al.
word2vec as released 62.5% 43.5%
word2vec trained on our data 42.5% 9.7%
SubGram trained on our data 42.3% 22.4%
Nine rules 71.9% 66.4%
27/86
Caveat on Ultimate Evaluation
Kocmi and Bojar (2016):
• submitted to TSD on March 22, 2016.
• appeared in TSD in September 2016.
… cited by 7.
Bojanowski et al. (2017):
• submitted to arxiv on July 15, 2016.
• appeared in TACL 2017.
… cited by 2994.
28/86
Caveat on Ultimate Evaluation
Kocmi and Bojar (2016):
• submitted to TSD on March 22, 2016.
• appeared in TSD in September 2016.
… cited by 7.
• No code released, no fast code implemented at all.
Bojanowski et al. (2017):
• submitted to arxiv on July 15, 2016.
• appeared in TACL 2017.
… cited by 2994.
• This is the FastText paper.
28/86
Evaluating Words against Human Assessment?
The whole idea of evaluating word vectors by relating to human
judgements is risky.
• Human-produced datasets are subjective.
• Similarity vs. relatedness.
• Relatedness: teacher ≈ student, coffee ≈ cup
• Similarity: teacher ≈ professor, car ≈ train
• Hill et al. (2017) observed a soft tendency:
• Monolingual models reflect non-specific relatedness,
• NMT models reflect conceptual similarity.
• We saw that too for English-Czech (Abdou et al., 2017).
• Even if we distinguish them, which should be reflected in embeddings?
Details: Faruqui et al. (2016); Survey of eval. methods: Bakarov (2018)
29/86
Sentence Representations
Encoder-Decoder Architecture
https://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-2/ 30/86
Continuous Space of Sentences−15 −10 −5 0 5 10 15 20
−20
−15
−10
−5
0
5
10
15
I gave her a card in the garden
In the garden , I gave her a card
She was given a card by me in the garden
She gave me a card in the garden
In the garden , she gave me a card
I was given a card by her in the garden
2-D PCA projection of 8000-D space representing sentences (Sutskever et al., 2014). 31/86
Fixed-Length Representation of Sentences??
Raymond Mooney:
You can’t cram the meaning of
a whole %&!$ing sentence into a single $&!*ing vector!
32/86
Aspects of Meaning
Aspects of Meaning (1/2)
• Meaning can be seen as a coarsening:
• Pictures: Semantic segmentation.
33/86
Aspects of Meaning (1/2)
• Meaning can be seen as a coarsening:
• Pictures: Semantic segmentation.
• Programs: The output they give (caveat: undecidable).
• Sentences: Reference to real world? Speaker’s intention?
33/86
Aspects of Meaning (1/2)
• Meaning can be seen as a coarsening:
• Pictures: Semantic segmentation.
• Programs: The output they give (caveat: undecidable).
• Sentences: Reference to real world? Speaker’s intention?
• Linguistic meaning captures the structure of expressions:
• Morphology, syntax, …
• Units of each layer composed into higher units (FGD, Sgall et al. (1986))
Illustration from http://www.cs.toronto.edu/~tingwuwang/semantic_segmentation.pdf.
33/86
Aspects of Meaning (2/2)
Aspects of sentence meaning as listed by Bojar et al. (2019).
Symbolic Continuous
Aspect of Meaning Theories Representations
Abstraction 3 ×
Compositionality 3 ∼
Learnability ? 3
Relatability (similarity, operations) ∼ ∼
Vagueness of Meaning × 3
Ambiguity of Expressions 3 ×
Statefulness ∼ 3
34/86
Compositionality of Meaning
Manning (2015):
Understanding novel and complex sentences crucially de-
pends on being able to construct their meaning compositionally
from smaller parts—words and multiword expressions—of which
they are constituted.
35/86
Compositionality of Vector Representations
Karlgren and Kanerva (2019) show “Holographic Reduced Reprs.”:
• Addition: Preserves similarity, useful to represent bag-of-…
• Hadamard product (elem-wise multiplication),
• Invertible; product dissimilar to its operands: 𝐴 ∗ 𝐵 ≁ 𝐴.
• Bipolar vectors ({−1, +1}𝑛) are inverse of themselves.
• Can represent variable assignment {𝑥 = 𝑎, 𝑦 = 𝑏, 𝑧 = 𝑐} using bipolar
vectors 𝑋, 𝑌 , and 𝑍 added into a vector (𝑋 ∗ 𝐴) + (𝑌 ∗ 𝐵) + (𝑍 ∗ 𝐶).
To recover the value of 𝑥, multiply by 𝑋:
𝑋 ∗ (𝑋 ∗ 𝐴) + 𝑋 ∗ (𝑌 ∗ 𝐵) + 𝑋 ∗ (𝑍 ∗ 𝐶)) = 𝐴 + noise + noise ∼ 𝐴
• Vector elements permutation,
• Also invertible; dissimilar; enormous number of permutations.
• Useful to represent structures, e.g. lists: 𝛱1 for CAR 𝛱2 for CDR:
(𝑎, 𝑏) represented with 𝛱1(𝑎) + 𝛱2(𝑏)
(In highly-dimensional spaces, most vectors are dissimilar; cosine or Pearson correlation of 0.25 indicate close similarity.) 36/86
Modelling Ambiguity?
Sentence-level embeddings always produced by an encoder.
• Encoder = A deterministic mapping from expression to meaning.
• Unclear how ambiguous expressions are and should be represented.
• Same problem with word vectors already:
Ideally, an expression would correspond to
a distribution over semantic space, not a single point. 37/86
Meaning Statefulness
Stateful Meaning Representation:
• Could be modelled by decoder state:
≈ “State of mind after reading the input and producing a partial output.”
• Better reflected in models with attention.
• Btw needed to interpret humour (Gluscevskij, 2017).
Stateless Meaning Representation:
• Encoder state.
• Points correspond to expressions.
• Ambiguity representation unclear.
38/86
Is Sentence Meaning Continuous?
We know that one Arabic sentence can have dozens of thousands of
English translation (Dreyer and Marcu, 2012):
Premiere of Iraq Nuri al-Maliki was given an excuse by President Bush, who expressed his confidence
in him, and he stated that the circumstances are complicated.
President Bush said that he trusts in Nouri Maliki, head of government of Iraq, and he stated that
he finds an excuse for him ”because the situation is tricky”.
Head of cabinet of Iraq Nuri al-Maliki was given an excuse by President Bush, who expressed his
trust in him, and he indicated that the circumstances are difficult.
Iraq’s head of cabinet Nuri al-Maliki was given a reason by President Bush, who expressed his trust
in him, and he indicated that the case is tricky.
President Bush said that he has faith in Iraqi head of cabinet Nouri al-Maliki, and he stated that he
finds an excuse for him ”for the case is complicated”.
39/86
Is Sentence Meaning Continuous?
Similarly: 70k Czech translations of 1 English sentence (Bojar et al., 2013)
And even though he is a political veteran, the Councilor Karel Brezina responded similarly.
A ačkoli ho lze považovat za politického veterána, radní Březina reagoval obdobně.
A i přestože je politický matador, radní Karel Březina odpověděl podobně.
Byť ho lze označit za politického veterána, Karel Březina reagoval podobně.
Byť ho můžeme prohlásit za politického veterána, byla i odpověď K. Březiny velmi podobná.
K. Březina, i když ho lze prohlásit za politického veterána, odpověděl velmi obdobně.
Odpověď Karla Březiny byla podobná, navzdory tomu, že je politickým veteránem.
Radní Březina odpověděl velmi obdobně, navzdory tomu, že ho lze prohlásit za politického veterána.
Reakce K. Březiny, třebaže je politický veterán, byla velmi obdobná.
Velmi obdobná byla i odpověď Karla Březiny, ačkoli ho lze prohlásit za politického veterána.
Q: Are all these paraphrases close in sent embedding spaces?
Q: How entagled are manifolds of different sents?
… work in progress with Petra Barančíková 40/86
Examining Continuous Space
Proposed strategy:
1. Propose directions of exploration.
2. Generate seed pairs of sentences for each of the directions.
3. Collect specimens along the proposed directions:
• interpolation, a “sentence in between”,
• extrapolation, “a sentence further in the hinted direction”.
• Allow people to say “impossible”.
4. Validate the relations.
5. Create the partially ordered set.
6. Search for a manifold covering the ordered set.
Some first ideas explored with Chris Callison-Burch.
First dataset for Czech released (Barančíková and Bojar, 2020). 41/86
Directions of Exploration (1/2)
• Politeness
• Tense
• Verity: How much the speaker believes the message.
• Modality: Willingness/Ability of the speaker to do it.
• “Counting” / Generic Numerals, Scalar adjectives
• I saw a handful of people there. / a big crowd / a massive crowd.
• freezing / cold / chilly
• “Negation”, but not only reversing the main predicate
• Complexity / simplicity, Length.
Thanks to Šárka Zikánová for some of the ideas.
42/86
Directions of Exploration (2/2)
• Specificity / Generality, Vagueness.
• Geese fly / Geese migrate / Geese migrate south / The Canadian geese
flew over the pond at friendly Farms in their southward migration.
• Hammer the hook into the wall. / Put the hook on the wall. / Do the
thingy in there.
• Contextual boundness.
• Give it to him. / Give the parcel to the man at the counter. / Give your
parcel to the operator at the post office.
• High/low style/English/class.
• Hey y’all it’s a nice day ain’t it?
• Greetings! Lovely weather we are having.
43/86
First Results of Getting Pairs
Can you please give me a minute? Could you leave me alone?
Close the door. Close the damn door man
Can you help me find something? I need you to help me get something.
May I talk to Mary? Is Mary here?
I’m sorry-I don’t believe we have met. Who the hell are you?
Can you move so I can see the screen? You aren’t made of glass, you know.
Will you kindly exit? I do not want you here!
Would you please get the mail? Get the mail!
Can I help you? What do you want?
Can you please help me with this? Get over here and help me!
Can you make me breakfast? Why are you not making me breakfast right now?
I tried to call were you busy? You never answer your phone.
44/86
First Results of Midpointing
Can you move so I can see the screen?
Blocking the view, friend.
Move your blocking the screen
Could you move a little bit, you’re blocking the screen.
Can you please move?
I can’t see, can you move a little?
Hey can you move.
Please move.
Can you move a bit?
You aren’t made of glass, you know.
45/86
After the Midpointing…Can you hurry eating?
All done?
Are you almost done eating?
Are you done eating yet?
Are you finished with your food?
Are you finished with your food yet?
Done with the food?Finished yet?
When will you be done with your food?
You're still not done with your food?
46/86
Ask Crowd to Partially Sort ThemCan you hurry eating?
All done?
Are you almost done eating?
Are you done eating yet?
Are you finished with your food?
Are you finished with your food yet?
Done with the food?Finished yet?
When will you be done with your food?
You're still not done with your food?
Can you hurry eating?
All done?
Are you almost done eating?
Are you done eating yet?
Are you finished with your food?
Are you finished with your food yet?
Done with the food?Finished yet?
When will you be done with your food?
You're still not done with your food?
47/86
Find Methods for Manifold LearningCan you hurry eating?
All done?
Are you almost done eating?
Are you done eating yet?
Are you finished with your food?
Are you finished with your food yet?
Done with the food?Finished yet?
When will you be done with your food?
You're still not done with your food?
Can you hurry eating?
All done?
Are you almost done eating?
Are you done eating yet?
Are you finished with your food?
Are you finished with your food yet?
Done with the food?Finished yet?
When will you be done with your food?
You're still not done with your food?
Manifold learning
48/86
Match Posets with Learned ManifoldsCan you hurry eating?
All done?
Are you almost done eating?
Are you done eating yet?
Are you finished with your food?
Are you finished with your food yet?
Done with the food?Finished yet?
When will you be done with your food?
You're still not done with your food?
Can you hurry eating?
All done?
Are you almost done eating?
Are you done eating yet?
Are you finished with your food?
Are you finished with your food yet?
Done with the food?Finished yet?
When will you be done with your food?
You're still not done with your food?
Manifold learning
Can you hurry eating?
All done?
Are you almost done eating?
Are you done eating yet?
Are you finished with your food?
Are you finished with your food yet?
Done with the food?
Finished yet?
When will you be done with your food?
You're still not done with your food?
Can you hurry eating?
All done?
Are you almost done eating?
Are you done eating yet?
Are you finished with your food?
Are you finished with your food yet?
Done with the food?
Finished yet?
When will you be done with your food?
You're still not done with your food?
Manifold learning
semi-supervised.
49/86
Evaluating Sentence
Representations
Evaluating Sentence Representations
Conneau and Kiela (2018) introduce SentEval:
• Given a sentence representation function, assess the fitness of the
representation in multiple tasks.
https://github.com/facebookresearch/SentEval/
Conneau et al. (2018) and others then compare several reprs incl.:
• SkipThough (Kiros et al., 2015):
• Predict sentence given the surrounding sentences.
• InferSent (Conneau et al., 2017):
• Train sentence representations on predicting entailment.
Extremely active research area, see BlackboxNLP workshops.
50/86
How “Semantic” are Seq2Seq Reprs?
Cífka and Bojar (2018):
• Trained several variations of Cho et al. (2014).
• Extracted sentence representations.
• Related BLEU and “semantics” of the representation:
• Evaluation through classification.
• Evaluation through similarity.
• Evaluation using paraphrases.
51/86

































Summary
• NMT systems can surpass humans within the given domain.
• We discussed learned representations.
• Illustrated word and sentence embeddings.
• We discussed aspects of meaning.
• Some level of “understanding” can be found in the representations.
• Follow the BlackBoxNLP workshops:
• POS, Syntax, Word Derivations, Compositionality…
• Still very far from human understanding.
• Big caveats need to be taken when interpreting results.
• The “utility” of syntax in NMT discussed last week.
• The exact composition of the task and the test set.
85/86
References
Mostafa Abdou, Vladan Gloncak, and Ondřej Bojar. 2017. Variable mini-batch sizing and pre-trained embeddings. In
Proceedings of the Second Conference on Machine Translation, pages 680–686, Copenhagen, Denmark, September.
Association for Computational Linguistics.
Amir Bakarov. 2018. A survey of word embeddings evaluation methods. CoRR, abs/1801.09536.
Petra Barančíková and Ondřej Bojar. 2020. COSTRA 1.0: A Dataset of Complex Sentence Transformations. In
Proceedings of the LREC 2020. ELRA.
Yevgeni Berzak, Andrei Barbu, Daniel Harari, Boris Katz, and Shimon Ullman. 2015. Do you see what I mean? visual
resolution of linguistic ambiguities. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language
Processing, pages 1477–1487, Lisbon, Portugal, September. Association for Computational Linguistics.
Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword
information. Transactions of the Association for Computational Linguistics, 5:135–146.
Ondřej Bojar, Matouš Macháček, Aleš Tamchyna, and Daniel Zeman. 2013. Scratching the Surface of Possible
Translations. In Proc. of TSD 2013, Lecture Notes in Artificial Intelligence, Berlin / Heidelberg. Západočeská univerzita
v Plzni, Springer Verlag.
Ondřej Bojar, Raffaella Bernardi, and Bonnie Webber. 2019. Representation of sentence meaning (a jnle special issue).
Natural Language Engineering, 25(4).
Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. 2014. Learning phrase representations using rnn encoder–decoder for statistical machine translation. In
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages
1724–1734, Doha, Qatar, October. Association for Computational Linguistics.
Ondřej Cífka and Ondřej Bojar. 2018. Are BLEU and Meaning Representation in Opposition? In Proceedings of the
56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1362–1371. 86/86


Multilingual
Machine Translation
Ondřej Bojar
April 30, 2020
NPFL087 Statistical Machine Translation
Charles University
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics unless otherwise stated
Outline
• Motivation for using more than 2 languages.
• Transfer Learning.
• Catastrophic Forgetting.
• Trivial Transfer Learning.
• Multi-Lingual NMT.
• Massively Multi-Lingual NMT.
Many slides on transfer learning by Tom Kocmi.
Many slides on multilingual models by Rico Sennrich and Adam Lopez.
1/70
Why Multilingual MT
• Help in low-resource settings.
• Words, morphemes or syntactic patterns common to more languages.
• Learning can reuse patterns seen in another dataset.
• Improve translation quality.
• Words are ambiguous, the third language can disambiguate.
• Truly multi-lingual environments.
• United Nations: 6 languages.
• EU official languages: 24.
• EUROSAI official languages: 43.
• INTOSAI official languages…
2/70
Transfer Learning
Motivation for NN Transfer LearningPerformance
Training steps
Without transfer learning
3/70
Motivation for NN Transfer LearningPerformance
Training steps
Without transfer learning
With transfer learning
4/70
Motivation for NN Transfer LearningPerformance
Training steps
Without transfer learning
With transfer learning
Better initial performace
Steeper slope
Better final performance
5/70
Steps of Transfer Learning
6/70
Steps of Transfer Learning
7/70
Interlude: Catastrophic Forgetting
• Kocmi and Bojar (2017) explore curriculum learning:
• Start with simpler sentences first, add complex ones later.0
5
10
15
0 10 20 30 40 50
BLEU
Steps (in millions examples)
Baseline
8/70
Interlude: Catastrophic Forgetting
• Kocmi and Bojar (2017) explore curriculum learning:
• Start with simpler sentences first, add complex ones later.
• When “simpler” means “shorter”:0
5
10
15
0 10 20 30 40 50
BLEU
Steps (in millions examples)
Baseline
Sorted by length
8/70
Interlude: Catastrophic Forgetting
• Kocmi and Bojar (2017) explore curriculum learning:
• Start with simpler sentences first, add complex ones later.
• When “simpler” means “shorter”:
• Clear jumps in score as bins of longer sentences are allowed.0
5
10
15
0 10 20 30 40 50
BLEU
Steps (in millions examples)
Baseline
Sorted by length
Curriculum by target length
8/70
Interlude: Catastrophic Forgetting
• Kocmi and Bojar (2017) explore curriculum learning:
• Start with simpler sentences first, add complex ones later.
• When “simpler” means “shorter”:
• Clear jumps in score as bins of longer sentences are allowed.
• Reversed curriculum unlearns to produce long sentences.0
5
10
15
0 10 20 30 40 50
BLEU
Steps (in millions examples)
Baseline
Sorted by length
Curriculum by target length
Reversed Curriculum by target length
8/70
Trivial Transfer Learning
• Early works (Zoph et al., 2016; Nguyen and Chiang, 2017) target
one common language (English).
• Kocmi and Bojar (2018) try even unrelated languages.
The trivial procedure:
• Train on one pair (“parent”), switch corpus to another (“child”).
• The only requirement: joint subword units across all langs.
9/70
Getting Balanced Vocabulary
10/70
Getting Balanced Vocabulary
11/70
Getting Balanced Vocabularythe_
že_
ying_
staying_
pra
pracovat_
...
12/70
English on Same SideParent model Corpus size
difference
Direction Baseline
(BLEU)
Transfer
(BLEU)
Δ
(BLEU)
Czech 9x from English 16.13 17.75 1.62 *
Czech 9x to English 19.19 22.42 3.23 *
Child model: Slovak
* statistically significant
13/70
English on Same SideParent model Corpus size
difference
Direction Baseline
(BLEU)
Transfer
(BLEU)
Δ
(BLEU)
Czech 9x from English 16.13 17.75 1.62 *
Czech 9x to English 19.19 22.42 3.23 *
Child model: Slovak
* statistically significant
14/70
English on Same SideParent model Corpus size
difference
Direction Baseline
(BLEU)
Transfer
(BLEU)
Δ
(BLEU)
Czech 9x from English 16.13 17.75 1.62 *
Czech 9x to English 19.19 22.42 3.23 *
Child model: Slovak
* statistically significant
Parent model Corpus size
difference
Direction Baseline
(BLEU)
Transfer
(BLEU)
Δ
(BLEU)
Finnish 3.5x from English 17.03 19.74 2.71 *
Russian 16x from English 17.03 20.09 3.06 *
Czech 50x from English 17.03 20.41 3.38 *
Finnish 3.5x to English 21.74 24.18 2.44 *
Russian 16x to English 21.74 23.54 1.80 *
Child model: Estonian
15/70
English on Same SideParent model Corpus size
difference
Direction Baseline
(BLEU)
Transfer
(BLEU)
Δ
(BLEU)
Czech 9x from English 16.13 17.75 1.62 *
Czech 9x to English 19.19 22.42 3.23 *
Child model: Slovak
* statistically significant
Parent model Corpus size
difference
Direction Baseline
(BLEU)
Transfer
(BLEU)
Δ
(BLEU)
Finnish 3.5x from English 17.03 19.74 2.71 *
Russian 16x from English 17.03 20.09 3.06 *
Czech 50x from English 17.03 20.41 3.38 *
Finnish 3.5x to English 21.74 24.18 2.44 *
Russian 16x to English 21.74 23.54 1.80 *
Child model: Estonian
16/70
English on Same SideParent model Corpus size
difference
Direction Baseline
(BLEU)
Transfer
(BLEU)
Δ
(BLEU)
Czech 9x from English 16.13 17.75 1.62 *
Czech 9x to English 19.19 22.42 3.23 *
Child model: Slovak
* statistically significant
Parent model Corpus size
difference
Direction Baseline
(BLEU)
Transfer
(BLEU)
Δ
(BLEU)
Finnish 3.5x from English 17.03 19.74 2.71 *
Russian 16x from English 17.03 20.09 3.06 *
Czech 50x from English 17.03 20.41 3.38 *
Finnish 3.5x to English 21.74 24.18 2.44 *
Russian 16x to English 21.74 23.54 1.80 *
Child model: Estonian
Related
Cyrillic
Related
Related
Related
Cyrillic
Biggest
17/70
English on Same Side, Parent Low-ResourceParent model Corpus size
difference
Direction Baseline
(BLEU)
Transfer
(BLEU)
Δ
(BLEU)
Estonian 0.3x from English 19.50 20.07 0.57 *
Estonian 0.3x to English 24.40 23.95 -0.45
Parent model Corpus size
difference
Direction Baseline
(BLEU)
Transfer
(BLEU)
Δ
(BLEU)
Slovak 0.1x from English 23.48 22.99 -0.49 *
Slovak 0.1x to English 29.61 28.20 -1.41 *
Child model: Finnish
Child model: Czech
18/70
English on Same Side, Parent Low-ResourceParent model Corpus size
difference
Direction Baseline
(BLEU)
Transfer
(BLEU)
Δ
(BLEU)
Estonian 0.3x from English 19.50 20.07 0.57 *
Estonian 0.3x to English 24.40 23.95 -0.45
Parent model Corpus size
difference
Direction Baseline
(BLEU)
Transfer
(BLEU)
Δ
(BLEU)
Slovak 0.1x from English 23.48 22.99 -0.49 *
Slovak 0.1x to English 29.61 28.20 -1.41 *
Child model: Finnish
Child model: Czech
19/70
English on the Other SideParent
model
Child model Corpus size
amplification
Baseline
(BLEU)
Transfer
(BLEU)
Δ
(BLEU)
Parent
Aligned Δ
EN - Finnish Estonian - EN 3.5x 21.74 22.75 1.01 * 2.44 *
EN - Russian Estonian - EN 16x 21.74 23.12 1.38 * 1.80 *
EN - Czech Estonian - EN 50x 21.74 22.80 1.06 *
Finnish - EN EN - Estonian 3.5x 17.03 18.19 1.16 * 2.71 *
Russian - EN EN - Estonian 16x 17.03 18.16 1.13 * 3.06 *
20/70
No Language in CommonParent model Corpus size
amplification
Baseline
(BLEU)
Transfer
(BLEU)
Δ
(BLEU)
Arabic - Russian 12x 21.74 22.23 0.49
Spanish - French 12x 21.74 22.24 0.50 *
Spanish - Russian 12x 21.74 22.52 0.78 *
French - Russian 12x 21.74 22.40 0.66 *
Child model: Estonian to English
21/70
No Language in CommonParent model Corpus size
amplification
Baseline
(BLEU)
Transfer
(BLEU)
Δ
(BLEU)
Arabic - Russian 12x 21.74 22.23 0.49
Spanish - French 12x 21.74 22.24 0.50 *
Spanish - Russian 12x 21.74 22.52 0.78 *
French - Russian 12x 21.74 22.40 0.66 *
Child model: Estonian to English
CyrillicArabic
Cyrillic
Cyrillic
22/70
The Better the Parent, the Better the Child12
14
16
18
0 250 500 750 1000
BLEU
Steps (in thousands)
Baseline en-et only
en-et after 50k of en-f
en-et after 100k of en-f
en-et after 200k of en-f
en-et after 400k of en-f
en-et after 800k of en-f
Non-comparable English-Finnish
23/70
The Lesser the Child, the Bigger the Gain10k 50k 100k 200k 400k 800k
0
5
10
15
20
1.95
5.74
9.39
11.96
14.94
17.03
12.46
15.95 17.61 17.95 19.04 19.74
BLEU
24/70
Why it Helps? Not Really Vocabulary (1/2)
Length BLEU Components BP
Base ENET 35326 48.1/21.3/11.3/6.4 0.979
ENRU+ENET 35979 51.0/24.2/13.5/8.0 0.998
ENCS+ENET 35921 51.7/24.6/13.7/8.1 0.996
(The reference length in the matching tokenization was 36062.)
• Child models produce longer outputs ⇒ lower brevity penalty.
• But 𝑛-gram precisions also better.
1-gram present in ENRU+ENET ENCS+ENET
Child, Base, Ref 15902 (44.2 %) 15924 (44.3 %)
Child only 9635 (26.8 %) 9485 (26.4 %)
Child, Base 7209 (20.0 %) 7034 (19.6 %)
Child, Ref 3233 (9.0 %) 3478 (9.7 %)
Total 35979 (100.0 %) 35921 (100.0 %)
• The 3k better toks are regular ET words, not NEs or numbers. 25/70
Why it Helps? Not Really Vocabulary (2/2)
26/70
Why it Helps? Sentence Lengths SomewhatParent Child
Sentence lengths BLEU Avg. words BLEU Avg. words
1-10 words 8.57 10.9 16.57 15.3
10-20 words 16.21 15.4 17.48 15.3
20-40 words 12.59 21.9 17.99 15.3
40-60 words 5.76 35.5 16.80 15.5
1-60 words 22.30 15.3 19.15 15.4
27/70
Why it Helps? Sentence Lengths SomewhatParent Child
Sentence lengths BLEU Avg. words BLEU Avg. words
1-10 words 8.57 10.9 16.57 15.3
10-20 words 16.21 15.4 17.48 15.3
20-40 words 12.59 21.9 17.99 15.3
40-60 words 5.76 35.5 16.80 15.5
1-60 words 22.30 15.3 19.15 15.4
28/70
Why it Helps? Sentence Lengths SomewhatParent Child
Sentence lengths BLEU Avg. words BLEU Avg. words
1-10 words 8.57 10.9 16.57 15.3
10-20 words 16.21 15.4 17.48 15.3
20-40 words 12.59 21.9 17.99 15.3
40-60 words 5.76 35.5 16.80 15.5
1-60 words 22.30 15.3 19.15 15.4
29/70
Multilingual MT
Multilingual MT Configurations
• Pivot translation (Cascading).
• Multi-lingual source (also called multi-way).
• Multi-lingual multi-source.
• Multi-lingual target.
• Multi-lingual multi-target.
• Both sides multi-lingual.
• (Both sides multi-lingual, multi-source, multi-target. ;-)
• Zero-shot training.
• i.e. translating an unseen pair when both the source and target langs were
covered in the training data in other pairs.
• “Beyond zero-shot” is translating from an unseen language.
30/70
Multi-Target and Multi-Source MT
• Multi-Target focus: Efficiency
• Decrease hardware resources compared using many separate models.
• Multi-Source focus: Resolving ambiguity thanks to existing translation
• E.g. Translating German “Schloss” to French is easier
if we can feed in the English translation (“castle” or “lock”).
• Training on: Multi-parallel or bi-parallel multilingual corpora.cs
…
Neural Network
de
fr
en
Figure 1: Multi-Target MTNeural NetworkNeural Network
fr
en
cs
it
…
+
+
+ Figure 2: Multi-Source MT 31/70
Ideal: Flexible Multi-Lingual MTru
Neural NetworkNeural Network
en
cs
it
…
AND/OR
sk
de
fr
...
AND/OR
AND/OR
Figure 3: Flexible multilingual MT
32/70
Multi-source translation
Quite an old idea (e.g. Och & Ney 2001)33/70
Multi-source translation
• Assorted techniques to do this in IBM-style or phrase-
based MT.
• Difficult to model directly due to independence
assumptions of these models.
• Usually done as a kind of system combination
(merging the output of two MT systems).
• But this introduces other problems, e.g. decoding.
• Fundamentally, it’s interpolation of conditional LMs.34/70
Direct multi-source
Zoph & Knight 2016
• Directly learns and uses p(English|French,German)
• For attention: two context vectors (uses p-local attention of
Luong, et al, but could use other methods).35/70
Multi-way MT
Firat et al. 2016 (two papers)
• Assume only many bilingual parallel corpora.
• For N languages: learn N encoders and N decoders.
• But what about attention?36/70
Multi-way MT
Firat et al. 2016 (two papers)
• Assume only many bilingual parallel corpora.
• For N languages: learn N encoders and N decoders.
• But what about attention?
p(fi|fi−1, ..., f1, e) = g(fi−1, si, ci)
ci =
|e|
∑
j=1
αij hj
αij = exp(aij )
∑|e|
k=1 exp(aik)
aij = a(si−1, hj )37/70
Multi-way MT
Firat et al. 2016 (two papers)
• As in Bahdanu et al. (2014), attention mechanism is
a feedforward function of both decoder hidden state
and encoder context vector.
• Shared between all encoders and decoders.
p(fi|fi−1, ..., f1, e) = g(fi−1, si, ci)
ci =
|e|
∑
j=1
αij hj
αij = exp(aij )
∑|e|
k=1 exp(aik)
aij = a(si−1, hj )
Everything
we need is
right here!38/70
Multi-way MT
Firat et al. 2016 (two papers)
Low-resource simulation
(using high-resource
European languages)39/70
Multi-way MT
Firat et al. 2016 (two papers)40/70
Multi-way MT
Firat et al. 2016 (two papers)
ok, but what about multi-source?41/70
Multi-way multi-source MT
Firat et al. 2016 (two papers)
• Still assumes only many bilingual parallel corpora.
• What to do if there are multiple input sentences?
• Early averaging (average context vectors).
• Late averaging (aka linear interpolation).
Early and late averaging are orthogonal, can be combined.42/70
43/70
Multi-way multi-source MT
Firat et al. 2016 (two papers)44/70
45/70
Zero-shot MT
Firat et al. 2016 (two papers)
• Suppose our bilingual parallel data include a pair of
languages for which we have no parallel data.
• Q: Can we use the multi-way encoder-decoder system
to translate Spanish into French?
English EnglishSpanish French46/70
47/70
48/70
Zero-shot MT
Firat et al. 2016 (two papers)
• Finetuning: what if we use a small amount of
parallel data in this setting?
• Q: Where would we get this data? Backtranslation
English EnglishSpanish French49/70
Zero-shot MT
Firat et al. 2016 (two papers)
• Finetuning: what if we use a small amount of
parallel data in this setting?
• Q: Where would we get this data? Backtranslation
English EnglishSpanish French
Spanish (MT)50/70
Zero-shot MT
Firat et al. 2016 (two papers)
• Finetuning: what if we use a small amount of
parallel data in this setting?
• Q: Where would we get this data? Backtranslation
English EnglishSpanish French
Spanish (MT)51/70
Zero-shot MT
Firat et al. 2016 (two papers)
• Finetuning: what if we use a small amount of
parallel data in this setting?52/70
Zero-shot MT
Firat et al. 2016 (two papers)
• Finetuning: what if we use a small amount of
parallel data in this setting?53/70
Simple Data Mixing
Do we really need separate encoders and decoders?
… simply feed in various language pairs:
Source Sent 1 (De) 2en versetzen Sie sich mal in meine Lage !
Target Sent 1 (En) put yourselves in my position .
Source Sent 2 (En) 2nl I flew on Air Force Two for eight years .
Target Sent 2 (Nl) ik heb acht jaar lang met de Air Force Two gevlogen .
• The model of the same size will learn both pairs.
• Hopefully benefiting from various similarities.
• Risk of catastrophic forgetting.
See Johnson et al. (2016) or Ha et al. (2017). 54/70
55/70
56/70
“Language Embeddings” from 927 BiblesEnglishmultilingual
NMT modelBible translations
in 927 languagesvector space
of language
embeddingslearning to translate
Portuguese - Swedish
(“zero-shot MT”)x1 x2 x3 x4
Ex
C
Z
Ey
y1 y2 y3
attention
encoder decoder
h1 h2 h3 h4trainlanguage
flags
input sentence
output sentence
Helsinki Neural
MT System
Tiedemann (2018) 57/70
“Language Embeddings” from 927 BiblesTrans-New Guinea
Otomanguean
Quechuan
Indo-European
Austronesian
Nilo-Saharan
Afro-Asiatic
Mayan
Niger-Congo
Creole
t-SNE of the language-embedding vectors, colored by language family. 58/70
Massively Multi-Lingual
Models
Available Data for EN↔100+ Langs
59/70
Translation Quality of Bilingual MT
60/70
Standard Transformer Model
61/70
Google Transformer Sizes
GPipe (Huang et al., 2019) introduces microbatches for faster training
of deep models across multiple GPUs.
Enc/Dec Depth FF Dim Heads Total Parameters GPUs Used
6 8192 16 400M 1 default
12 16384 32 1.3B 2 “wide”
24 8192 16 1.3B 4 “deep”
32 16384 32 3.0B 8
64 16384 32 6.0B 16
• “Deep” better than “wide” on low-resource languages.
• Indicates better generalization.
• Further tricks needed to keep the training stable.
62/70
Massively Multilingual Models
63/70
Massive Massively Multilingual Models
64/70
Google-Sized Experiment
The recent 50 billion parameters Transformer needed further trick:
• sparsely-gated mixture of experts (Shazeer et al., 2017):
⇒ BLEU on 100 langs re-gained and improved by 125x larger model.
https://ai.googleblog.com/2019/10/exploring-massively-multilingual.html
65/70
Domain Adapters to Recover Practical Sizes
• Bapna and Firat (2019) propose tiny tunable “adapter” layers.
1. Pretrain on a large mixed-language corpus.
2. Inject adapter layers.
3. Finetune adapter layers for each of the target tasks.
66/70
Domain Adapters into English
67/70
Domain Adapters from English
68/70
Summary
• Transfer learning in NMT works.
⇒ NMT can exploit more and less related data.
• Trivial Transfer: Parent just has to be larger.
• Even unrelated language pairs can help.
• Probably better initialization.
• Multi-source, multi-target, …, flexible multi-lingual setups.
• Language families emerge in language token embedding.
• Model capacity is the bottleneck.
• Models 125x large for 100 languages in one model
allow gains on high-resource languages, too.
• With tiny adaptors instead of mixture of experts
model sizes can decrease again.
69/70
References
Ankur Bapna and Orhan Firat. 2019. Simple, scalable adaptation for neural machine translation. In Proceedings of the
2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP), pages 1538–1548, Hong Kong, China, November. Association for
Computational Linguistics.
Thanh-Le Ha, Jan Niehues, and Alexander H. Waibel. 2017. Effective strategies in zero-shot neural machine
translation. CoRR, abs/1711.07893.
Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam,
Quoc V Le, Yonghui Wu, and zhifeng Chen. 2019. Gpipe: Efficient training of giant neural networks using pipeline
parallelism. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in
Neural Information Processing Systems 32, pages 103–112. Curran Associates, Inc.
Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda B.
Viégas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2016. Google’s multilingual neural
machine translation system: Enabling zero-shot translation. CoRR, abs/1611.04558.
Tom Kocmi and Ondřej Bojar. 2017. Curriculum Learning and Minibatch Bucketing in Neural Machine Translation. In
Proceedings of Recent Advances in NLP (RANLP 2017).
Tom Kocmi and Ondřej Bojar. 2018. Trivial Transfer Learning for Low-Resource Neural Machine Translation. In
Proceedings of the Third Conference on Machine Translation, Volume 1: Research Papers, volume 1, pages 244–252,
Stroudsburg, PA, USA. Association for Computational Linguistics, Association for Computational Linguistics.
Toan Q. Nguyen and David Chiang. 2017. Transfer learning across low-resource, related languages for neural machine
translation. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2:
Short Papers), pages 296–301. Asian Federation of Natural Language Processing.
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton, and Jeff Dean. 2017.70/70


Multi-Modal Translation
Speech and Vision
Ondřej Bojar
May 7, 2020
NPFL087 Statistical Machine Translation
Charles University
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics unless otherwise stated
Outline
• Overview of Multi-Modal Translation.
• Speech Translation ≈ ASR + MT.
• Problems at ASR-MT boundary.
• End-to-end SLT approaches.
• Visual information for MT.
Some pictures and tables from Sulubacak et al. (2019).
1/53
Overview of Multi-Modal MT
From survey by Sulubacak et al. (2019):MT TextText
2/53
Overview of Multi-Modal MT
From survey by Sulubacak et al. (2019):MT TextText
ASR
Speech
Language B
Language A
2/53
Overview of Multi-Modal MT
From survey by Sulubacak et al. (2019):MT TextText
ASR
Speech
Language B
Language A
TTS
Speech
2/53
Overview of Multi-Modal MT
From survey by Sulubacak et al. (2019):MT TextText
ASR
Speech
Language B
Language A
TTS
Speech
SLT
2/53
Overview of Multi-Modal MT
From survey by Sulubacak et al. (2019):MT TextText
ASR
Speech
Language B
Language A
TTS
Speech
SLT
S2S
2/53
Overview of Multi-Modal MT
From survey by Sulubacak et al. (2019):MT TextText
ASR
Speech
Language B
Language A
TTS
Speech
SLT
S2S
IGT
IC
IGT = image-guided
2/53
Overview of Multi-Modal MT
From survey by Sulubacak et al. (2019):MT TextText
ASR
Speech
Language B
Language A
TTS
Speech
SLT
S2S
IGT
IC
VGT
VD
IGT = image-guided, VGT = video-guided translation
2/53
Spoken Language
Translation
Basic Terms
• MT = Machine Translation = Text Translation
• Input are (mostly grammatically correct) individual sentences.
• Sentences may come in documents or not.
• (Document-level MT processes a sequence of sentences at once.)
3/53
Basic Terms
• MT = Machine Translation = Text Translation
• Input are (mostly grammatically correct) individual sentences.
• Sentences may come in documents or not.
• (Document-level MT processes a sequence of sentences at once.)
• Incremental MT
• MT of gradually growing input.
• MT decides whether to wait for more words or emit current word.
• Aims at stable output.
3/53
Basic Terms
• MT = Machine Translation = Text Translation
• Input are (mostly grammatically correct) individual sentences.
• Sentences may come in documents or not.
• (Document-level MT processes a sequence of sentences at once.)
• Incremental MT
• MT of gradually growing input.
• MT decides whether to wait for more words or emit current word.
• Aims at stable output.
• SLT = Spoken Language Translation
• Input is the sound in one language.
• Output is text (sometimes also speech).
• Sentences may or may not be assumed and produced.
• S2S = S2ST = Speech-to-Speech Translation
• Direct modelling, e.g. can aim to preserve voice or prosodics.
3/53
Spoken Language Translation
Cascaded ASR + MT
NN Prospects: ASR Surpassing Humans
• Switchboard conversational speech benchmark (2000).
• 40 phone calls between two random native English speakers.7x more datafirst DNNsequence losshuman level2012 2013 2014 2015 2016 2017
6
8
10
12
14
16
word
error
rate
Plot by https://awni.github.io/speech-recognition/
4/53
MT Surpassing Humans for News
Seg-Level English→Czech 2018
Ave. % Ave. z System
1 84.4 0.667 CUNI-Transformer
2 79.8 0.521 uedin
78.6 0.483 Professional Translation
4 68.1 0.128 online-B
5 59.4 −0.178 online-A
6 54.1 −0.354 online-G
Doc-Aware English→German 2019
Ave. Ave. z System
90.3 0.347 Facebook-FAIR
93.0 0.311 Microsoft-WMT19-sent-doc
92.6 0.296 Microsoft-WMT19-doc-level
90.3 0.240 Professional Translation
87.6 0.214 MSRA-MADL
88.7 0.213 UCAM
89.6 0.208 NEU
87.5 0.189 MLLP-UPV
87.5 0.130 eTranslation
86.8 0.119 dfki-nmt
84.2 0.094 online-B
… 10 more systems here …
76.3 −0.400 online-X
43.3 −1.769 en-de-task
See lecture #1 for all caveats of MT evaluation.
5/53
SLT Pipeline
1. Run ASR.
2. Run MT.
6/53
SLT Pipeline
1. Run ASR Recognize lowercase words.
2. Run MT Translate sentences.
7/53
SLT Pipeline
1. Run ASR Recognize lowercase words.
2. Segment into sentences.
3. Run MT Translate sentences.
8/53
SLT Pipeline
1. Run ASR Recognize lowercase words.
2. Segment into sentences.
3. Consider how to handle uncertainty!
4. Run MT Translate sentences.
9/53
SLT Pipeline
1. Acquire sound.
2. Run ASR Recognize lowercase words.
3. Segment into sentences.
4. Consider how to handle uncertainty!
5. Run MT Translate sentences.
6. Present output.
10/53
SLT Pipeline When Deployed
1. Acquire sound.
2. Ship to ASR worker.
3. Run ASR Recognize lowercase words.
4. Ship to sentence segmenter.
5. Segment into sentences.
6. Ship to translation worker.
7. Consider how to handle uncertainty!
8. Run MT Translate sentences.
9. Ship to presentation worker.
10. Present output.
11/53
SLT Pipeline When Deployed
1. Acquire sound.
2. Ship to ASR worker.
3. Run ASR Recognize lowercase words.
4. Ship to sentence segmenter.
5. Segment into sentences.
6. Ship to translation worker.
7. Consider how to handle uncertainty!
8. Run MT Translate sentences.
9. Ship to presentation worker.
10. Present output.
In realtime!In realtime!
11/53
Overall Architecture in ELITR
• Components can run distributed, connected via “bi-sockets”.Mediator
Sound Input
ASR
Client
Workers
Dummy Output
Segmenter MT Presentation
Web
• Connections always open, reused across clients.
• TCP communication ⇒ relies on network capacity.
12/53
Spoken Language Translation
Network Issues
Failures Due to Setup
Over our test sessions, we saw:
• slow network at various steps,
• partially working misconfiguration.Mediator
Sound Input
ASR
Client
Workers
Dummy Output
Segmenter MT Presentation
Web
...network issues at
one of the partners
13/53
Failures Due to Setup
Over our test sessions, we saw:
• slow network at various steps,
• partially working misconfiguration.Mediator
Sound Input
ASR
Client
Workers
Dummy Output
Segmenter MT Presentation
Web
13/53
Failures Due to Setup
Over our test sessions, we saw:
• slow network at various steps,
• partially working misconfiguration.Mediator
Sound Input
ASR
Client
Workers
Dummy Output
Segmenter MT Presentation
Web
queuing of web...
update events
13/53
Failures Due to Setup
Over our test sessions, we saw:
• slow network at various steps,
• partially working misconfiguration.Mediator
Sound Input
ASR
Client
Workers
Dummy Output
Segmenter MT Presentation
Web
...running locally but
shipping sound twice
13/53
Spoken Language Translation
Sound Acquisition
Microphone Position
14/53
Headset Mic vs. Shirt Mic
A micro-test (just 3.5 minutes in total) with two microphones:
Word Error Rate Headset Shirt Diff
EN ASR 0.32 0.39 -0.07
CS ASR 0.14 0.17 -0.03
15/53
Microphone Distance and Other Errors
https://www.sweetwater.com/insync/5-ways-your-mic-technique-is-ruining-your-vocals/
16/53
Microphone Distance and Other Errors
https://www.sweetwater.com/insync/5-ways-your-mic-technique-is-ruining-your-vocals/
16/53
Microphone Distance and Other Errors
https://www.sweetwater.com/insync/5-ways-your-mic-technique-is-ruining-your-vocals/
16/53
Microphone Distance and Other Errors
https://www.sweetwater.com/insync/5-ways-your-mic-technique-is-ruining-your-vocals/
16/53
Volume Settings along the Pipeline
A number of volume controls is on the way:
• Wireless microphone output volume.
• Sound card input volume.
• Line/Mic Level. • Padding.
• Automatic clipping of too loud signal.
⇒ You need to carefully ‘track’ the signal step by step.
17/53
Spoken Language Translation
Realistic ASR Quality
ASR Challenges
Speaker intents You have a botel? Oh, yes. We’re situated in hearth
of České Budějovice.
Reality You have a bottle? Oh, yes. VeeR situated in haRd
of České Budějovice. + BACKGROUND NOISE
Unknowledgeable You have a bottle? Oh, yes. We’re situated in hearth
person hears of Che... WHICH CITY?
Noise-sensitive ASR ∅ oh yes the the of ∅
Noise-resistent ASR you have somebody to Oh, yes, we are situated in hard
which is can we do?
Knowledgeable person You have a botel? Oh, yes, we’re situated in hearth
/ Future ASR of České Budějovice.
• Non-“standard” pronunciation, background noise, OOV, named entities
18/53
ASR on Non-Native High-School Students
20
40
60
80
100
Google UEDIN KIT
WER
Recognized by all
20
40
60
80
100
Google UEDIN KIT
WER
All recordings
System: Google UEDIN KIT
19/53
ASR of Non-Natives in Noisy Environment
• Human error level: 4–6% WER (word error rate).
• Best neural nets are reportedly there, too.
• Our test of 90-second speeches of high-school students:
• Average WER: 40–50% KIT, 80–90% Google, UEDIN.
The best recognized segment:
Manual Google UEDIN KIT
why do you wear
those high heels ,
if you would wear
some sneakers ?
I know one really
good store , that
deals with the
sale of freetime
why do ∅ where
does high heels if
you would wear
some sneakers I
no one really good
star that deals
with the sale of
freedown food to
’re ready where
tells us if you
would ∅ sneaker
us I know won the
really good store
that deals with the
sale of freedown
food
why do are those
highs heels if you
would where some
sneakers i no one
really good story
that deals with the
sale of freetime
food to our 20/53
Spoken Language Translation
Realistic MT Quality
General Translation Errors, Domain Issues
ASR But it is much more difficult to ask if you do not have any clue.
MTde Aber es ist viel schwieringer zu fragen, ob Sie keine Vorstellung
davon haben.
MTcs Je však mnohem těžší ptát se, zda nemáte ponětí.
• “if” should be translated as “wann”/“když” in this context.
ASR You can be reported after some profanities.
MTcs Můžete být hlášeni o některých profesních věcech.
Gloss You can be reported due to some professional things.
21/53
ASR Errors Multiplied in MT
• Errors in ASR are mostly similar words.
• Reasonably easy for the user to recover from transcript errors.
• MT takes these wrong words as fully trustworthy.
• MT happily reorders the sentence to sound best, including wrong words.
• No information about ASR and MT confidence available!
ASR And the goal of my thesis is to fold.
MTcs A cílem mé teorie je rozdrobit se.
Gloss And the goal of my theory is to fall apart.
Ref A cíle má moje teze dva.
Gloss And there are two goals of my thesis.
22/53
Spoken Language Translation
ASR + MT Integration
ASR + MT Integration
• ASR emits string of lowercase words.
• MT expects individual correct sentences.
Options to bridge the gap:
1. Insert punctuation into ASR output ⇒ new step: Segmentation.
2. Change ASR to predict directly correct punctuation.
3. Fully end-to-end SLT.
23/53
Approaches to Segmentation
• Language-Model-based: LM score without and with punctuation:
P(some sneakers I know) ≷ P(some sneakers, I know) ≷
≷ P(some sneakers. I know) ≷ P(some sneakers? I know)
• Sequence-labelling:
• Label each word with punctuation that should follow it.
• Many techniques possible: HMM, CRF, LSTM, …
• Machine-translation:
• Input: Text without punctuation.
• Output: Text with punctuation.
• Approaches: PBMT, NMT.
A critical decision whether to allow access to the sound:
• Delays, prosody, intonation are very informative. 24/53
Errors in Segmentation
• Errors in precision lead to confusing MT output:
Speaker …all too well…
ASR+Segm …this approach does not generalize all too. Well,
so to somehow concludes that the whole talk.
• Errors in recall make too much content unstable, see below.
25/53
Spoken Language Translation
End-to-End SLT
Motivation for End-to-End SLT
Benefits:
• Uncertainty directly handled.
• Target-language considerations influence speech recognition.
• Potentially fewer NN parameters.
Drawbacks:
• Insufficient training data.
• Speech + transcript and parallel texts much more common than
speech + translation.
• 20–40x longer input sequences (sound timeframes vs. subwords).
• Difficult alignment problem within sentences/utterances.
• Non-golden utterance segmentation not yet much considered.
26/53
SLT Training TechniquesDecoder
Speech
Encoder
Standard
Decoder
Text
Encoder
Decoder
Decoder
Speech
Encoder
Target Source
Decoder
Speech
Encoder
Pretraining
Decoder
Speech
Encoder
Knowledge
distillation
Decoder
Speech
Encoder
Decoder
Text
Encoder
ASR pretrainingMT pretraining
TEACHER STUDENT
Multi-task
Learning
27/53
Proof-of-Concept End-to-End SLT (Berard et al., 2016)
• Synthetic French speech into English text (7 concatenative voices).
• MFCCs → deep LSTM encoder → attn → deep LSTM decoder.(a) Machine translation alignment (b) Speech translation alignment
• End-to-end results not far from ASR+MT given synthetic input. 28/53
First Truly End-to-End SLT
Bérard et al. (2018) presents the first truly end-to-end SLT:
• Speech encoder:
• 2 layers converting 𝑛-dim input into 𝑛′-dim.
• 2 layers of convolution
• 3-layer bidirectional LSTM
• Attention
• Char-level decoder
• Used either to predict English transcription (|𝑉 | = 46),
• or French translation (|𝑉 | = 167)
29/53
Bérard et al. (2018) Pre-Training20000 40000 60000 80000
steps
30
40
50
60
70
WER (ASR)
14
15
16
17
18
19
20
21
BLEU (MT)
ASR mono
ASR multi
MT mono
MT multi
30/53
Bérard et al. (2018) Resultsgreedy beam ensemble params
Test BLEU (million)
Cascaded 14.6 14.6 15.8 6.3 + 15.9
End-to-End 12.3 12.9
15.5† 9.4Pre-trained 12.6 13.3
Multi-task 12.6 13.4
Table 4: AST results on Augmented LibriSpeech test. † com-
bines the end-to-end, pre-trained and multi-task models.
31/53
Recent End-to-End SLT Results (Sulubacak et al., 2019)Table 4: BLEU scores for SLT methods on English→ French Augmented LibriSpeech/test.
All systems are end-to-end, except for the pipeline system marked with a dagger ( †).
Approach BLEU ↑ Training data Description
SLT (h) ASR (h) MT (sent)
B´erard et al (2018) 13.4 100h CNN+LSTM. Multi-task.
Di Gangi et al (2019b) 13.8 236h CNN+Transformer.
Bahar et al (2019) 17.0 100h 130h 95k Pyramidal LSTM. Pretraining, augmentation.
Liu et al (2019) 17.0 100h Transformer. Knowledge distillation.
Inaguma et al (2019a) 17.3 472h CNN+LSTM. Multilingual.
Pino et al (2019) 21.7 100h 902h 29M CNN+Transformer. Pretraining, augmentation.
Pino et al (2019) † 21.8 100h 902h 29M End-to-end ASR. CNN+LSTM.
32/53
Transformer Adapted for Speech Input Gangi et al. (2019)
33/53
Translatotron (Jia et al., 2019)log-mel spectrogram
(Spanish)
8-layer
Stacked
BLSTM
Encoder
concat
Speaker
Encoder
speaker
reference
utterance
Attention
Attention
Multihead
Attention
2× LSTM Decoder
2× LSTM Decoder
Spectrogram
Decoder
Vocoder
phonemes
(Spanish)
phonemes
(English)
linear freq
spectrogram
(English)
waveform
(English)
Auxiliary recognition tasks
• Speech transcripts still needed to train (but not at inference).
• Somewhat worse that SLT+TTS.
• Allows to transfer the voice across languages.
https://google-research.github.io/lingvo-lab/translatotron/
34/53
Spoken Language Translation
Presentation
The Importance of Presentation
• Presentation issues can kill the whole show.
• Bad font size may make output impossible to follow.
• Too much flicker, jumping text, …
• Recent fully NN ASR operate on a moving window of say 8 seconds.
• The output is too unstable to follow, let alone if translated by MT.
• Presentation must be tested on stage.
• Sizing, visibility, … cannot be checked remotely.
35/53
Subtitle View
36/53
Paragraph View
37/53
ASR/Segmentation Updates (Cho et al., 2012; Cho et al., 2017)ASR
SEG1
MT1 ...Pixelen auf Ihrem Bildschirm. Zu jedem Zeitpunkt. Es ist auch eine sehr flexible Architektur...
pixels on your screen at any given moment it is also very flexible architecture of this is an entire book
ASR update which arrives laterunstable ASR outputstable ASR output
pixels on your screen. At any given moment. It is also very flexible architecture of...
unstable segmenter outputstable segmenter output
completed expected incoming
pixels on your screen. At any given moment. It is also very flexible. Architecture of this is an entire book.
unstable segmenter outputstable segmenter output
completed completed expected expected
SEG2
MT2 ...Pixelen auf Ihrem Bildschirm. Zu jedem Zeitpunkt. Sie ist auch sehr flexibel. Die architektur ist ein ganzes Buch.
should reset happen, this would be the new beginning
38/53
Cognitive Load, Overall Usability
• Users confirm that transcript and slides must be on the same
screen.
• Adding slide streaming/sharing to both Subtitle and Paragraph view.
• Overall usability:
• Often still bad, due to the cummulation of errors.
• Two foreign colleagues reported they could follow a Czech talk,
if fully focussed on the text.
• Desired settings differ from user to user:
• Those who understand source language will need
simultaneity over precision and stability.
• Those who cannot understand source need
stability and precision and are happy to wait for seconds.
39/53
Spoken Language Translation
Evaluation
Evaluating Spoken Language Translation
Three aspects o simultaneous (‘on-line’) SLT:
• Quality of the final translation.
• … equals standard MT quality estimates.
• Lag behind the source.
• Some lag is inevitable, e.g. waiting for the German verb.
• Flicker
• How many words are corrected?
40/53
Mismatch in SegmentingWisst ihr es? Es ist eine Katze, ist es das nicht? Ja, so ist es.
Isn't it? yes is it like that.
German Ref
English ASR
...
......
...
Do you know it's a cat,
• Consider English→German SLT.
• No matter what the MT does with the recognized English,
segments won’t match.
41/53
Mismatch in SegmentingWisst ihr es? Es ist eine Katze, ist es das nicht? Ja, so ist es.
Isn't it? yes is it like that.
German Ref
English ASR
...
......
...
Do you know it's a cat,
Planned strategy:
• Follow reference segmentation.
• Find best matching hypothesis segmentation.
• a) Expand by full segments.
• b) Expand by a few words around the best-matching segment.
Or ignore the problem by force-segmenting into ∼30s chunks.
41/53
Visual Information in MT
Motivation for Multi-Modal Translation (1/2)
Input A tennis player is getting ready.
Output A Tenista se připravuje.
42/53
Motivation for Multi-Modal Translation (1/2)
Input A tennis player is getting ready.
Output A Tenista se připravuje. ← male
Output B Tenistka se připravuje. ← female
42/53
Motivation for Multi-Modal Translation (1/2)
Input A tennis player is getting ready.
Output A Tenista se připravuje. ← male
Output B Tenistka se připravuje. ← female
42/53
Motivation for Multi-Modal Translation (2/2)
Hindi Visual Genome (Parida et al., 2019) provides 30k picture
descriptions from visualgenome.org, translated into Hindi.
1: Two lambs lying in the sun.
Hindi MT: दो भेड़ के बċचे सूरज मȅ झूठ बोल रहे हȈ
Gloss: Two baby sheep are telling lies …
Selected surrounding captions:
2. Sheep standing in the grass
3. Sheep with black face and legs
4. Sheep eating grass
5. Lamb sitting in grass.
43/53
Hindi Visual Genome Challenge Test Set
• A test set created by scanning the 3.15M unique strings for
ambiguous words.
• Only 19 words with multiple (automatic) translations were
identified:
Word Segment Count Word Segment Count
1 Stand 180 11 English 42
2 Court 179 12 Fair 41
3 Players 137 13 Fine 45
4 Cross 137 14 Press 35
5 Second 117 15 Forms 44
6 Block 116 16 Springs 30
7 Fast 73 17 Models 25
8 Date 56 18 Forces 9
9 Characters 70 19 Penalty 4
10 Stamp 60 Total 1400
44/53
Example from the “Challenge Test Set”
Street sign advising of penalty. The penalty box is white lined.
45/53
Attention to Source Words (?)
46/53
Attention to Source Image
47/53
Hierarchical Attention (Libovický and Helcl, 2017)Source: a man sleeping in a green room on a
couch .
Reference: ein Mann schl¨aft in einem gr¨unen
Raum auf einem Sofa .
Output with attention:ein
Mann
schläft
auf
einem
grünen
Sofa
in
einem
grünen
Raum
.
(1)
(2)
(3)
(1) source, (2) image, (3) sentinel
48/53
Recent Multi-Modal MT Results (Sulubacak et al., 2019)B L E U ↑ M E T E O R ↑ Ty p e D e scr ip tio n A r ch .
Elliott et al (2015) † 9.7 (N/A) 24.7 (N/A) E,D Conditional LMs RNN
Caglayan et al (2016a) † 29.3 (↓4.6) 48.5 (↓4.3) A Shared Attention RNN
Calixto et al (2016) † 28.8 (N/A) 49.6 (N/A) A Separate Attention RNN
Huang et al (2016) † 36.8 (↑2.0) 54.4 (↑2.3) IF Parallel RCNN-LSTMs RNN
Hitschler et al (2016) † 34.3 (N/A) 56.0 (N/A) R Retrieval + Reranking SMT
Toyama et al (2016) 36.5 (↑1.6) 56.0 (↑0.7) L Variational RNN
Shah et al (2016) † 34.8 (↑0.2) 56.7 (↑0.1) R Visual Reranking SMT
Caglayan et al (2016a) † 36.2 (– 0.0) 57.5 (↑0.1) R Visual Reranking SMT
Helcl and Libovick´y (2017) 31.9 (↓2.7) 49.4 (↓2.3) A Hierarchical Attention RNN
Calixto and Liu (2017) 36.9 (↑3.2) 54.3 (↑2.0) I Input Prepend & Append RNN
Calixto et al (2017) 36.5 (↑2.8) 55.0 (↑2.7) A Gated Attention RNN
Calixto and Liu (2017) 37.3 (↑3.6) 55.1 (↑2.8) D Decoder Init. RNN
Elliott and K´ad´ar (2017) 36.8 (↑1.3) 55.8 (↑1.8) T Imagination RNN
Caglayan et al (2017a) 38.2 (↑0.1) 57.6 (↑0.3) E,D Encoder Decoder Init. RNN
37.8 (↓0.3) 57.7 (↑0.4) O Multiplicative Interaction RNN
Delbrouck and Dupont (2017b) 40.5 (N/A) 57.9 (N/A) A Encoder Attention + CBN RNN
Arslan et al (2018) 41.0 (↑2.4) 53.5 (↓1.5) A Parallel Attention Transformer
Calixto et al (2018) 37.6 (↑2.6) 56.0 (↑1.1) L Variational RNN
Helcl