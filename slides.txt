
Metrics of MT Quality
OndÅ™ej Bojar
February 20, 2020
NPFL087 Statistical Machine Translation
Charles University
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics unless otherwise stated
Course Outline
1. Metrics of MT Quality.
2. Approaches to MT. SMT, PBMT, NMT, NP-hardness.
3. NMT (Seq2seq, Attention. Transformer). Neural Monkey.
4. Parallel texts. Sentence and word alignment. hunalign, GIZA++.
5. PBMT: Phrase Extraction, Decoding, MERT. Moses.
6. Morphology in MT. Factors or segmenting, data or linguistics.
7. Syntax in SMT (constituency, dependency, deep).
8. Syntax in NMT (soft constraints/multitask, network structure).
9. Towards Understanding: Word and Sentence Representations.
10. Advanced: Multi-Lingual MT. Multi-Task Training. Chefâ€™s Tricks.
11. Project presentations.
1/84
Outline
â€¢ Task of MT (formulating a simplified goal).
â€¢ Manual evaluation.
â€¢ Automatic evaluation.
â€¢ Empirical confidence bounds.
â€¢ End-to-end vs. component evaluation.
â€¢ Summary: Evaluation caveats.
2/84
Importance of Measuring MT Output
You need a metric to be able to check your progress.
An example from the history:
â€¢ Manual judgement at Euratom (Ispra) of a Systran system (Russianâ†’English) in
1972 revealed huge differences in judging; (Blanchon et al., 2004):
â€¢ 1/5 (Dâ€“) for output quality (evaluated by teachers of language),
â€¢ 4.5/5 (A+) for usability (evaluated by nuclear physicists).
Metrics can drive the research for the topics they evaluate.
â€¢ Some measured improvement required by sponsors: NIST MT Eval,
DARPA, TC-STAR, EuroMatrix+.
â€¢ BLEU has lead to a focus on phrase-based MT.
â€¢ Other metrics may similarly change the communityâ€™s focus.
3/84
Our Goal in MT
We restrict the task of MT to the following conditions.
â€¢ No writersâ€™ ambitions, we prefer literal translation.
â€¢ No attempt at handling cultural differences.
Expected output quality:
1. Worth reading. (Not speaking the src. lang. I can sort of understand.)
2. Worth editing. (I can edit the MT output to obtain publishable text.)
3. Worth publishing, no editing needed.
In general, weâ€™re aiming at level 1 or 2. Level 3 remains risky.
4/84
Basic Manual Evaluation Decisions
What to Show to the annotators when assessing the candidate?
â€¢ REF-based ... only the (human) reference
â€¢ SRC-based ... only the source
â€¢ SRC&REF-based ... both
Context to Consider:
â€¢ Sentence-level ... sentences in random order
â€¢ Document-level ... obtain single score per document
â€¢ Document-aware ... show whole documents, scores per sentence
What to Ask from annotators (scoring technique):
â€¢ Some relative score over several candidates?
â€¢ Some absolute score for a single output?
â€¢ A more complicated question? 5/84
Scoring Techniques
Black-box: Judging hypotheses produced by MT systems:
â€¢ Adequacy and fluency of whole sentences.
Somewhat revisited under the name Direct assessment (DA).
â€¢ Relative ranking (RR) of full sentences by several MT systems:
Longer sentences hard to rank. Candidates incomparably poor.
â€¢ Ranking of constituents, i.e. parts of sentences:
Tackles the issue of long sentences. Does not evaluate overall coherence.
â€¢ Comprehension test: Blind editing+correctness check.
â€¢ Task-based: Does MT output help as much as the original?
Do I dress appropriately given a translated weather forecast?
Gray-box: Analyzing errors in systemsâ€™ output.
â€¢ HMEANT, HUME: Is the core event structure preserved?
â€¢ MQM: Multi-dimensional quality metrics.
Glass-box: System-dependent: Does this component work? 6/84
Direct Assessment: Adequacy
Graham et al. (2013) propose a simple continuous scale:
â€¢ To what extent MT adequately expresses the meaning of REF?
âŠ• After âˆ¼15 judgements, each annotator stabilizes.
âŠ– Interpretable by averaging over many judgements of many people.
âŠ– 30â€“70(!)% of participating Turkers unreliable.
âŠ– Too few non-English speakers on Amazon Mechanical Turk. 7/84
Direct Assessment: Fluency
DA for fluency:
â€¢ To what extent the MT is fluent English?
â€¢ The source or reference are not shown at all.
â€¢ Fluency used only to break ties in adequacy.
8/84
Recent Result: MT Surpassing Humans: 2018
â€¢ WMT 2018 English-to-Czech news translation results: (Bojar et al., 2018)
Ave. % Ave. z System
1 84.4 0.667 CUNI-Transformer
2 79.8 0.521 uedin
78.6 0.483 Professional Translation
4 68.1 0.128 online-B
5 59.4 âˆ’0.178 online-A
6 54.1 âˆ’0.354 online-G
9/84
Recent Result: MT Surpassing Humans: 2018
â€¢ WMT 2018 English-to-Czech news translation results: (Bojar et al., 2018)
Ave. % Ave. z System
1 84.4 0.667 CUNI-Transformer
2 79.8 0.521 uedin
78.6 0.483 Professional Translation
4 68.1 0.128 online-B
5 59.4 âˆ’0.178 online-A
6 54.1 âˆ’0.354 online-G
Caveats:
â€¢ Humans translated whole documents, MT individual segments.
â€¢ Evaluation was done for individual segments.
9/84
SRC-Based Doc-Level DA
âŠ– Mental overload.
âŠ– Too few scores
collected â‡’ Difficult to
get statistical
significance.
10/84
SRC-Based Pseudo-Doc-Aware DA
â€¢ Score sentences using DA one by one.
â€¢ In the original order (i.e. not shuffled).
â‡’ Mentally manageable.
Problems of the first run at WMT19 (Barrault et al., 2019):
â€¢ No way to go back to previous sentences.
â€¢ All sentences in a row must come from the same MT system.
â€¢ No longer independent probes (violating statistical assumptions).
11/84
Recent Results: MT Surpassing Humans: 2019
Englishâ†’Czech
Ave. Ave. z System
1 91.2 0.642 Professional Translators
2 86.0 0.402 CUNI-DocTransformer-T2T
86.9 0.401 CUNI-Transformer-T2T-2018
85.4 0.388 CUNI-Transformer-T2T-2019
5 81.3 0.223 CUNI-DocTransformer-Marian
80.5 0.206 uedin
7 70.8 âˆ’0.156 online-Y
71.4 âˆ’0.195 TartuNLP-c
9 67.8 âˆ’0.300 online-G
10 68.0 âˆ’0.336 online-B
11 60.9 âˆ’0.594 online-A
12 59.3 âˆ’0.651 online-X
12/84
Recent Results: MT Surpassing Humans: 2019
Englishâ†’Czech
Ave. Ave. z System
1 91.2 0.642 Professional Translators
2 86.0 0.402 CUNI-DocTransformer-T2T
86.9 0.401 CUNI-Transformer-T2T-2018
85.4 0.388 CUNI-Transformer-T2T-2019
5 81.3 0.223 CUNI-DocTransformer-Marian
80.5 0.206 uedin
7 70.8 âˆ’0.156 online-Y
71.4 âˆ’0.195 TartuNLP-c
9 67.8 âˆ’0.300 online-G
10 68.0 âˆ’0.336 online-B
11 60.9 âˆ’0.594 online-A
12 59.3 âˆ’0.651 online-X
Englishâ†’German
Ave. Ave. z System
90.3 0.347 Facebook-FAIR
93.0 0.311 Microsoft-WMT19-sent-doc
92.6 0.296 Microsoft-WMT19-doc-level
90.3 0.240 Professional Translation
87.6 0.214 MSRA-MADL
88.7 0.213 UCAM
89.6 0.208 NEU
87.5 0.189 MLLP-UPV
87.5 0.130 eTranslation
86.8 0.119 dfki-nmt
84.2 0.094 online-B
â€¦ 10 more systems here â€¦
76.3 âˆ’0.400 online-X
43.3 âˆ’1.769 en-de-task
12/84
SRC-Based Doc-Aware 10-RankME
Mix of all:
â€¢ Two or more systems
considered.
â€¢ Whole document shown.
â€¢ A section of 10 consecutive
sentences scored in
(1) adequacy, (2) fluency,
(3) overall.
â‡’ Combines relative, absolute,
doc-level, sent-level.
âŠ– Very time-consuming.
13/84
Relative Ranking of Sentences
14/84
Relative Ranking (Eye-Tracked)
Project suggestion: Analyze the recorded data: path patterns / errors in words.
15/84
Relative Ranking of Constituents
16/84
Interpreting Manual RanksA
B
C
better
D
See also Bojar et al. (2011). 17/84
Interpreting Manual RanksA
B
C
better
D "block"
See also Bojar et al. (2011). 18/84
Interpreting Manual RanksA
B
C
better
D
A
B
C
E
See also Bojar et al. (2011). 19/84
Interpreting Manual RanksA
B
C
better
D
A
B
C
E
Who Wins WMT?
See also Bojar et al. (2011). 20/84
Interpreting Manual RanksA
B
C
better
D
A
B
C
E
[Systems] are ranked
based on how frequently
they were judged to be
better than or equal to
any other system.
See also Bojar et al. (2011). 21/84
Interpreting Manual RanksA
B
C
better
D
A
B
C
E
A
B
C
D
A
B
C
E
"â‰¥ All in Block"
A: 1/2
B: 0/2
C: 0/2
D: 0/1
E: 1/1
See also Bojar et al. (2011). 22/84
Interpreting Manual RanksA
B
C
better
D
A
B
C
E
A
B
C
E
"â‰¥ All in Block"
A: 1/2
B: 0/2
C: 0/2
D: 0/1
E: 1/1
See also Bojar et al. (2011). 23/84
Interpreting Manual RanksA
B
C
better
D
A
B
C
E
Simulated
Pairwise "â‰¥ All in Block"
A: 1/2
B: 0/2
C: 0/2
D: 0/1
E: 1/1
See also Bojar et al. (2011). 24/84
Interpreting Manual RanksA
B
C
better
D
A
B
C
E
Simulated
Pairwise
A
B
C
D
A>B
A>C
A>D
B=C
B>D
C>D
"â‰¥ All in Block"
A: 1/2
B: 0/2
C: 0/2
D: 0/1
E: 1/1
See also Bojar et al. (2011). 25/84
Interpreting Manual RanksA
B
C
better
D
A
B
C
E
A>B
A>C
A>D
B=C
B>D
C>D
A<B
A<C
B=C
A<E
B<E
C<E
Simulated
Pairwise
A
B
C
E
A<B
A<C
B=C
A<E
B<E
C<E
"â‰¥ All in Block"
A: 1/2
B: 0/2
C: 0/2
D: 0/1
E: 1/1
See also Bojar et al. (2011). 26/84
Interpreting Manual RanksA
B
C
better
D
A
B
C
E
A>B
A>C
A>D
B=C
B>D
C>D
A<B
A<C
B=C
A<E
B<E
C<E
Simulated
Pairwise "â‰¥ All in Block"
A: 1/2
B: 0/2
C: 0/2
D: 0/1
E: 1/1
See also Bojar et al. (2011). 27/84
Interpreting Manual RanksA
B
C
better
D
A
B
C
E
"â‰¥ Others"
A: 3/6
B: 4/6
C: 4/6
D: 0/3
E: 3/3
A>B
A>C
A>D
B=C
B>D
C>D
A<B
A<C
B=C
A<E
B<E
C<E
Simulated
Pairwise "â‰¥ All in Block"
A: 1/2
B: 0/2
C: 0/2
D: 0/1
E: 1/1
See also Bojar et al. (2011). 28/84
Interpreting Manual RanksA
B
C
better
D
A
B
C
E
"â‰¥ Others"
A: 3/6
B: 4/6
C: 4/6
D: 0/3
E: 3/3
A>B
A>C
A>D
B=C
B>D
C>D
A<B
A<C
B=C
A<E
B<E
C<E
Simulated
Pairwise "â‰¥ All in Block"
A: 1/2
B: 0/2
C: 0/2
D: 0/1
E: 1/1
See also Bojar et al. (2011). 29/84
Interpreting Manual RanksA
B
C
better
D
A
B
C
E
"â‰¥ Others"
A: 3/6
B: 4/6
C: 4/6
D: 0/3
E: 3/3
A>B
A>C
A>D
B=C
B>D
C>D
A<B
A<C
B=C
A<E
B<E
C<E
Simulated
Pairwise "â‰¥ All in Block"
A: 1/2
B: 0/2
C: 0/2
D: 0/1
E: 1/1
See also Bojar et al. (2011). 30/84
Interpreting Manual RanksA
B
C
better
D
A
B
C
E
"â‰¥ Others"
A: 3/6
B: 4/6
C: 4/6
D: 0/3
E: 3/3
A>B
A>C
A>D
B=C
B>D
C>D
A<B
A<C
B=C
A<E
B<E
C<E
Simulated
Pairwise "â‰¥ All in Block"
A: 1/2
B: 0/2
C: 0/2
D: 0/1
E: 1/1
See also Bojar et al. (2011). 31/84
Interpreting Manual RanksA
B
C
better
D
A
B
C
E
"â‰¥ Others"
A: 3/6
B: 4/6
C: 4/6
D: 0/3
E: 3/3
A>B
A>C
A>D
B=C
B>D
C>D
A<B
A<C
B=C
A<E
B<E
C<E
Simulated
Pairwise "â‰¥ All in Block"
A: 1/2
B: 0/2
C: 0/2
D: 0/1
E: 1/1
A: 3/6
B: 4/6 A: 1/2
B: 0/2
See also Bojar et al. (2011). 32/84
Comprehension 1/2 (Blind Editing)
33/84
Comprehension 2/2 (Judging)
34/84
Quiz-Based Evaluation (1/1)
An approximation of task-based evaluation.
Preparation: English texts and Czech yes/no questions:
â€¢ We found English text snippets hopefully by native speakers.
â€¢ We equipped each snippet with 3 yes/no questions in Czech.
3 different snippet lengths (1..3 sents.), 4 different topics:
â€¢ Meeting: when, where, how often, with whom, â€¦
â€¢ Directions: driving/walking instructions, finding buildings, â€¦
â€¢ Basic quizes: maths, physics, biology, â€¦ simple questions.
â€¢ Politics/News: elections chances, affairs, finance news, â€¦
Annotation: Given machine-translated snippet, answer the questions. 35/84
Quiz-Based Evaluation (2/2)
Moses 2007 Google 16.2.2010
Na provoz svÄ›tla na roundabout, obrÃ¡tit
levice a projet ballymun. OtoÄit vlevo
na kÅ™iÅ¾ovatce. ballymun / Collins Av-
enue Road Dcu je umÃ­stÄ›na na Collins
500m na pravÃ©m boku Avenue.
Na semaforech na kruhovÃ½ objezd,
odboÄit doleva a jet pÅ™es Ballymun.
OdboÄit vlevo na Collins Avenue / Bal-
lymun silniÄnÃ­ kÅ™iÅ¾ovatky. DCU se
nachÃ¡zÃ­ na Collins Avenue 500 m na
pravÃ© stranÄ›.
ZaÅ¡krtnÄ›te pravdivÃ¡ tvrzenÃ­:
1. DCU leÅ¾Ã­ na Collins Avenue.
2. V danÃ©m mÄ›stÄ› majÃ­ na kruhovÃ½ch objezdech zÅ™ejmÄ› semafory.
3. PÅ™i pÅ™Ã­jezdu budete mÃ­t DCU po levÃ© stranÄ›.
Original: At the traffic lights on the roundabout, turn left and drive through Ballymun. Turn left at
the Collins Avenue/Ballymun Road crossroads. DCU is located on Collins Avenue 500m on the right
hand side. Correct answer: yyn
36/84
Maturita (GCSE)-Like (VojtÄ›chovÃ¡ et al., 2019)Manual evaluaï¿½on by domain experts, scoring in categories:
1. Language Resources â€“ Spelling and Morphology
2. Vocabulary â€“ Adequacy of Terms Used
3. Vocabulary â€“ Clarity of the Text in Terms of Used Words
4. Syntax and Word Order
5. Coherence and Overall Understanding of the Text
0.5 1 1.5 2 2.5 3 3.5 4
Best SystemReference
ploï¿½ed as average rank for beï¿½er comparibility beï¿½er âŸ· worse
en-cs
37/84
Superhuman MT Translating Agreements?
Supplement No. 1 to the agreement on the sublease the apartment, of 13th May 2016
On the day, month and year written below Marta BureÅ¡ovÃ¡, pers. no. 695604/3017
Address: Radimova 8, Prague 6, 169 00 as the tenant on the one hand (Hereinafter
referred to as â€the tenantâ€) and KarolÃ­na ÄŒernÃ¡, pers. no. 136205/891 Address:
AlfrÃ©dova 13, Praha 4, 142 00 As a lessee on the other (Hereinafter referred to as â€the
lesseeâ€) collectively also referred to as â€the Contracting partiesâ€ have agreed on this
Supplement No. 1 to the Agreement on the sublease the apartment, of 13th May 2016
(hereinafter referred to as the â€Supplement No. 1â€)
I. Introductory Provisions
On 13th May 2016, the tenant and the lessee closed the Agreement on the sublease of
the apartment, under which the tenant let the lessee use the apartment No. 4 (area 49
mÂ²) of size 1+1/L in the ground floor of the house in Prague 4, AlfrÃ©dova 13, â€¦
(VojtÄ›chovÃ¡ et al., 2019)
38/84
Superhuman MT Translating Agreements?
Dodatek Ä. 1 ke smlouvÄ› o podnÃ¡jmu bytu ze dne 13. kvÄ›tna 2016
V den, mÄ›sÃ­c a rok nÃ­Å¾e napsanÃ© Marta BureÅ¡ovÃ¡, pers. no.
695604/3017 Adresa: Radimova 8, Praha 6, 169 00 jako nÃ¡jemce na
jednÃ© stranÄ› (dÃ¡le jen â€nÃ¡jemceâ€œ) a KarolÃ­na ÄŒernÃ¡, pers. no.
136205/891 Adresa: AlfrÃ©dova 13, Praha 4, 142 00 jako nÃ¡jemce na
stranÄ› druhÃ© (dÃ¡le jen â€nÃ¡jemceâ€œ) spoleÄnÄ› oznaÄovanÃ© takÃ© jako
â€smluvnÃ­ stranyâ€œ se dohodly na tomto dodatku Ä. 1 ke smlouvÄ› o
podnÃ¡jmu, dÃ¡le jen â€nÃ¡jemnÃ­ smlouvaâ€œ, dÃ¡le jen â€13. kvÄ›tna 2016â€œ).
I. ÃšvodnÃ­ ustanovenÃ­
Dne 13. kvÄ›tna 2016 nÃ¡jemce a nÃ¡jemce uzavÅ™eli smlouvu o dalÅ¡Ã­m
pronÃ¡jmu bytu, podle nÃ­Å¾ nÃ¡jemce pronajÃ­mÃ¡ nÃ¡jemci byt Ä. 4 (plocha
49 mÂ²) o velikosti 1+1/l v pÅ™Ã­zemÃ­ domu v Praze 4, AlfrÃ©dova 13, â€¦ 39/84
Superhuman MT Translating Agreements?
Dodatek Ä. 1 ke smlouvÄ› o podnÃ¡jmu bytu ze dne 13. kvÄ›tna 2016
V den, mÄ›sÃ­c a rok nÃ­Å¾e napsanÃ© Marta BureÅ¡ovÃ¡, pers. no.
695604/3017 Adresa: Radimova 8, Praha 6, 169 00 jako nÃ¡jemce na
jednÃ© stranÄ› (dÃ¡le jen â€nÃ¡jemceâ€œ) a KarolÃ­na ÄŒernÃ¡, pers. no.
136205/891 Adresa: AlfrÃ©dova 13, Praha 4, 142 00 jako nÃ¡jemce na
stranÄ› druhÃ© (dÃ¡le jen â€nÃ¡jemceâ€œ) spoleÄnÄ› oznaÄovanÃ© takÃ© jako
â€smluvnÃ­ stranyâ€œ se dohodly na tomto dodatku Ä. 1 ke smlouvÄ› o
podnÃ¡jmu, dÃ¡le jen â€nÃ¡jemnÃ­ smlouvaâ€œ, dÃ¡le jen â€13. kvÄ›tna 2016â€œ).
I. ÃšvodnÃ­ ustanovenÃ­
Dne 13. kvÄ›tna 2016 nÃ¡jemce a nÃ¡jemce uzavÅ™eli smlouvu o dalÅ¡Ã­m
pronÃ¡jmu bytu, podle nÃ­Å¾ nÃ¡jemce pronajÃ­mÃ¡ nÃ¡jemci byt Ä. 4
(plocha 49 mÂ²) o velikosti 1+1/l v pÅ™Ã­zemÃ­ domu v Praze 4, AlfrÃ©dova 39/84
HMEANT (Lo and Wu, 2011)
â€¢ Improved evaluation of adequacy compared to BLEU.
â€¢ Reduced human labour of HTER (Snover et al., 2006).
Essence: Is the basic event structure understandable?
(Who did what to whom, when, where and why.)
1. Identify semantic frames and roles in ref & hyp.
â€¢ Manual (5â€“15 min of training) or automatic (shallow SRL).
2. Mark match/partial/mismatch of each predicate and each
argument.
â€¢ Manual.
3. Calculate prec & rec across all frames in the sentence.
4. Report f-score.
40/84
HMEANT Illustration: Motivation
41/84
HMEANT Illustration: SRL
42/84
HMEANT Illustration: SRL
43/84
HMEANT Illustration: SRL
44/84
HMEANT Illustration: SRL
45/84
HMEANT Illustration: SRL
46/84
HMEANT Illustration: SRL
47/84
HMEANT Illustration: SRL
48/84
HMEANT Illustration: Alignment
49/84
HMEANT Illustration: Alignment
50/84
HMEANT Illustration
51/84
HMEANT Illustration
52/84
HUME
HUME (Birch et al., 2016) improves over HMEANT by:
â€¢ using semantic trees (UCCA, Abend and Rappoport (2013)),
â€¢ using source rather than reference,
â€¢ using trees on the source only, not malformed hypothesis.
Two manual stages again:
1. Create UCCA tree for the source (can reuse for more systems!).
2. Label UCCA tree indicating how much was preserved by MT.
53/84
HUME Annotation
â€¢ Leafs get R/O/G (traffic lights): bad, mixed, good.
â€¢ Structure gets A/B: adequate, bad.
54/84
HMEANT/HUME are Close to FGD
Project suggestion:
Use t-layer tools to:
â€¢ Improve UCCA parser, or
â€¢ Automate: parse to UCCA
or t-trees, predict R/O/G,
A/B.
55/84
Evaluation by Flagging Errors
Classification of MT errors, following Vilar et al. (2006).punct::Bad Punctuation
unk::Unknown Word missC::Content Word missA::Auxiliary Word
ows::Short Range
ops::Short Range
owl::Long Range
opl::Long Range
lex::Wrong Lexical Choice
disam::Bad Disambiguation
form::Bad Word Form
extra::Extra Word
Error Missing Word
Word Order
Incorrect Words
Word Level
Phrase Level
Bad Word Sense
56/84
Standard MQM (Core)
(Lommel et al., 2014)
57/84
Standard MQM (Overkill)
58/84
MQM Decision Tree (Simplified)MQM annotators guidelines (version 1.4, 2014-11-17) Page 2
Does an unneeded
function word
appear?
Is a needed function
word missing?
Are â€œfunction
wordsâ€ (preposi-
tions, articles,
â€œhelperâ€ verbs, etc.)
incorrect?
Is an incorrect function
word used?
Is the text garbled or
otherwise impossible to
understand?
No
Fluency
(general)*
Grammar
(general)
Function words
(general)
Yes
Extraneous Missing Incorrect
Unintelligible
No
No
No
Is the text grammatically
incorrect?
No
No
Yes No No No
Accuracy
(general)*
No No No No Are words or phrases
translated inappropri-
ately?
Mistranslation
Yes
Are terms translated
incorrectly for the do-
main or contrary to any
terminology resources?
Terminology
Yes
Is there text in the
source language that
should have been
translated?
Untranslated
Yes
Is source content
inappropriately omitted
from the target?
Omission
Yes
Yes YesYes
Yes
Has unneeded content
been added to the
target text?
Addition
Is typography, other than
misspelling or capitaliza-
tion, used incorrectly?
Are one or more words
misspelled/capitalized
incorrectly?
Typography
Spelling
No
No
Yes
Yes
YesDo words appear in
the wrong order? Word order
Yes
Yes Accuracy
Fluency
Grammar
Is the wrong form
of a word used?
Is the part of speech
incorrect?
Do two or more words
not agree for person,
number, or gender?
Is a wrong verb form or
tense used?
Word form
(general)
Part of speech
Yes No No No
Yes
Tense/mood/aspect
Yes
Agreement
Yes Word form
Function words
Note: For any question, if the answer is unclear, select â€œNoâ€
Is the issue related to the fact
that the text is a translation
(e.g., the target text does not
mean what the source text
does)?
No
MQM Annotation Decision Tree
59/84
MQM Decision Tree (Full)Start here â†’
Verity
Internation-
alization
DesignFluency
Subtypes of Inter-
nationalization are
currently undened.
A1
Has content present in the source
been inappropriately omitted from
the target?
Yes Go to A2
No Go to A3
A2
Is a variable omitted from the target
content?
Yes Omitted variable
No Omission
A3
Has content not present in the
source been inappropriately added
to the source?
Yes Addition
No Go to A4
A4
Has content been left in the source
language that should have been
translated?
Yes Go to A5
No Go to A6
A5
Is the untranslated content in a
graphic?
Yes Untranslated graphic
No Untranslated
A6
Are words or phrases translated
incorrectly?
Yes Go to A7
No Accuracy (general)
A7
Is a domain- or organization-
speci$c word or phrase translated
incorrectly?
Yes Go to A8
No Go to A10
A8
Is the word or phrase translated
contrary to company-speci$c
terminology guidelines?
Yes Company terminology
No Go to A9
A9
Is the word or phrase translated
contrary to guidelines established in
a normative document (e.g., law or
standard)?
Yes Normative terminology
No Terminology
A10
Is the translation overly literal?
Yes Overly literal
No Go to A11
A11
Is the translated content a â€œfalse
friendâ€ (faux ami)?
Yes False friend
No Go to A12
A12
Is a named entity (such as the name
of a person, place, or organization)
translated incorrectly?
Yes Entity
No Go to A13
A13
Was content translated that should
not have been translated?
Yes Should not have been translated
No A14
A14
Was a date or time translated
incorrectly?
Yes Date/time
No A15
A15
Were units (e.g., for measurement or
currency) translated incorrectly?
Yes Unit conversion
No A16
A16
Were numbers translated
incorrectly?
Yes Number
No A17
A17
Is the translation in improper exact
match from translation memory?
F1
Is the content written at a level
of formality inappropriate for the
subject matter, audience, or text
type?
Yes Go to F2
No Go to F3
F2
Does the content use slang or other
unsuitable word variants?
Yes Variants/slang
No Register
F3
Is the content stylistically
inappropriate?
Yes Stylistics
No Go to F4
F4
Is the content inconsistent with
itself?
Yes Go to F5
No Go to F10
F5
Are abbreviations used
inconsistently?
Yes Abbreviations
No Go to F6
F6
Is text inconsistent with graphics?
Yes Image vs. text
No Go to F7
F7
Is the discourse structure of the
content inconsistent?
Yes Discourse
No Go to F8
F8
Is terminology inconsistent within
the content (without being a mis-
translation)?
Yes Terminological inconsistency
No Go to F9
F9
Are cross-references or links
inconsistent in what they point to?
Yes Inconsistent link/cross-reference
No Inconsistency
F10
Does the content use unidiomatic
expressions?
Yes Unidiomatic
No Go to F11
F11
Is content inappropriately
duplicated?
Yes Duplication
No Go to F12
F12
Is the wrong term used? (Generally
assessed for source text only)
Yes Go to F13
No Go to F14
F13
Is the term used contrary to
guidelines established in a
normative document (e.g., law or
standard)?
Yes Monolingual normative terminology
No Monolingual terminology
F14
Is the content ambiguous?
Yes Go to F15
No Go to F16
F15
Is a pronoun or other linguistically
referential structure unclear as to its
reference/antecedent?
Yes Unclear reference
No Ambiguity
F16
Is content spelled incorrectly
(including incorrect capitalization)?
Yes Go to F17
No Go to F19
F17
Is content capitalized incorrectly?
Yes Capitalization
No Go to F18
F18
Are diacritics (e.g., Â¨, Â´, Ë, Ëœ) missing or
incorrect?
Yes Diacritics
No Spelling
F19
Does the content violate a formal
style guide (e.g., Chicago Manual of
Style or organization style guide)?
Yes Go to F20
No Go to F22
F20
Is the violation speci$c to a
company/organizationâ€™s internal/
house style guide?
Yes Company style
No Go to F21
F21
Is the violation of a third-party
style guide (e.g. Chicago Manual
of Style, American Psychological
Association)?
Yes 3rd-party style
No Style guide
F22
Does the content display problems
with typography (spacing or
punctuation)
Yes Company style
No Go to F26
F23
Are quote marks or brackets
unpaired (i.e., one of a paired set of
punctuation is missing)?
Yes Unpaired quote marks or brackets
No Go to F24
F24
Is punctuation used incorrectly?
Yes Punctuation
No Go to F25
F25
Is whitespace used incorrectly (i.e.,
missing, extra, inconsistent)?
Yes Whitespace
No Typography
F26
Is the content grammatically
incorrect?
Yes Go to F27
No Go to F33
F27
Is an incorrect form of a word used?
Yes Go to F28
No Go to F31
F28
Is the wrong part of speech used?
Yes Part of speech
No Go to F29
F29
Does the content show problems
with agreement (number, gender,
case, etc.)?
Yes Agreement
No Go to F30
F30
Does the content use an incorrect
verbal tense, mood, or aspect?
Yes Tense/mood/aspect
No Word form
F31
Are words in the wrong order?
Yes Word order
No Go to F32
F32
Are functions words (such as articles,
â€œhelper verbsâ€, or prepositions) used
incorrectly?
Yes Function words
No Grammar
F33
Does the content violate locale-
specic conventions (i.e., it is $ne for
the language, but not for the target
locale)?
Yes Go to F34
No Go to F40
F34
Are dates shown in the wrong
format for the target locale (e.g.,
D-M-Y when Y-M-D is expected)?
Yes Date format
No Go to F35
F35
Are times in the wrong format for
the target locale (e.g., AM/PM when
24-hour time is expected)?
Yes Time format
No Go to F36
F36
Are measurements in the wrong
format for the target locale (e.g.,
metric units used when Imperial are
expected)?
Yes Measurement format
No Go to F37
F37
Are numbers formatted incorrectly
for the target locale (e.g., comma
used as thousands separator when a
dot is expected)?
Yes Number format
No Go to F38
F38
Does the content use the wrong
type of quote mark for the target
locale (e.g., single quotes when
double quotes are expected)?
Yes Quote mark type
No Go to F39
F39
Does the content violate any
relevant national language
standards (e.g., using disallowed
words from another locale)?
Yes National language standard
No Locale convention
F40
Does the content use an incorrect
character encoding?
Yes Character encoding
No Go to F41
F41
Does the content use characters
that are not allowed according to
speci$cations?
Yes Nonallowed characters
No Go to F42
F42
Does the content violate a formal
pattern (e.g., regular expression)
that de$nes what the content may
contain?
Yes Pattern problem
No Go to F43
F43
Is content sorted incorrectly for the
target locale and sorting type?
Yes Sorting
No Go to F44
F44
Is the content inconsistent with a
corpus of known-good content?
(Note: Almost always determined by
a computer program.)
Yes Corpus conformance
No Go to F45
F45
Are links or cross-references broken
or inaccurate?
Yes Go to F46
No Go to F47
F46
Are internal links or cross-references
broken or inaccurate?
Yes Document-internal
No Document-external
F47
Are there problems with an index or
Table of Content (ToC)?
Yes Go to F48
No Go to F51
F48
Are page references in an index or
Table of Content (ToC) incorrect?
Yes Page references
No Go to F49
F49
Is the format of an index or Table of
Content (ToC) incorrect?
F50
Are items missing from an index or
Table of Content (ToC)?
F51
Is content unintelligible (i.e., the
Duency is bad enough that the
nature of the problem cannot be
determined)?
V1
Is the content unsuitable for the
end-user (target audience)?
Yes End-user suitability
No Go to V2
V2
Is the content incomplete or missing
needed information?
Yes Go to V3
No Go to V5
V3
Are lists within the content
incomplete or missing needed
information?
Yes Lists
No Go to V4
V4
Are procedures described within
the content incomplete or missing
needed information?
Yes Procedures
No Completeness
V5
Does the content violate any legal
requirements for the target locale or
intended audience?
Yes Legal requirements
No Go to V6
V6
Does the content inappropriately
include information that does apply
not to the target locale or that is
otherwise inaccurate for it?
Yes Locale-specific content
No Verity
D1
Does the formatting issue apply
globally to the entire document?
Yes Go to D2
No Go to D8
D2
Are colors used incorrectly?
Yes Color
No Go to D3
D3
Is the overall font choice incorrect
Yes Global font choice
No Go to D4
D4
Are footnotes/endnotes formatted
incorrectly?
Yes Footnote/endnote format
No Go to D5
D5
Are margins for the document
incorrect?
Yes Margins
No Go to D6
D6
Are widows/orphans present in the
content?
Yes Widows/orphans
No Go to D7
D7
Are there improper page breaks?
Yes Page break
No Overall design (layout)
D8
Is local formatting (within content)
incorrect?
Yes Go to D9
No Go to D17
D9
Is text aligned incorrectly?
Yes Text alignment
No Go to D10
D10
Are paragraphs indented improperly
or not indented when they should
be?
Yes Paragraph indentation
No Go to D11
D11
Are fonts used incorrectly within
content (rather than globally)?
Yes Go to D12
No Go to D15
D12
Are bold or italic used incorrectly?
Yes Bold/italic
No Go to D13
D13
Is a wrong font size used?
Yes Wrong size
No Go to D14
D14
Are single-width fonts used when
double-width fonts should be used
(or vice versa)?
(Applies to CJK text only.)
Yes Single/double-width
No Font
D15
Is text kerning (space between
letters) incorrect (text too tight/too
loose)?
Yes Kerning
No Go to D16
D16
Is the leading (line spacing of text)
incorrect (e.g., double spacing when
single spacing is expected)?
Yes Leading
No Local formatting
D17
Is translated text missing from the
layout (i.e., it has been translated
but is not visible in the formatted
version)?
Yes Missing text
No Go to D18
D18
Is markup (e.g., formatting codes)
used incorrectly or in a technically
invalid fashion?
Yes Go to D19
No Go to D24
D19
Is markup used inconsistently (e.g.,
<i> is used in some places and
<em> in others)?
Yes Inconsistent markup
No Go to D20
D20
Does markup appear in the wrong
place within content?
Yes Misplaced markup
No Go to D21
D21
Has markup been inappropriately
added to the content?
Yes Added markup
No Go to D22
D22
Is needed markup missing from the
content?
Yes Missing markup
No Go to D23
D23
Does markup appear to be
incorrect? (Note: Generally detected
by computer processes)
Yes Missing markup
No Markup
D24
Are there problems with graphic
and/or tables?
Yes Go to D25
No Go to D28
D25
Are graphics or tables positioned
incorrectly on the page or with
respect to surrounding text?
Yes Position
No Go to D26
D26
Are graphics or tables missing from
the text?
Yes Missing graphic/table
No Go to D27
D27
Are there problems with call-outs or
captions for graphics or tables?
Yes call-outs and captions
No Graphics and tables
D28
Are portions of text invisible due to
text expansion?
Yes Truncation/text expansion
No Go to D29
D29
Is text longer than is allowed (but
remains visible)?
Yes Truncation/text expansion
No Length
Multidimensional Quality Metrics (MQM): Full Decision Tree
e Multidimensional Quality Metrics (MQM) Framework provides a hierarchical categorization of error types that occur in translated or localized products. Based on a detailed analysis of existing translation quality metrics, it provides a #exible typology of issue types
that can be applied to analytic or holistic translation quality evaluation tasks. Although the full MQM issue tree (which, as of November 2014, contains 115 issue types categorized into ,ve major branches) is not intended to be used in its entirety for any particular evalu-
ation task, this overview chart presents a â€œdecision treeâ€ suitable for selecting an issue type from it. In practical terms, however, an individual metric would have a smaller decision tree that covers just the issues contained in that metric.
To use the decision tree start with the ,rst question and follow the appropriate answers until a speci,c issue type is reached.
General 1
Is the issue related to a diHerence in
meaning between the source and
target?
Yes Go to Accuracy
No Go to General 2
General 2
Is the issue related to the linguistic
or mechanical formulation of the
content
Yes Go to Fluency
No Go to General 3
General 3
Is the issue related to the
appropriateness of the content for the
target audience or locale (separate from
whether it is translated correcty)?
Yes Go to Verity
No Go to General 4
General 4
Is the issue related to the
presentational/display aspects of
the content?
Yes Go to Design
No Go to General 5
General 5
Is the issue related to whether or
not the content was set up properly
to support subsequent translation/
adaptation?
Yes Go to Internationalization
No Go to General 6
General 6
Is the issue addressed in the
Compatability branch
Yes Go to Compatability
No Other
Accuracy
1+ 2          
            '
        ,   
   *0*- 2    Other, it may
           
  ,         
3  -
60/84
Error Flagging Example
Annotation rules:
â€¢ Mark/suggest as little as necessary.
â€¢ Compare to source, not to reference. Literal translation ok.
â€¢ Preserve white space. Donâ€™t add or remove word/line breaks.
â€¢ Only insert error labels followed by ::.
â€¢ For missing words, use _ instead of space, if necessary.
Src Perhaps there are better times ahead.
Ref MoÅ¾nÃ¡ se tedy blÃ½skÃ¡ na lepÅ¡Ã­ Äasy.
MoÅ¾nÃ¡, Å¾e extra::tam jsou lepÅ¡Ã­ disam::krÃ¡t lex::dopÅ™edu.
MoÅ¾nÃ¡ extra::tam jsou pÅ™Ã­hodnÄ›jÅ¡Ã­ Äasy vpÅ™edu.
missC::v_budoucnu MoÅ¾nÃ¡ form::je lepÅ¡Ã­ Äasy.
MoÅ¾nÃ¡ jsou lepÅ¡Ã­ Äasy lex::vpÅ™ed.
61/84
Results on WMT09 Dataset
google cu-bojar pctrans cu-tectomt Total
Automatic: BLEU 13.59 14.24 9.42 7.29 â€“
Manual: Rank 0.66 0.61 0.67 0.48 â€“
disam 406 379 569 659 2013
lex 211 208 231 340 990
Total bad word sense 617 587 800 999 3003
missA 84 111 96 138 429
missC 72 199 42 108 421
Total missed words 156 310 138 246 850
form 783 735 762 713 2993
extra 381 313 353 394 1441
unk 51 53 56 97 257
Total serious errors 1988 1998 2109 2449 8544
ows 117 100 157 155 529
punct 115 117 150 192 574
â€¦ â€¦ â€¦ â€¦ â€¦ â€¦
tokenization 7 12 10 6 35
Total errors 2319 2354 2536 2895 10104 62/84
Contradictions in Manual Evaluation
Results for WMT10:
Evaluation Method Google CU-Bojar PC Translator TectoMT
â‰¥ others (WMT10 official) 70.4 65.6 62.1 60.1
> others 49.1 45.0 49.4 44.1
Edits deemed acceptable [%] 55 40 43 34
Quiz-based evaluation [%] 80.3 75.9 80.0 81.5
Automatic: BLEU 0.16 0.15 0.10 0.12
Automatic: NIST 5.46 5.30 4.44 5.10
Results for WMT19:
â€¢ Best systems match humans in GCSE-like scoring.
â€¢ They score worse in pseudo-doc-aware DA.
â€¢ They are absolutely terrible on agreements.
â€¦ each technique provides a different picture. 63/84
Problems of Manual Evaluation
â€¢ Expensive in terms of time/money.
â€¢ Subjective (some judges are more careful/better at guessing).
â€¢ Not quite consistent judgments from different people.
â€¢ Not quite consistent judgments from a single person!
â€¢ Not reproducible (too easy to solve a task for the second time).
â€¢ Experiment design is critical!
â€¢ Black-box evaluation important for users/sponsors.
â€¢ Gray/Glass-box evaluation important for the developers.
â€¢ SRC-based allows to compare with humans.
â€¢ Sentence-level no longer relevant for large language pairs.
64/84
Automatic Evaluation
â€¢ Comparing MT output to reference translation.
â€¢ (Reference-less evaluation is called Quality Estimation.)
â€¢ Fast and cheap.
â€¢ Deterministic, replicable.
â€¢ Allows automatic model optimization (â€œtuningâ€, MERT).
â€¢ Usually good for checking progress.
â€¢ Usually bad for comparing systems of different types.
65/84
BLEU (Papineni et al., 2002)
â€¢ Based on geometric mean of ğ‘›-gram precision.
â‰ˆ ratio of 1- to 4-grams of hypothesis confirmed by a ref. translation
Src The legislators hope that it will be approved in the next few days . Confirmed
Ref ZÃ¡konodÃ¡rci doufajÃ­ , Å¾e bude schvÃ¡len v pÅ™Ã­Å¡tÃ­ch nÄ›kolika dnech . 1 2 3 4
Moses ZÃ¡konodÃ¡rci doufajÃ­ , Å¾e bude schvÃ¡len v nejbliÅ¾Å¡Ã­ch dnech . 9 7 5 4
TectoMT ZÃ¡konodÃ¡rci doufajÃ­ , Å¾e bude schvÃ¡leno dalÅ¡Ã­ pÃ¡ru volna . 6 4 3 2
Google ZÃ¡konodÃ¡rci nadÄ›ji , Å¾e bude schvÃ¡len v nÄ›kolika pÅ™Ã­Å¡tÃ­ch dnÅ¯ . 9 4 3 2
PC Tr. ZÃ¡konodÃ¡rci doufajÃ­ Å¾e to bude schvÃ¡lenÃ½ v nejbliÅ¾Å¡Ã­ch dnech . 7 2 0 0
n-grams confirmed: none, unigram, bigram, trigram, fourgram
E.g. Moses produced 10 unigrams (9 confirmed), 9 bigrams (7 confirmed), â€¦
BLEU = BP â‹… exp(1
4 log( 9
10) + 1
4 log(7
9 ) + 1
4 log(5
8) + 1
4 log( 4
7 ))
BP is â€œbrevity penaltyâ€; 1
4 are uniform weights, the â€œdenominatorâ€ equivalent for 4
âˆšâ‹… in
geometric mean in the log domain. 66/84
BLEU: Avoid Cheating/Gaming the Metric
â€¢ Confirmed counts â€œclippedâ€ to avoid overgeneration.
â€¢ â€œBrevity penaltyâ€ applied to avoid too short output:
BP = { 1 if ğ‘ > ğ‘Ÿ
ğ‘’1âˆ’ğ‘Ÿ/ğ‘ if ğ‘ â‰¤ ğ‘Ÿ
Ref 1: The cat is on the mat .
Ref 2: There is a cat on the mat .
Candidate: The the the the the the the .
â‡’ Clipping: only 3
8 unigrams confirmed.
Candidate: The the .
â‡’ 3
3 unigrams confirmed but the output is too short.
â‡’ BP = ğ‘’1âˆ’7/3 = 0.26 strikes.
The candidate length ğ‘ and â€œeffectiveâ€ ref. length ğ‘Ÿ calculated over the whole test set. 67/84
BLEU Properties
â€¢ Within the range 0-1, often written as 0 to 100%.
â€¢ Human translation against other humans: ~60%
â€¢ Google Chineseâ†’English: ~30%, Arabicâ†’English: ~50%.
â€¢ BLEU for individual sentences not reliable.
â€¢ More so with only 1 reference translation:
Src â€ We â€™ ve made great progress .
Ref â€ UÄinili jsme velkÃ½ pokrok .
Moses â€ my jsme udÄ›lali velkÃ½ pokrok .
TectoMT â€ UdÄ›lali jsme velkÃ½ pokrok .
Google â€ My jsme dosÃ¡hli obrovskÃ©ho pokroku .
PC Translator â€ udÄ›lali jsme velkÃ½ pokrok .
68/84
Test Set Influence on BLEU
HavlÃ­Äek (2007) evaluates the influence of:
â€¢ number of reference translations,
â€¢ translation direction.
on human-produced text (1 human translation against 4 others).
csâ†’en, Professionals enâ†’cs, Math Students
Refs Indiv. Results Avg Indiv. Results Avg
1 41.15 32.66 34.03 35.95 3.66 8.62 5.79 6.02
2 49.09 49.78 41.26 46.71 9.82 8.26 9.36 9.15
3 52.63 52.63 13.06 13.06
â‡’ heavy dependence on the number of references.
More references allow to match more n-grams of MT output.
â‡’ heavy dependence on the translation direction and quality. 69/84
Correlation with Human Judgments
BLEU scores vs. human rank, the higher, the better:
6 7 8 9 10 11 12 13 14 15 16 17
âˆ’3.5
âˆ’3.3
âˆ’3.1
âˆ’2.9
âˆ’2.7
âˆ’2.5bbbbbcbcbcbc
WMT08 Results In-domain â€¢ Out-of-domain âˆ˜
BLEU Rank BLEU Rank
Factored Moses 15.91 -2.62 11.93 -2.89
PC Translator 8.48 -2.78 8.41 !! -2.60
TectoMT 9.28 -3.29 6.94 -3.26
Vanilla Moses 12.96 -3.33 9.64 -3.26
â‡’ PC Translator nearly won Rank but nearly lost in BLEU. 70/84
Dirty Tricks
â€¢ PCEDT 1.0 (ÄŒmejrek et al., 2004) contains test set with:
â€¢ 1 English original,
â€¢ 1 Czech translation,
â€¢ 4 English back-translations (via Czech).
â€¢ ÄŒmejrek et al. (2003) evaluate csâ†’en MT using all 5 English
sentences: they include the original source among the references
and report 5-fold average of BLEU (on 4 refs).
â€¢ The additional accepted variance in output increases BLEU
compared to BLEU on the 4 back-translations only.
5-fold Avg of 4-BLEU 4 refs only
PBT, no additional LM 34.8Â±1.3 32.5
PBT, bigger LM 36.4Â±1.3 34.2
PBT, more parallel texts, bigger LM 38.1Â±0.8 36.8 71/84
Improving BLEU in csâ†’en MT
A summary of older experiments. (Bojar et al., 2006; Bojar, 2006)
Deterministic pre- and post-processing
similar tokenization of reference +10.0 !!!
lemmatization for alignment +2.0
handling numbers +0.9
fixing clear BLEU errors +0.5 !
dependency-based corpus expansion +0.3
More parallel or target-side monolingual data
out-of-domain parallel texts, bigger in-domain LM +5.0
bigged in-domain LM +1.7
out-of-domain parallel texts, also in LM +0.4
adding a raw dictionary +0.2
â€¢ Complicated methods bring a little.
â€¢ Data bring more.
â€¢ Huge jumps from superficial properties but just higher BLEU, same MT quality.
72/84
Finding Clear BLEU Losses
Missing bigram = all references contained it but not the hypothesis.
Superfluous bigram = the hypothesis contained it but none of the references.
Top missing bigrams:
19 , " 12 â€ said
12 of the 10 Free Europe
10 Radio Free 7 . "
6 L.J. Hooker 6 United States
6 in the 6 the United
6 the strike â€¦
Top superfluous bigrams:
26 , '' 18 '' .
14 â€ said 12 , which
11 SvobodnÃ¡ Evropa 8 , when
8 the state 7 , who
7 J. Hooker 7 L. J.
7 company GM â€¦
Four simple rules to improve BLEU by +0.2 to +0.5 on a particular test set:
'' . â†’ . " L. J. Hooker â†’ L.J. Hooker
'' â†’ " the U.S. â†’ the United States
73/84
Technical Problems of BLEU
BLEU scores are not comparable:
â€¢ across languages.
â€¢ on different test sets.
â€¢ with different number of reference translations.
â€¢ with different implementations of the evaluation tool.
â€¢ There are different definitions of â€œreference lengthâ€:
Papineni et al. (2002) not specific. One can choose the shortest, longest,
average, closest (the smaller or the larger!).
â€¢ Very sensitive to tokenization:
Beware esp. of malformed tokenization of Czech by foreign tools.
â‡’ Use a fixed implementation, e.g. sacreBLEU (Post, 2018). 74/84
Fundamenal Problems of BLEU
â€¢ BLEU overly sensitive to word forms and sequences of tokens.
Confirmed Contains
by Ref Error Flags 1-grams 2-grams 3-grams 4-grams
Yes Yes 6.34% 1.58% 0.55% 0.29%
Yes No 36.93% 13.68% 5.87% 2.69%
No Yes 22.33% 41.83% 54.64% 63.88%
No No 34.40% 42.91% 38.94% 33.14%
Total ğ‘›-grams 35 531 33 891 32 251 30 611
30â€“40% of tokens not confirmed by reference but without errors.
â‡’ Enough space for MT systems to differ unnoticed.
â‡’ Low BLEU scores correlate even less. 75/84
Fixing Fundamenal Issues of BLEU
Evaluate coarser units:
â€¢ Lemmas or deep-lemmas instead of word forms:
â€¢ e.g. SemPOS (Kos and Bojar, 2009): bags of t-lemmas.
â€¢ Sequences of characters:
â€¢ e.g. chrF3 (PopoviÄ‡, 2015): F-score of character 6-grams.
â€¢ Use shorter of gappy sequences:
â€¢ e.g. BEER (Stanojevic and Simaâ€™an, 2014) uses characters and also pairs
of (not necessarily adjacent) words.
Use better references:
â€¢ Using more references alone helps.
â€¢ Post-edited references serve better.
â€¢ e.g. HTER (Snover et al., 2006): Measuring edit distance to manually
corrected output.
76/84
Post-Edited References Serve Better0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
10 100 1000
Correlation of
BLEU and manual ranks
Test set size
Refs: official 1
Refs: postedited 1
Refs: postedited 6
Refs: postedited 7
Refs: postedited 8
â€¢ Refs created by post-editing serve better than independent ones.
â€¢ 100 sents with 6â€“7 postedited refs as good as 3k indep refs. 77/84
Post-Edited Refs Better0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
10 100 1000
Correlation of
BLEU and manual ranks
Test set size
Refs: official 1
Refs: postedited 1
Refs: postedited 6
Refs: postedited 7
Refs: postedited 8
â€¢ â€¦ but error bars quite wide
â‡’ specific sentences important. 78/84
Fundamenal Problem of Correlation468101214161820
sacreBLEU
Correlation with DA
0
-1
1
â€¢ Correlation depends on the underlying set of MT systems.
â€¢ Often poor correlation when only top-scoring systems are
considered, see Ma et al. (2019). 79/84
Fundamenal Problem of Correlation0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
-2 -1.5 -1 -0.5 0 0.5
SacreBLEU-BLEU
DA
Top 4
Top 6
Top 8
Top 10
Top 12
Top 15
All systems
80/84
Empirical Confidence Intervals
In statistics, confidence intervals indicate how well was a parameter (e.g. the mean) of
a random variable with known/assumed distribution estimated from a set of repeated
measurements.
â€¢ We donâ€™t want to assume any distribution!
â€¢ How to â€œrepeatâ€ experiments with a deterministic MT system?
Use â€œbootstrappingâ€ (Koehn, 2004):
1. Obtain 1000 different test sets:
Randomly select sents., repeat some, ignore some, preserving test set size.
2. Sort by the score.
3. Drop top and bottom 2.5% (i.e. 25 out of 1000) results.
â‡’ The lowest and highest remaining scores are 95% empirical
confidence interval around the score obtained on the full test set. 81/84
End-to-end vs. Component Eval.
â€¢ Similar to black vs. glass box evaluation and translation vs.
task-based evaluation.
â€¢ Evaluation of a single component may not correlate with overall
performance of the system.
Pre-processing Symmetrization Alignment Error Rate BLEU
Lemmas + singletons Intersection 14.6 30.8
Lemmas Intersection 15.0 29.8
Lemmas Union 17.2 32.0
Lemmas + singletons Union 17.4 31.9
Baseline (word forms) Union 25.5 29.8
Baseline (word forms) Intersection 27.4 28.2
Data by Bojar et al. (2006). See also e.g. Lopez and Resnik (2006). 82/84
Summary, The Moral of the Story
Metrics drive research:
â€¢ Measure the property that â€œsaves moneyâ€ in your application.
â€¢ Design automatic metrics to correlate with humans.
Comparisons of automatic scores trustworthy
only under all the following:
â€¢ a single test set was used (of your domain of interest),
â€¢ evaluated by a single evaluation tool (hopefully without bugs),
E.g. for BLEU different tools tokenize and define ref. length differently.
â€¢ the metric reflects your final objective (AER vs. BLEU),
â€¢ confidence intervals are estimated.
83/84
References
Omri Abend and Ari Rappoport. 2013. Universal Conceptual Cognitive Annotation (UCCA). In Proceedings of the 51st
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 228â€“238, Sofia,
Bulgaria, August. Association for Computational Linguistics.
LoÃ¯c Barrault, OndÅ™ej Bojar, Marta R. Costa-jussÃ , Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow,
Matthias Huck, Philipp Koehn, Shervin Malmasi, Christof Monz, Mathias MÃ¼ller, Santanu Pal, Matt Post, and Marcos
Zampieri. 2019. Findings of the 2019 conference on machine translation (wmt19). In Proceedings of the Fourth
Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 1â€“61, Florence, Italy, August.
Association for Computational Linguistics.
Alexandra Birch, Omri Abend, OndÅ™ej Bojar, and Barry Haddow. 2016. HUME: Human UCCA-Based Evaluation of
Machine Translation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,
pages 1264â€“1274, Austin, Texas, November. Association for Computational Linguistics. peer-reviewed.
HervÃ© Blanchon, Christian Boitet, and Laurent Besacier. 2004. Spoken Dialogue Translation Systems Evaluation:
Results, New Trends, Problems and Proposals. In Proceedings of International Conference on Spoken Language
Processing ICSLP 2004, Jeju Island, Korea, October.
OndÅ™ej Bojar, Evgeny Matusov, and Hermann Ney. 2006. Czech-English Phrase-Based Machine Translation. In FinTAL
2006, volume LNAI 4139, pages 214â€“224, Turku, Finland, August. Springer.
OndÅ™ej Bojar, MiloÅ¡ ErcegovÄeviÄ‡, Martin Popel, and Omar Zaidan. 2011. A Grain of Salt for the WMT Manual
Evaluation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 1â€“11, Edinburgh, Scotland,
July. Association for Computational Linguistics.
OndÅ™ej Bojar, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, and
Christof Monz. 2018. Findings of the 2018 Conference on Machine Translation (WMT18). In Proceedings of the Third
Conference on Machine Translation, Volume 2: Shared Task Papers, Brussels, Belgium, October. Association for 84/84


Approaches to MT:
SMT, PBMT, NMT
OndÅ™ej Bojar
March 5, 2020
NPFL087 Statistical Machine Translation
Charles University
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics unless otherwise stated
Outline
â€¢ Approaches to MT.
â€¢ What makes MT statistical.
â€¢ Probability of a sentence, Bayesâ€™ law.
â€¢ Log-linear model.
â€¢ Phrase-Based MT.
â€¢ Features used.
â€¢ Training Pipeline.
â€¢ Unjustified independence assumptions.
â€¢ Neural MT.
â€¢ Deep learning summary.
â€¢ Representing text
â€¢ Encoder-decoder architecture.
1/51
Approaches to Machine Translation
â€¢ The deeper analysis, the easier the transfer should be.
â€¢ A hypothetical interlingua captures pure meaning.
â€¢ Statistical systems learn â€œautomaticallyâ€ from data.
â€¢ Rule-based systems implemented by linguists-programmers.
Until NMT, it was best to combine the approaches.
2/51
Zap through History
â€¢ Rule-Based MT.
Linguists/language experts write rules.
Controlled Language: Authors restricted to produce MT-translatable text.
â€¢ Example-Based MT.
Given translation memories, find examples similar to input.
â€¢ Statistical MT:
1. Word-Based.
2. Phrase-Based.
3. Syntax-Based.
4. Neural.
3/51
Quotes
Warren Weaver (1949):
I have a text in front of me which is written in Russian but I am going to
pretend that it is really written in English and that is has been coded in some
strange symbols. All I need to do is strip off the code in order to retrieve
the information contained in the text.
Noam Chomsky (1969):
â€¦the notion â€œprobability of a sentenceâ€ is an entirely useless one, under any
known interpretation of this term.
Frederick Jelinek (80â€™s; IBM; later JHU and sometimes ÃšFAL)
Every time I fire a linguist, the accuracy goes up.
Hermann Ney (RWTH Aachen University):
MT = Linguistic Modelling + Statistical Decision Theory 4/51
The Statistical Approach
(Statistical = Information-theoretic.)
â€¢ Specify a probabilistic model.
= How is the probability mass distributed among possible outputs given
observed inputs.
â€¢ Specify the training criterion and procedure.
= How to learn free parameters from training data.
Notice:
â€¢ Linguistics helpful when designing the models:
â€¢ How to divide input into smaller units.
â€¢ Which bits of observations are more informative.
5/51
Ultimate Goal of Traditional SMT
Find minimum translation units (MTUs) âˆ¼ graph partitions:
â€¢ such that they are frequent across many sentence pairs.
â€¢ without imposing (too hard) constraints on reordering.
â€¢ (ideally in an unsupervised fashion, no reliance on linguistics).
Available data: Word co-occurrence statistics:
â€¢ In large monolingual data (usually up to 109 words).
â€¢ In smaller parallel data (up to 107 words per language).
â€¢ Optional automatic rich linguistic annotation.
6/51
Statistical MT
Given a source (foreign) language sentence ğ‘“ğ½
1 = ğ‘“1 â€¦ ğ‘“ğ‘— â€¦ ğ‘“ğ½ ,
Produce a target language (English) sentence ğ‘’ğ¼
1 = ğ‘’1 â€¦ ğ‘’ğ‘— â€¦ ğ‘’ğ¼ .
Among all possible target language sentences, choose the sentence with
the highest probability:Ì‚
ğ‘’Ì‚
ğ¼
1 = argmax
ğ¼,ğ‘’ğ¼
1
ğ‘(ğ‘’ğ¼
1|ğ‘“ğ½
1 ) (1)
We stick to the ğ‘’ğ¼
1, ğ‘“ğ½
1 notation despite translating from English to Czech.
7/51
Brute-Force MT
Translate only sentences listed in a â€œtranslation memoryâ€ (TM):
Good morning. = DobrÃ© rÃ¡no.
How are you? = Jak se mÃ¡Å¡?
How are you? = Jak se mÃ¡te?
ğ‘(ğ‘’ğ¼
1|ğ‘“ğ½
1 ) = { 1 if ğ‘’ğ¼
1 = ğ‘“ğ½
1 seen in the TM
0 otherwise (2)
Any problems with the definition?
8/51
Brute-Force MT
Translate only sentences listed in a â€œtranslation memoryâ€ (TM):
Good morning. = DobrÃ© rÃ¡no.
How are you? = Jak se mÃ¡Å¡?
How are you? = Jak se mÃ¡te?
ğ‘(ğ‘’ğ¼
1|ğ‘“ğ½
1 ) = { 1 if ğ‘’ğ¼
1 = ğ‘“ğ½
1 seen in the TM
0 otherwise (2)
â€¢ Not a probability. There may be ğ‘“ğ½
1 , s.t. âˆ‘ğ‘’ğ¼
1
ğ‘(ğ‘’ğ¼
1|ğ‘“ğ½
1 ) > 1.
â‡’ Have to normalize, use count(ğ‘’ğ¼
1,ğ‘“ğ½
1 )
count(ğ‘“ğ½
1 ) instead of 1.
â€¢ Not â€œsmoothâ€, no generalization:
Good morning. â‡’ DobrÃ© rÃ¡no.
Good evening. â‡’ âˆ… 8/51
Bayesâ€™ Law
Bayesâ€™ law for conditional probabilities: ğ‘(ğ‘|ğ‘) = ğ‘(ğ‘|ğ‘)ğ‘(ğ‘)
ğ‘(ğ‘)
So in our case:Ì‚
ğ‘’Ì‚
ğ¼
1 = argmax
ğ¼,ğ‘’ğ¼
1
ğ‘(ğ‘’ğ¼
1|ğ‘“ğ½
1 ) Apply Bayesâ€™ law
= argmax
ğ¼,ğ‘’ğ¼
1
ğ‘(ğ‘“ğ½
1 |ğ‘’ğ¼
1)ğ‘(ğ‘’ğ¼
1)
ğ‘(ğ‘“ğ½
1 )
ğ‘(ğ‘“ğ½
1 ) constant
â‡’ irrelevant in maximization
= argmax
ğ¼,ğ‘’ğ¼
1
ğ‘(ğ‘“ğ½
1 |ğ‘’ğ¼
1)ğ‘(ğ‘’ğ¼
1)
Also called â€œNoisy Channelâ€ model.
9/51
Motivation for Noisy ChannelÌ‚
ğ‘’Ì‚
ğ¼
1 = argmax
ğ¼,ğ‘’ğ¼
1
ğ‘(ğ‘“ğ½
1 |ğ‘’ğ¼
1)ğ‘(ğ‘’ğ¼
1) (3)
Bayesâ€™ law divided the model into components:
ğ‘(ğ‘“ğ½
1 |ğ‘’ğ¼
1) Translation model (â€œreversedâ€, ğ‘’ğ¼
1 â†’ ğ‘“ğ½
1 )
â€¦is it a likely translation?
ğ‘(ğ‘’ğ¼
1) Language model (LM)
â€¦is the output a likely sentence of the target language?
â€¢ The components can be trained on different sources.
There are far more monolingual data â‡’ language model can be more reliable.
10/51
Without EquationsInput Global Search
for sentence with highest probability Output
Parallel Texts
Translation Model
Monolingual Texts
Language Model
11/51
Summary of Language Models
â€¢ ğ‘(ğ‘’ğ¼
1) should report how â€œgoodâ€ sentence ğ‘’ğ¼
1 is.
â€¢ We surely want ğ‘(The the the.) < ğ‘(Hello.)
â€¢ How about ğ‘(The cat was black.) < ğ‘(Hello.)?
â€¦We donâ€™t really care in MT. We hope to compare synonymic sentences.
LM is usually a 3-gram language model:
ğ‘(â†± â†± The cat was black . â†° â†° ) = ğ‘(The| â†± â†±) ğ‘(cat| â†± The) ğ‘(was|The cat)
ğ‘(black|cat was) ğ‘(.|was black) ğ‘(â†° |black .)
ğ‘(â†° |. â†°)
Formally, with ğ‘› = 3:
ğ‘LM(ğ‘’ğ¼
1) =
ğ¼
âˆ
ğ‘–=1
ğ‘(ğ‘’ğ‘–|ğ‘’ğ‘–âˆ’1
ğ‘–âˆ’ğ‘›+1) (4)
12/51
Estimating and Smoothing LM
ğ‘(ğ‘¤1) = count(ğ‘¤1)
total words observed Unigram probabilities.
ğ‘(ğ‘¤2|ğ‘¤1) = count(ğ‘¤1ğ‘¤2)
count(ğ‘¤1) Bigram probabilities.
ğ‘(ğ‘¤3|ğ‘¤2, ğ‘¤1) = count(ğ‘¤1ğ‘¤2ğ‘¤3)
count(ğ‘¤1ğ‘¤2) Trigram probabilities.
Unseen ngrams (ğ‘(ngram) = 0) are a big problem, invalidate whole
sentence: ğ‘LM(ğ‘’ğ¼
1) = â‹¯ â‹… 0 â‹… â‹¯ = 0
â‡’ Back-off with shorter ngrams:
ğ‘LM(ğ‘’ğ¼
1) = âˆğ¼
ğ‘–=1( 0.8 â‹… ğ‘(ğ‘’ğ‘–|ğ‘’ğ‘–âˆ’1, ğ‘’ğ‘–âˆ’2)+
0.15 â‹… ğ‘(ğ‘’ğ‘–|ğ‘’ğ‘–âˆ’1)+
0.049 â‹… ğ‘(ğ‘’ğ‘–)+
0.001 ) â‰  0
(5)
13/51
From Bayes to Log-Linear Model
Och (2002) discusses some problems of Equation 3:
â€¢ Models estimated unreliably â‡’ maybe LM more important:Ì‚
ğ‘’Ì‚
ğ¼
1 = argmax
ğ¼,ğ‘’ğ¼
1
ğ‘(ğ‘“ğ½
1 |ğ‘’ğ¼
1)(ğ‘(ğ‘’ğ¼
1))2 (6)
â€¢ In practice, â€œdirectâ€ translation model equally good:Ì‚
ğ‘’Ì‚
ğ¼
1 = argmax
ğ¼,ğ‘’ğ¼
1
ğ‘(ğ‘’ğ¼
1|ğ‘“ğ½
1 )ğ‘(ğ‘’ğ¼
1) (7)
â€¢ Complicated to correctly introduce other dependencies.
â‡’ Use log-linear model instead.
14/51
Log-Linear Model (1)
â€¢ ğ‘(ğ‘’ğ¼
1|ğ‘“ğ½
1 ) is modelled as a weighted combination of models, called
â€œfeature functionsâ€: â„1(â‹…, â‹…) â€¦ â„ğ‘€ (â‹…, â‹…)
ğ‘(ğ‘’ğ¼
1|ğ‘“ğ½
1 ) = exp(âˆ‘ğ‘€
ğ‘š=1 ğœ†ğ‘šâ„ğ‘š(ğ‘’ğ¼
1, ğ‘“ğ½
1 ))
âˆ‘ğ‘’â€²ğ¼â€²
1
exp(âˆ‘ğ‘€
ğ‘š=1 ğœ†ğ‘šâ„ğ‘š(ğ‘’â€²ğ¼â€²
1 , ğ‘“ğ½
1 )) (8)
â€¢ Each feature function â„ğ‘š(ğ‘’, ğ‘“) relates source ğ‘“ to target ğ‘’.
E.g. the feature for ğ‘›-gram language model:
â„LM(ğ‘“ğ½
1 , ğ‘’ğ¼
1) = log
ğ¼
âˆ
ğ‘–=1
ğ‘(ğ‘’ğ‘–|ğ‘’ğ‘–âˆ’1
ğ‘–âˆ’ğ‘›+1) (9)
â€¢ Model weights ğœ†ğ‘€
1 specify the relative importance of features. 15/51
Log-Linear Model (2)
As before, the constant denominator not needed in maximization:Ì‚
ğ‘’Ì‚
ğ¼
1 = argmaxğ¼,ğ‘’ğ¼
1
exp(âˆ‘ğ‘€
ğ‘š=1 ğœ†ğ‘šâ„ğ‘š(ğ‘’ğ¼
1, ğ‘“ğ½
1 ))
âˆ‘ğ‘’â€²ğ¼â€²
1
exp(âˆ‘ğ‘€
ğ‘š=1 ğœ†ğ‘šâ„ğ‘š(ğ‘’â€²ğ¼â€²
1 , ğ‘“ğ½
1 ))
= argmaxğ¼,ğ‘’ğ¼
1
exp(âˆ‘ğ‘€
ğ‘š=1 ğœ†ğ‘šâ„ğ‘š(ğ‘’ğ¼
1, ğ‘“ğ½
1 ))
(10)
16/51
Relation to Noisy Channel
With equal weights and only two features:
â€¢ â„TM(ğ‘’ğ¼
1, ğ‘“ğ½
1 ) = log ğ‘(ğ‘“ğ½
1 |ğ‘’ğ¼
1) for the translation model,
â€¢ â„LM(ğ‘’ğ¼
1, ğ‘“ğ½
1 ) = log ğ‘(ğ‘’ğ¼
1) for the language model,
log-linear model reduces to Noisy Channel:Ì‚
ğ‘’Ì‚
ğ¼
1 = argmaxğ¼,ğ‘’ğ¼
1
exp(âˆ‘ğ‘€
ğ‘š=1 ğœ†ğ‘šâ„ğ‘š(ğ‘’ğ¼
1, ğ‘“ğ½
1 ))
= argmaxğ¼,ğ‘’ğ¼
1
exp(â„TM(ğ‘’ğ¼
1, ğ‘“ğ½
1 ) + â„LM(ğ‘’ğ¼
1, ğ‘“ğ½
1 ))
= argmaxğ¼,ğ‘’ğ¼
1
exp(log ğ‘(ğ‘“ğ½
1 |ğ‘’ğ¼
1) + log ğ‘(ğ‘’ğ¼
1))
= argmaxğ¼,ğ‘’ğ¼
1
ğ‘(ğ‘“ğ½
1 |ğ‘’ğ¼
1)ğ‘(ğ‘’ğ¼
1)
(11)
17/51
Common Features of PBMT
â€¢ Phrase translation probability:
â„Phr(ğ‘“ğ½
1 , ğ‘’ğ¼
1, ğ‘ ğ¾
1 ) = log âˆğ¾
ğ‘˜=1 ğ‘(Ìƒğ‘“ğ‘˜|Ìƒğ‘’ğ‘˜) where ğ‘(Ìƒğ‘“ğ‘˜|Ìƒğ‘’ğ‘˜) = count(Ìƒğ‘“,Ìƒ ğ‘’)
count(Ìƒğ‘’)
â‡’ Are all used unitsÌƒ ğ‘“ â†”Ìƒ ğ‘’ likely translations?
â€¢ Word count/penalty: â„wp(ğ‘’ğ¼
1, â‹…, â‹…) = ğ¼
â‡’ Do we prefer longer or shorter output?
â€¢ Phrase count/penalty: â„pp(â‹…, â‹…, ğ‘ ğ¾
1 ) = ğ¾
â‡’ Do we prefer translation in more or fewer less-dependent bits?
â€¢ Reordering model: different basic strategies (Lopez, 2009)
â‡’ Which source spans can provide continuation at a moment?
â€¢ ğ‘›-gram LM: â„LM(â‹…, ğ‘’ğ¼
1, â‹…) = log âˆğ¼
ğ‘–=1 ğ‘(ğ‘’ğ‘–|ğ‘’ğ‘–âˆ’1
ğ‘–âˆ’ğ‘›+1)
â‡’ Is output ğ‘›-gram-wise coherent?
18/51
Features: Constructing or Scoring?
Are features used to construct hypotheses or just score them?
â€¢ Phrase translation probabilities: â‡’ for construction, see below.
â€¢ Counts/penalties: â‡’ for scoring only.
â€¢ Language models: â‡’ for scoring only.
But it could be used for construction: predict next word and confirm from
translation.
19/51
Traditional MT â€œPipelineâ€
â€œTraining the Translation Modelâ€
1. Find relevant parallel texts.
2. Align at the level of sentences.
3. Align at the level of words.
4. Extract translation units, with scores (co-oc. stats.).
(Language Model similar, â€œsimpleâ€ words co-oc. stats, no alignment.)
â€œTuningâ€ (â€œMERTâ€) = Actual training in the ML sense
5. Identify TM/LM/other model component weights.
Translation:
6. Decompose input into known units.
7. Search for best combinations of units. 20/51
1: Align Training SentencesNemÃ¡m Å¾Ã¡dnÃ©ho psa.
I have no dog.
VidÄ›l koÄku.
a cat.He saw
21/51
2: Align WordsNemÃ¡m Å¾Ã¡dnÃ©ho psa.
I have no dog.
VidÄ›l koÄku.
a cat.He saw
22/51
3: Extract Phrase Pairs (MTUs)NemÃ¡m Å¾Ã¡dnÃ©ho psa.
I have no dog.
VidÄ›l koÄku.
a cat.He saw
23/51
4: New InputNemÃ¡m Å¾Ã¡dnÃ©ho psa.
I have no dog.
VidÄ›l koÄku.
a cat.He saw
New input: koÄku.NemÃ¡m
24/51
4: New InputNemÃ¡m Å¾Ã¡dnÃ©ho psa.
I have no dog.
VidÄ›l koÄku.
a cat.He saw
New input: koÄku.NemÃ¡m
... I don't have cat.
25/51
5: Pick Probable Phrase Pairs (TM)NemÃ¡m Å¾Ã¡dnÃ©ho psa.
I have no dog.
VidÄ›l koÄku.
a cat.He saw
New input: koÄku.NemÃ¡m
... I don't have cat.
New input: NemÃ¡m
I have
26/51
6: So That ğ‘›-Grams Probable (LM)NemÃ¡m Å¾Ã¡dnÃ©ho psa.
I have no dog.
VidÄ›l koÄku.
a cat.He saw
New input: koÄku.NemÃ¡m
... I don't have cat.
New input: NemÃ¡m
I have
koÄku.
a cat.
27/51
Meaning Got Reversed!NemÃ¡m Å¾Ã¡dnÃ©ho psa.
I have no dog.
VidÄ›l koÄku.
a cat.He saw
New input: koÄku.NemÃ¡m
... I don't have cat.
New input: NemÃ¡m
I have
koÄku.
a cat. âœ˜
28/51
What Went Wrong?Ì‚
ğ‘’Ì‚
ğ¼
1 = argmax
ğ¼,ğ‘’ğ¼
1
ğ‘(ğ‘“ğ½
1 |ğ‘’ğ¼
1)ğ‘(ğ‘’ğ¼
1) = argmax
ğ¼,ğ‘’ğ¼
1
âˆ
(Ì‚ğ‘“,Ì‚ğ‘’)âˆˆphrase pairs of ğ‘“ğ½
1 ,ğ‘’ğ¼
1
ğ‘(Ì‚ğ‘“|Ì‚ğ‘’)ğ‘(ğ‘’ğ¼
1) (12)
â€¢ Too strong phrase-independence assumption.
â€¢ Phrases do depend on each other.
Here â€œnemÃ¡mâ€ and â€œÅ¾Ã¡dnÃ©hoâ€ jointly express one negation.
â€¢ Word alignments ignored that dependence.
But adding it would increase data sparseness.
â€¢ Language model is a separate unit.
â€¢ ğ‘(ğ‘’ğ¼
1) models the target sentence independently of ğ‘“ğ½
1 .
29/51
Redefining ğ‘(ğ‘’ğ¼
1|ğ‘“ğ½
1 )
What if we modelled ğ‘(ğ‘’ğ¼
1|ğ‘“ğ½
1 ) directly, word by word:
ğ‘(ğ‘’ğ¼
1|ğ‘“ğ½
1 ) = ğ‘(ğ‘’1, ğ‘’2, â€¦ ğ‘’ğ¼ |ğ‘“ğ½
1 )
= ğ‘(ğ‘’1|ğ‘“ğ½
1 ) â‹… ğ‘(ğ‘’2|ğ‘’1, ğ‘“ğ½
1 ) â‹… ğ‘(ğ‘’3|ğ‘’2, ğ‘’1, ğ‘“ğ½
1 ) â€¦
=
ğ¼
âˆ
ğ‘–=1
ğ‘(ğ‘’ğ‘–|ğ‘’1, â€¦ ğ‘’ğ‘–âˆ’1, ğ‘“ğ½
1 )
(13)
â€¦this is â€œjust a cleverer language model:â€ ğ‘(ğ‘’ğ¼
1) = âˆğ¼
ğ‘–=1 ğ‘(ğ‘’ğ‘–|ğ‘’1, â€¦ ğ‘’ğ‘–âˆ’1)
Main Benefit: All dependencies available.
But what technical device can learn this?
30/51
NNs: Universal Approximators
â€¢ A neural network with a single hidden layer (possibly huge) can
approximate any continuous function to any precision.
â€¢ (Nothing claimed about learnability.)
https://www.quora.com/How-can-a-deep-neural-network-with-ReLU-activations-in-its-hidden-layers-approximate-any-function
31/51
Play with playground.tensorflow.org
âˆ’0.43ğ‘¥1 âˆ’ 0.89ğ‘¥2 + 2.0 > 0
and âˆ’0.67ğ‘¥1 + 0.89ğ‘¥2 + 2.1 > 0
and 1.4ğ‘¥1 âˆ’ 0.067ğ‘¥2 + 2.3 > 0 32/51
A DL â€œProgramâ€ Is Just a Computationâ€¦
In fact: 1 tanh(âˆ’0.43ğ‘¥1âˆ’0.89ğ‘¥2 + 2.0)
+1 tanh(âˆ’0.67ğ‘¥1+0.89ğ‘¥2 + 2.1)
+1 tanh(1.4ğ‘¥1âˆ’0.067ğ‘¥2 + 2.3)âˆ’ğœ‹/2 > 0 33/51
â€¦ with Parameters Guessed Automatically
In fact: 1 tanh(âˆ’0.43ğ‘¥1âˆ’0.89ğ‘¥2 + 2.0)
+1 tanh(âˆ’0.67ğ‘¥1+0.89ğ‘¥2 + 2.1)
+1 tanh(1.4ğ‘¥1âˆ’0.067ğ‘¥2 + 2.3)âˆ’ğœ‹/2 > 0 34/51
Perfect Features
1ğ‘¥2
1 + 1ğ‘¥2
2 âˆ’ 1 < 0
35/51
Bad Features & Low Depth
36/51
Too Complex NN Fails to Learn
37/51
Deep NNs for Image Classification
38/51
Processing Text with NNs
â€¢ Map each word to a vector of 0s and 1s (â€œ1-hot repr.â€):
cat â†¦ (0, 0, â€¦ , 0, 1, 0, â€¦ , 0)
â€¢ Sentence is then a matrix: the cat is on the mat
â†‘ a 0 0 0 0 0 0
about 0 0 0 0 0 0
â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦
cat 0 1 0 0 0 0
Vocabulary size: â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦
1.3M English is 0 0 1 0 0 0
2.2M Czech â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦
the 1 0 0 0 1 0
â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦
â†“ zebra 0 0 0 0 0 0
Main drawback: No relations, all words equally close/far. 39/51
Processing Text with NNs
â€¢ Map each word to a vector of 0s and 1s (â€œ1-hot repr.â€):
cat â†¦ (0, 0, â€¦ , 0, 1, 0, â€¦ , 0)
â€¢ Sentence is then a matrix: the cat is on the mat
â†‘ a 0 0 0 0 0 0
about 0 0 0 0 0 0
â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦
cat 0 1 0 0 0 0
Vocabulary size: â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦
1.3M English is 0 0 1 0 0 0
2.2M Czech â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦
the 1 0 0 0 1 0
â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦
â†“ zebra 0 0 0 0 0 0
Main drawback: No relations, all words equally close/far. 40/51
Processing Text with NNs
â€¢ Map each word to a vector of 0s and 1s (â€œ1-hot repr.â€):
cat â†¦ (0, 0, â€¦ , 0, 1, 0, â€¦ , 0)
â€¢ Sentence is then a matrix: the cat is on the mat
â†‘ a 0 0 0 0 0 0
about 0 0 0 0 0 0
â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦
cat 0 1 0 0 0 0
Vocabulary size: â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦
1.3M English is 0 0 1 0 0 0
2.2M Czech â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦
the 1 0 0 0 1 0
â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦
â†“ zebra 0 0 0 0 0 0
Main drawback: No relations, all words equally close/far. 41/51
Solution: Word Embeddings
â€¢ Idea: Map each word to a dense vector.
â€¢ Result: 300â€“2000 dimensions instead of 1â€“2M.
â€¢ The dimensions have no clear interpretation.
â€¢ The â€œembeddingâ€ is the mapping.
â€¢ Technically, the first layer of NNs for NLP is the matrix that maps 1-hot
input to the first layer.
â€¢ Embeddings are trained for each particular task.
â€¢ Sentence classification (sentiment analysis, etc.)
â€¢ Neural language modelling.
â€¢ The famous word2vec (Mikolov et al., 2013):
â€¢ CBOW: Predict the word from its four neighbours.
â€¢ Skip-gram: Predict likely neighbours given the word.
â€¢ End-to-end neural MT.
42/51
Further Compression: Sub-Words
â€¢ SMT struggled with productive morphology (>1M wordforms).
nejneobhodpodaÅ™ovÃ¡vatelnÄ›jÅ¡Ã­mi, DonaudampfschifffahrtsgesellschaftskapitÃ¤n
â€¢ NMT can handle only 30â€“80k dictionaries.
â‡’ Resort to sub-word units.
Orig ÄeskÃ½ politik svezl migranty
Syllables Äes kÃ½ âŠ” po li tik âŠ” sve zl âŠ” mig ran ty
Morphemes Äesk Ã½ âŠ” politik âŠ” s vez l âŠ” migrant y
Char Pairs Äe sk Ã½ âŠ” po li ti k âŠ” sv ez l âŠ” mi gr an ty
Chars Ä e s k Ã½ âŠ” p o l i t i k âŠ” s v e z l âŠ” m i g r a n t y
BPE 30k ÄeskÃ½ politik s@@ vez@@ l mi@@ granty
BPE (Byte-Pair Encoding) uses ğ‘› most common substrings (incl. frequent words).
43/51
Variable-Length Inputs
Variable-length input can be handled by recurrent NNs:
â€¢ Reading one input symbol at a time.
â€¢ The same (trained) transformation used every time.
â€¢ Unroll in time (up to a fixed length limit).
Tricks needed to train (to avoid â€œvanishing gradientsâ€):
â€¢ LSTM, Long Short-Term Memory Cells (Hochreiter and Schmidhuber, 1997).
â€¢ GRU, Gated Recurrent Unit Cells (Chung et al., 2014).
44/51
NNs as Translation Model in SMT
Cho et al. (2014) proposed:
â€¢ encoder-decoder architecture and
â€¢ GRU unit (name given later by Chung et al. (2014))
â€¢ to score variable-length phrase pairs in PBMT.x1 x2 xT
yT' y2 y1
c
Decoder
Encoder
45/51
NMT: Sequence to Sequence
Sutskever et al. (2014) use:
â€¢ LSTM RNN encoder-decoder
â€¢ to consume
and produce variable-length sentences.
First the Encoder:
46/51
Then the Decoder
Remember: ğ‘(ğ‘’ğ¼
1|ğ‘“ğ½
1 ) = ğ‘(ğ‘’1|ğ‘“ğ½
1 ) â‹… ğ‘(ğ‘’2|ğ‘’1, ğ‘“ğ½
1 ) â‹… ğ‘(ğ‘’3|ğ‘’2, ğ‘’1, ğ‘“ğ½
1 ) â€¦
â€¢ Again RNN, producing one word at a time.
â€¢ The produced word fed back into the network.
â€¢ (Word embeddings in the target language used here.)
47/51
Encoder-Decoder Architecture
https://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-2/
48/51
Ultimate Goal of SMT vs. NMT
Goal of â€œclassicalâ€ SMT (e.g. PBMT):
Find minimum translation units âˆ¼ graph partitions:
â€¢ such that they are frequent across many sentence pairs.
â€¢ without imposing (too hard) constraints on reordering.
â€¢ in an unsupervised fashion.
Goal of neural MT:
Avoid minimum translation units. Find NN architecture that
â€¢ Reads input in as original form as possible.
â€¢ Produces output in as final form as possible.
â€¢ Can be optimized end-to-end in practice.
49/51
Summary of the Lecture
â€¢ Statistical MT chooses the most probable sentence:Ì‚
ğ‘’Ì‚
ğ¼
1 = argmaxğ¼,ğ‘’ğ¼
1
ğ‘(ğ‘’ğ¼
1|ğ‘“ğ½
1 )
â€¢ The probability modelled in Bayesâ€™ or Log-Linear decomposition:Ì‚
ğ‘’Ì‚
ğ¼
1 = argmaxğ¼,ğ‘’ğ¼
1
ğ‘(ğ‘“ğ½
1 |ğ‘’ğ¼
1)ğ‘(ğ‘’ğ¼
1)
orÌ‚ ğ‘’Ì‚ ğ¼
1 = argmaxğ¼,ğ‘’ğ¼
1
exp(âˆ‘ğ‘€
ğ‘š=1 ğœ†ğ‘šâ„ğ‘š(ğ‘’ğ¼
1, ğ‘“ğ½
1 ))
â€¢ Phrase-Based MT models ğ‘(ğ‘“ğ½
1 |ğ‘’ğ¼
1) as product of phrase
translation probabilities in a segmentation ğ‘ ğ¾
1 : âˆğ¾
ğ‘˜=1 ğ‘(Ìƒğ‘“ğ‘˜|Ìƒğ‘’ğ‘˜)
â€¢ Other (ling.-motivated) decompositions or features possible.
â€¢ Probabilities estimated from data (parallel/monolingual).
â€¢ Neural MT predict word by word; â€œjust a clever LMâ€.
â€¢ Sub-word units, word embeddings, encoder-decoder.
50/51
References
Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. 2014. Learning phrase representations using rnn encoderâ€“decoder for statistical machine translation. In
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages
1724â€“1734, Doha, Qatar, October. Association for Computational Linguistics.
Junyoung Chung, Ã‡aglar GÃ¼lÃ§ehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated recurrent
neural networks on sequence modeling. CoRR, abs/1412.3555.
Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long short-term memory. Neural Comput., 9(8):1735â€“1780,
November.
Adam Lopez. 2009. Translation as weighted deduction. In Proceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009), pages 532â€“540, Athens, Greece, March. Association for Computational Linguistics.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector
space. CoRR, abs/1301.3781.
Franz Joseph Och. 2002. Statistical Machine Translation: From Single-Word Models to Alignment Templates. Ph.D.
thesis, RWTH Aachen University.
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to Sequence Learning with Neural Networks. In
Advances in neural information processing systems, pages 3104â€“3112.
51/51


Basic
Sequence-to-Sequence
(with Attention)
OndÅ™ej Bojar
March 12, 2020
NPFL087 Statistical Machine Translation
Charles University
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics unless otherwise stated
Outline
â€¢ Basic NN building blocks for NMT.
â€¢ Processing Text.
â€¢ Neural Language Model.
â€¢ Vanilla Sequence-to-Sequence.
â€¢ Attention.
Many of the slides based on RANLP 2017 tutorial (Helcl and Bojar, 2017).
1/40
Encoder-Decoder Architecture
https://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-2/
2/40
Basic NN Building Blocks
One Fully Connected Layer
â€¢ One fully-connected layer converts an input (column) vector ğ‘¥
to an output (column) vector â„:
â„ = ğ‘“(ğ‘Š ğ‘¥ + ğ‘), (1)
â€¢ ğ‘Š is a weight matrix of input columns and output rows,
â€¢ ğ‘ a bias vector of length of output,
â€¢ ğ‘“(â‹…) is a non-linearity applied usually elementwise.W
x
b
+f )(
3/40
One Layer tanh(ğ‘Š ğ‘¥ + ğ‘), 2Dâ†’2D
Skew:
ğ‘Š
Transpose:
ğ‘
Non-lin.:
tanh
Animation by http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/
4/40
Feed-Forward Neural Network
ğ‘¥
â†“ â†“
â„1 = ğ‘“(ğ‘Š1ğ‘¥ + ğ‘1) ï¿¿
â†“â†‘ â†“ â†‘
â„2 = ğ‘“(ğ‘Š2â„1 + ğ‘2) ï¿¿
â†“â†‘ â†“ â†‘
â‹® â‹® ï¿¿
â†“â†‘ â†“ â†‘
â„ğ‘› = ğ‘“(ğ‘Šğ‘›â„ğ‘›âˆ’1 + ğ‘ğ‘›) ï¿¿
â†“â†‘ â†“ â†‘
ğ‘œ = ğ‘”(ğ‘Šğ‘œâ„ğ‘› + ğ‘ğ‘œ) âˆ‚ğ¸
âˆ‚ğ‘Šğ‘œ
= âˆ‚ğ¸
âˆ‚ğ‘œ â‹… âˆ‚ğ‘œ
âˆ‚ğ‘Šğ‘œ
â†“ â†“ â†‘
ğ¸ = ğ‘’(ğ‘œ, ğ‘¡) â†’ âˆ‚ğ¸
âˆ‚ğ‘œ
blue: Training item (input ğ‘¥, output ğ‘¡), red: Trainable parameters (ğ‘Š1, ğ‘1, â€¦ ). 5/40
Four Layers, Disentagling Spirals
Animation by http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/ 6/40
Variable-Length Inputs and Outputs
Variable-length input can be handled by recurrent NNs:
â€¢ Processing one input symbol at a time.
â€¢ Initial state â„0 = (0) (or some sentence representation).
â€¢ The same (trained) transformation ğ´ used every time.
â„ğ‘¡ = ğ´(â„ğ‘¡âˆ’1, ğ‘¥ğ‘¡) (2)
â€¢ Unroll in time (up to a fixed length limit).
7/40
Vanilla RNN
â„ğ‘¡ = tanh (ğ‘Š [â„ğ‘¡âˆ’1; ğ‘¥ğ‘¡] + ğ‘) (3)
[â„ğ‘¡âˆ’1; ğ‘¥ğ‘¡] is concatenation of â„ğ‘¡âˆ’1 and ğ‘¥ğ‘¡
â€¢ Vanishing gradient problem.
â€¢ Non-linear transformation always applied.
â‡’ Type theory: â„ğ‘¡ and â„ğ‘¡âˆ’1 live in different vector spaces.
8/40
LSTM and GRU Cells for RNN
â€¢ LSTM, Long Short-Term Memory Cells (Hochreiter and Schmidhuber, 1997).
â€¢ GRU, Gated Recurrent Unit Cells (Chung et al., 2014):
ğ‘§ğ‘¡ = ğœ (ğ‘Šğ‘§[â„ğ‘¡âˆ’1; ğ‘¥ğ‘¡] + ğ‘ğ‘§) (4)
ğ‘Ÿğ‘¡ = ğœ (ğ‘Šğ‘Ÿ[â„ğ‘¡âˆ’1; ğ‘¥ğ‘¡] + ğ‘ğ‘Ÿ) (5)Ìƒ
â„ğ‘¡ = tanh (ğ‘Š [ğ‘Ÿğ‘¡ âŠ™ â„ğ‘¡âˆ’1; ğ‘¥ğ‘¡]) (6)
â„ğ‘¡ = (1 âˆ’ ğ‘§ğ‘¡) âŠ™ â„ğ‘¡âˆ’1 + ğ‘§ğ‘¡ âŠ™Ìƒ â„ğ‘¡ (7)
â€¢ Gates control:
â€¢ what to use from input ğ‘¥ğ‘¡ (GRU: everything),
â€¢ what to use from hidden state â„ğ‘¡âˆ’1 (reset gate ğ‘Ÿğ‘¡),
â€¢ what to put into output (update gate ğ‘§ğ‘¡)
â€¢ Linear â€œinformation highwayâ€ preserved.
â‡’ All states â„ğ‘¡ belong to the same vector space. 9/40
Processing Text
From Categorical Words to Numbers
â€¢ Map each word to a vector of 0s and 1s (â€œ1-hot repr.â€):
cat â†¦ (0, 0, â€¦ , 0, 1, 0, â€¦ , 0)
â€¢ Sentence is then a matrix: the cat is on the mat
â†‘ a 0 0 0 0 0 0
about 0 0 0 0 0 0
â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦
cat 0 1 0 0 0 0
Vocabulary size: â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦
1.3M English is 0 0 1 0 0 0
2.2M Czech â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦
on 0 0 0 1 0 0
â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦
the 1 0 0 0 1 0
â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦
â†“ zebra 0 0 0 0 0 0
10/40
Sub-Words to Reduce Vocabulary Size
â€¢ SMT struggles with productive morphology (>1M wordforms).
nejneobhodpodaÅ™ovÃ¡vatelnÄ›jÅ¡Ã­mi, DonaudampfschifffahrtsgesellschaftskapitÃ¤n
â€¢ NMT can handle only 30â€“80k dictionaries.
â‡’ Resort to sub-word units.
Orig ÄeskÃ½ politik svezl migranty
Syllables Äes kÃ½ âŠ” po li tik âŠ” sve zl âŠ” mig ran ty
Morphemes Äesk Ã½ âŠ” politik âŠ” s vez l âŠ” migrant y
Char Pairs Äe sk Ã½ âŠ” po li ti k âŠ” sv ez l âŠ” mi gr an ty
Chars Ä e s k Ã½ âŠ” p o l i t i k âŠ” s v e z l âŠ” m i g r a n t y
BPE 30k ÄeskÃ½ politik s@@ vez@@ l mi@@ granty
BPE (Byte-Pair Encoding, (Sennrich et al., 2016)) or Googleâ€™s wordpieces (Wu et al., 2016) and
Tensor2Tensorâ€™s SubwordTextEncoder use ğ‘› most common substrings (incl. frequent words).
11/40
Word (Actually Token) Embeddings
â€¢ Idea: Map each token to a dense vector in continuous space.
â€¢ Result: 300â€“2000 dimensions instead of 1â€“2M.
â€¢ The dimensions have no clear interpretation.
â€¢ The â€œembeddingâ€ is the mapping.
â€¢ Technically, the first layer of NNs for NLP is the matrix that maps 1-hot
input to the first layer.
â€¢ Embeddings are trained for each particular task.
â€¢ Sentence classification (sentiment analysis, etc.)
â€¢ Neural language modelling.
â€¢ The famous word2vec (Mikolov et al., 2013):
â€¢ CBOW: Predict the word from its four neighbours.
â€¢ Skip-gram: Predict likely neighbours given the word.
â€¢ End-to-end neural MT.
12/40
Output: Softmax over Vocabulary
Outputs of the RNN are:
1. Projected (scaled up) to the size of the vocabulary ğ‘‰ ,
2. Normalized with softmax.
â‡’ Distribution over all possible target tokens.
ğ‘™(ğ‘¤)ğ‘¡ = ğ‘Šğ‘™â„ğ‘¡ + ğ‘ğ‘™
ğ‘(ğ‘¤)ğ‘¡ = exp ğ‘™(ğ‘¤)ğ‘¡
âˆ‘ğ‘¤â€²âˆˆğ‘‰ exp ğ‘™(ğ‘¤â€²)ğ‘¡
â€¢ ğ‘™(ğ‘¤)ğ‘¡ = logits/energies for word ğ‘¤ in time ğ‘¡
â€¢ ğ‘Šğ‘™: weight matrix (hidden state Ã— voc. size)
â€¦ this is big.
â€¢ Softmax normalization: exp â‹…
âˆ‘ exp â‹…
â€¦ this is costly.
â€¢ Tricks what to do with it
(negative sampling, hierarchical softmax)
â€“ not frequently used 13/40
Neural Language Modeling
RNN Language Model
â€¢ Train RNN as a classifier for next words (unlimited history):<s> w 1 w 2 w 3 w 4
p(w 1 ) p(w 2 ) p(w 3 ) p(w 4 ) p(w 5 )
â€¢ Can be used:
â€¢ To estimate sentence probability / perplexity.
â€¢ To sample from the distribution:<s>
~w 1 ~w 2 ~w 3 ~w 4 ~w 5
14/40
Two Views on RNN LM
â€¢ RNN is a for loop / functional map over sequential data
â€¢ all outputs are conditional distributions
â†’ probabilistic distribution over sequences of words
ğ‘ƒ (ğ‘¤1, â€¦ , ğ‘¤ğ‘›) =
ğ‘›
âˆ
ğ‘–=1
ğ‘ƒ (ğ‘¤ğ‘–|ğ‘¤ğ‘–âˆ’1, â€¦ , ğ‘¤1)
15/40
Bidirectional RNN for Input<s> x 1 x 2 x 3 x 4
h 1h 0 h 2 h 3 h 4
...
16/40
Bidirectional RNN for Input<s> x 1 x 2 x 3 x 4
h 1h 0 h 2 h 3 h 4
...
â€¢ read the input sentence from both sides
16/40
Bidirectional RNN for Input<s> x 1 x 2 x 3 x 4
h 1h 0 h 2 h 3 h 4
...
â€¢ read the input sentence from both sides
â€¢ concatenate hidden states from each direction
â€¢ every â„ğ‘– stores information about the whole sentence
16/40
Encoder-Decoder
Architecture
Encoder-Decoder Architecture
â€¢ exploits the conditional LM scheme
Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. â€œSequence to sequence learning with neural networks.â€ Advances in neural information processing systems. 2014.
17/40
Encoder-Decoder Architecture
â€¢ exploits the conditional LM scheme
â€¢ two networks
Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. â€œSequence to sequence learning with neural networks.â€ Advances in neural information processing systems. 2014.
17/40
Encoder-Decoder Architecture
â€¢ exploits the conditional LM scheme
â€¢ two networks
1. a network processing the input sentence into a single vector representation
(encoder)
Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. â€œSequence to sequence learning with neural networks.â€ Advances in neural information processing systems. 2014.
17/40
Encoder-Decoder Architecture
â€¢ exploits the conditional LM scheme
â€¢ two networks
1. a network processing the input sentence into a single vector representation
(encoder)
2. a neural language model initialized with the output of the encoder
(decoder)
Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. â€œSequence to sequence learning with neural networks.â€ Advances in neural information processing systems. 2014.
17/40
Encoder-Decoder Model â€“ Image<s><s> x 1 x 2 x 3 x 4
~y 1 ~y 2 ~y 3 ~y 4 ~y 5
18/40
Encoder-Decoder Model â€“ Image<s><s> x 1 x 2 x 3 x 4
~y 1 ~y 2 ~y 3 ~y 4 ~y 5
source language input + target language LM
18/40
Encoder-Decoder Model â€“ Code
state = np.zeros(sent_repr_size)
for w in input_words:
input_embedding = source_embeddings[w]
state, _ = enc_cell(state, input_embedding)
last_w = "<s>"
while last_w != "</s>":
last_w_embedding = target_embeddings[last_w]
state, dec_output = dec_cell(state, last_w_embedding)
logits = output_projection(dec_output)
last_w = np.argmax(logits)
yield last_w
19/40
Encoder-Decoder Model â€“ Formal Notation
Data
input tokens (source language) x = (ğ‘¥1, â€¦ , ğ‘¥ğ‘‡ğ‘¥ )
output tokens (target language) y = (ğ‘¦1, â€¦ , ğ‘¦ğ‘‡ğ‘¦ )Ì‚Ì‚Ì‚Ì‚
20/40
Encoder-Decoder Model â€“ Formal Notation
Data
input tokens (source language) x = (ğ‘¥1, â€¦ , ğ‘¥ğ‘‡ğ‘¥ )
output tokens (target language) y = (ğ‘¦1, â€¦ , ğ‘¦ğ‘‡ğ‘¦ )
Encoder
initial state â„0 â‰¡ 0
ğ‘—-th state â„ğ‘— = RNNenc(â„ğ‘—âˆ’1, ğ‘¥ğ‘—) = tanh(ğ‘ˆğ‘’â„ğ‘—âˆ’1 + ğ‘Šğ‘’ğ¸ğ‘’ğ‘¥ğ‘— + ğ‘ğ‘’)
final state â„ğ‘‡ğ‘¥Ì‚Ì‚Ì‚Ì‚
20/40
Encoder-Decoder Model â€“ Formal Notation
Data
input tokens (source language) x = (ğ‘¥1, â€¦ , ğ‘¥ğ‘‡ğ‘¥ )
output tokens (target language) y = (ğ‘¦1, â€¦ , ğ‘¦ğ‘‡ğ‘¦ )
Encoder
initial state â„0 â‰¡ 0
ğ‘—-th state â„ğ‘— = RNNenc(â„ğ‘—âˆ’1, ğ‘¥ğ‘—) = tanh(ğ‘ˆğ‘’â„ğ‘—âˆ’1 + ğ‘Šğ‘’ğ¸ğ‘’ğ‘¥ğ‘— + ğ‘ğ‘’)
final state â„ğ‘‡ğ‘¥
Decoder
initial state ğ‘ 0 = â„ğ‘‡ğ‘¥
ğ‘–-th decoder state ğ‘ ğ‘– = RNNdec(ğ‘ ğ‘–âˆ’1,Ì‚ ğ‘¦ğ‘–âˆ’1) = tanh(ğ‘ˆğ‘‘ğ‘ ğ‘–âˆ’1 + ğ‘Šğ‘‘ğ¸ğ‘‘Ì‚ ğ‘¦ğ‘–âˆ’1 + ğ‘ğ‘‘)
ğ‘–-th word score ğ‘¡ğ‘– = tanh(ğ‘ˆğ‘œğ‘ ğ‘– + ğ‘Šğ‘œğ¸ğ‘‘Ì‚ ğ‘¦ğ‘–âˆ’1 + ğ‘ğ‘œ) (â€œoutput projectionâ€)
outputÌ‚ ğ‘¦ğ‘– = arg max ğ‘‰ğ‘œğ‘¡ğ‘–
20/40
Encoder-Decoder: Training Objective
For output word ğ‘¦ğ‘– we have:
â€¢ estimated conditional distributionÌ‚ ğ‘ğ‘– = exp ğ‘¡ğ‘–
âˆ‘ exp ğ‘¡ğ‘—
(softmax function)Ì‚Ì‚Ì‚Ì‚Ì‚
21/40
Encoder-Decoder: Training Objective
For output word ğ‘¦ğ‘– we have:
â€¢ estimated conditional distributionÌ‚ ğ‘ğ‘– = exp ğ‘¡ğ‘–
âˆ‘ exp ğ‘¡ğ‘—
(softmax function)
â€¢ unknown true distribution ğ‘ğ‘–, we lay ğ‘ğ‘– â‰¡ 1 [ğ‘¦ğ‘–]Ì‚Ì‚Ì‚Ì‚Ì‚
21/40
Encoder-Decoder: Training Objective
For output word ğ‘¦ğ‘– we have:
â€¢ estimated conditional distributionÌ‚ ğ‘ğ‘– = exp ğ‘¡ğ‘–
âˆ‘ exp ğ‘¡ğ‘—
(softmax function)
â€¢ unknown true distribution ğ‘ğ‘–, we lay ğ‘ğ‘– â‰¡ 1 [ğ‘¦ğ‘–]
Cross entropy â‰ˆ distance ofÌ‚ ğ‘ and ğ‘:
â„’ = ğ»(Ì‚ğ‘, ğ‘) = Eğ‘ (âˆ’ logÌ‚ ğ‘)Ì‚Ì‚
21/40
Encoder-Decoder: Training Objective
For output word ğ‘¦ğ‘– we have:
â€¢ estimated conditional distributionÌ‚ ğ‘ğ‘– = exp ğ‘¡ğ‘–
âˆ‘ exp ğ‘¡ğ‘—
(softmax function)
â€¢ unknown true distribution ğ‘ğ‘–, we lay ğ‘ğ‘– â‰¡ 1 [ğ‘¦ğ‘–]
Cross entropy â‰ˆ distance ofÌ‚ ğ‘ and ğ‘:
â„’ = ğ»(Ì‚ğ‘, ğ‘) = Eğ‘ (âˆ’ logÌ‚ ğ‘) = âˆ’ âˆ‘
ğ‘£âˆˆğ‘‰
ğ‘(ğ‘£) logÌ‚ ğ‘(ğ‘£) = âˆ’ logÌ‚ ğ‘(ğ‘¦ğ‘–)
21/40
Encoder-Decoder: Training Objective
For output word ğ‘¦ğ‘– we have:
â€¢ estimated conditional distributionÌ‚ ğ‘ğ‘– = exp ğ‘¡ğ‘–
âˆ‘ exp ğ‘¡ğ‘—
(softmax function)
â€¢ unknown true distribution ğ‘ğ‘–, we lay ğ‘ğ‘– â‰¡ 1 [ğ‘¦ğ‘–]
Cross entropy â‰ˆ distance ofÌ‚ ğ‘ and ğ‘:
â„’ = ğ»(Ì‚ğ‘, ğ‘) = Eğ‘ (âˆ’ logÌ‚ ğ‘) = âˆ’ âˆ‘
ğ‘£âˆˆğ‘‰
ğ‘(ğ‘£) logÌ‚ ğ‘(ğ‘£) = âˆ’ logÌ‚ ğ‘(ğ‘¦ğ‘–)
â€¦computing âˆ‚â„’
âˆ‚ğ‘¡ğ‘–
is quite simple
See https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/
21/40
Encoder-Decoder: Training Objective
For output word ğ‘¦ğ‘– we have:
â€¢ estimated conditional distributionÌ‚ ğ‘ğ‘– = exp ğ‘¡ğ‘–
âˆ‘ exp ğ‘¡ğ‘—
(softmax function)
â€¢ unknown true distribution ğ‘ğ‘–, we lay ğ‘ğ‘– â‰¡ 1 [ğ‘¦ğ‘–]
Cross entropy â‰ˆ distance ofÌ‚ ğ‘ and ğ‘:
â„’ = ğ»(Ì‚ğ‘, ğ‘) = Eğ‘ (âˆ’ logÌ‚ ğ‘) = âˆ’ âˆ‘
ğ‘£âˆˆğ‘‰
ğ‘(ğ‘£) logÌ‚ ğ‘(ğ‘£) = âˆ’ logÌ‚ ğ‘(ğ‘¦ğ‘–)
â€¦computing âˆ‚â„’
âˆ‚ğ‘¡ğ‘–
is quite simple
See https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/
â€¦but we expect the model to produce
the exact word at the exact position! 21/40
Implementation: Runtime vs. Training
runtime:Ì‚ ğ‘¦ğ‘— (decoded) Ã— training: ğ‘¦ğ‘— (ground truth)<s> x 1 x 2 x 3 x 4
<s>
~y 1 ~y 2 ~y 3 ~y 4 ~y 5
<s> y 1 y 2 y 3 y 4
loss
22/40
Encoder-Decoder Architecture
Decoding
Greedy Decoding
â€¢ In each step, the model computes a distribution over the
vocabulary ğ‘‰ (given source x, the previous outputs â„, and the
model parameters ğœƒ).
ğ‘(ğ‘¦|â„) = ğ‘”(x, â„, ğœƒ)
23/40
Greedy Decoding
â€¢ In each step, the model computes a distribution over the
vocabulary ğ‘‰ (given source x, the previous outputs â„, and the
model parameters ğœƒ).
ğ‘(ğ‘¦|â„) = ğ‘”(x, â„, ğœƒ)
â€¢ In greedy decoding:
ğ‘¦âˆ— = argmax
ğ‘¦âˆˆğ‘‰
ğ‘(ğ‘¦|â„)
23/40
Greedy Decoding
â€¢ In each step, the model computes a distribution over the
vocabulary ğ‘‰ (given source x, the previous outputs â„, and the
model parameters ğœƒ).
ğ‘(ğ‘¦|â„) = ğ‘”(x, â„, ğœƒ)
â€¢ In greedy decoding:
ğ‘¦âˆ— = argmax
ğ‘¦âˆˆğ‘‰
ğ‘(ğ‘¦|â„)
â€¢ Repeat, until an end-of-sentence symbol (</s>) is decoded.
23/40
Greedy Decoding â€” cont.
â€¢ Pros:
â€¢ Fast and memory-efficient
â€¢ Gives reasonable results
â€¢ Cons:
â€¢ We are interested in the most probable sentence:
(ğ‘¦âˆ—)ğ‘
ğ‘–=0 = argmax
(ğ‘¦)ğ‘
ğ‘–=0
ğ‘(ğ‘¦0, â€¦ , ğ‘¦ğ‘ |â„)
â€¢ Other methods: better results for the cost of a slow-down.
24/40
Beam Search
â€¢ Instead of taking the arg max in every, step, keep a list (or beam)
of ğ‘˜-best scoring hypotheses.
25/40
Beam Search
â€¢ Instead of taking the arg max in every, step, keep a list (or beam)
of ğ‘˜-best scoring hypotheses.
â€¢ Hypothesis = partially decoded sentence â†’ score
25/40
Beam Search
â€¢ Instead of taking the arg max in every, step, keep a list (or beam)
of ğ‘˜-best scoring hypotheses.
â€¢ Hypothesis = partially decoded sentence â†’ score
â€¢ Hypothesis score ğœ“ğ‘¡ = (ğ‘¦1, ğ‘¦2 â€¦ , ğ‘¦ğ‘¡) is the probability of the
decoded sentence prefix up to ğ‘¡-th word.
ğ‘(ğ‘¦1, â€¦ , ğ‘¦ğ‘¡|â„) = ğ‘(ğ‘¦1|â„) â‹… â‹¯ â‹… ğ‘(ğ‘¦ğ‘¡|ğ‘¦1, â€¦ , ğ‘¦ğ‘¡âˆ’1|â„)
25/40
Beam Search
â€¢ Instead of taking the arg max in every, step, keep a list (or beam)
of ğ‘˜-best scoring hypotheses.
â€¢ Hypothesis = partially decoded sentence â†’ score
â€¢ Hypothesis score ğœ“ğ‘¡ = (ğ‘¦1, ğ‘¦2 â€¦ , ğ‘¦ğ‘¡) is the probability of the
decoded sentence prefix up to ğ‘¡-th word.
ğ‘(ğ‘¦1, â€¦ , ğ‘¦ğ‘¡|â„) = ğ‘(ğ‘¦1|â„) â‹… â‹¯ â‹… ğ‘(ğ‘¦ğ‘¡|ğ‘¦1, â€¦ , ğ‘¦ğ‘¡âˆ’1|â„)
â€¢ Rule to compute the score of an extended hypothesis ğœ“ğ‘¡:
ğ‘(ğœ“ğ‘¡, ğ‘¦ğ‘¡+1|â„) = ğ‘(ğœ“ğ‘¡|â„) â‹… ğ‘(ğ‘¦ğ‘¡+1|â„)
25/40
Beam Search
â€¢ Instead of taking the arg max in every, step, keep a list (or beam)
of ğ‘˜-best scoring hypotheses.
â€¢ Hypothesis = partially decoded sentence â†’ score
â€¢ Hypothesis score ğœ“ğ‘¡ = (ğ‘¦1, ğ‘¦2 â€¦ , ğ‘¦ğ‘¡) is the probability of the
decoded sentence prefix up to ğ‘¡-th word.
ğ‘(ğ‘¦1, â€¦ , ğ‘¦ğ‘¡|â„) = ğ‘(ğ‘¦1|â„) â‹… â‹¯ â‹… ğ‘(ğ‘¦ğ‘¡|ğ‘¦1, â€¦ , ğ‘¦ğ‘¡âˆ’1|â„)
â€¢ Rule to compute the score of an extended hypothesis ğœ“ğ‘¡:
ğ‘(ğœ“ğ‘¡, ğ‘¦ğ‘¡+1|â„) = ğ‘(ğœ“ğ‘¡|â„) â‹… ğ‘(ğ‘¦ğ‘¡+1|â„)
â€¢ Prefers shorter hypotheses â†’ normalization necessary.
25/40
Beam Search â€” Algorithm
1. Begin with a single empty hypothesis in the beam.
26/40
Beam Search â€” Algorithm
1. Begin with a single empty hypothesis in the beam.
2. In each time step:
26/40
Beam Search â€” Algorithm
1. Begin with a single empty hypothesis in the beam.
2. In each time step:
2.1 Extend all hypotheses in the beam by ğ‘˜ most probable words (we call these
candidate hypotheses).
26/40
Beam Search â€” Algorithm
1. Begin with a single empty hypothesis in the beam.
2. In each time step:
2.1 Extend all hypotheses in the beam by ğ‘˜ most probable words (we call these
candidate hypotheses).
2.2 Sort the candidate hypotheses by their score.
26/40
Beam Search â€” Algorithm
1. Begin with a single empty hypothesis in the beam.
2. In each time step:
2.1 Extend all hypotheses in the beam by ğ‘˜ most probable words (we call these
candidate hypotheses).
2.2 Sort the candidate hypotheses by their score.
2.3 Put the best ğ‘˜ hypotheses in the new beam.
26/40
Beam Search â€” Algorithm
1. Begin with a single empty hypothesis in the beam.
2. In each time step:
2.1 Extend all hypotheses in the beam by ğ‘˜ most probable words (we call these
candidate hypotheses).
2.2 Sort the candidate hypotheses by their score.
2.3 Put the best ğ‘˜ hypotheses in the new beam.
2.4 If a hypothesis from the beam reaches the end-of-sentence symbol, we
move it to the list of finished hypotheses.
26/40
Beam Search â€” Algorithm
1. Begin with a single empty hypothesis in the beam.
2. In each time step:
2.1 Extend all hypotheses in the beam by ğ‘˜ most probable words (we call these
candidate hypotheses).
2.2 Sort the candidate hypotheses by their score.
2.3 Put the best ğ‘˜ hypotheses in the new beam.
2.4 If a hypothesis from the beam reaches the end-of-sentence symbol, we
move it to the list of finished hypotheses.
3. Finish (1) at the final time step or (2) all ğ‘˜-best hypotheses end
with </s>.
26/40
Beam Search â€” Algorithm
1. Begin with a single empty hypothesis in the beam.
2. In each time step:
2.1 Extend all hypotheses in the beam by ğ‘˜ most probable words (we call these
candidate hypotheses).
2.2 Sort the candidate hypotheses by their score.
2.3 Put the best ğ‘˜ hypotheses in the new beam.
2.4 If a hypothesis from the beam reaches the end-of-sentence symbol, we
move it to the list of finished hypotheses.
3. Finish (1) at the final time step or (2) all ğ‘˜-best hypotheses end
with </s>.
4. Sort the hypotheses by their score and output the best one.
26/40
Attentive
Sequence-to-Sequence
Learning
Main Idea
Vanilla sequence-to-sequence was degrading with sentence length.
Goal of attention:
â€¢ Do not force the network to catch long-distance dependencies.
â€¢ Use decoder state only for:
â€¢ target sentence dependencies (=LM) and
â€¢ a as query for the source word sentence
27/40
Inspiration: Neural Turing Machine
â€¢ general architecture for
learning algorithmic tasks,
finite imitation of Turing
Machine
28/40
Inspiration: Neural Turing Machine
â€¢ general architecture for
learning algorithmic tasks,
finite imitation of Turing
Machine
â€¢ needs to address memory
somehow â€“ either by position
or by content
28/40
Inspiration: Neural Turing Machine
â€¢ general architecture for
learning algorithmic tasks,
finite imitation of Turing
Machine
â€¢ needs to address memory
somehow â€“ either by position
or by content
â€¢ in fact does not work well
â€“ it hardly manages simple algorithmic tasks
28/40
Inspiration: Neural Turing Machine
â€¢ general architecture for
learning algorithmic tasks,
finite imitation of Turing
Machine
â€¢ needs to address memory
somehow â€“ either by position
or by content
â€¢ in fact does not work well
â€“ it hardly manages simple algorithmic tasks
â€¢ content-based addressing â†’ attention 28/40
Attentive Sequence-to-Sequence Learning
Attention Mechanism
Attention Mechanism<s> x 1 x 2 x 3 x 4
~y i ~y i+1
h 1h 0 h 2 h 3 h 4
...
+
Ã—
Î± 0
Ã—
Î± 1
Ã—
Î± 2
Ã—
Î± 3
Ã—
Î± 4
s is i-1 s i+1
+
29/40
Attention Mechanism in Equations (1)
Inputs:
decoder state ğ‘ ğ‘–
encoder states â„ğ‘— = [âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—â„ğ‘—;âƒ–âƒ–âƒ–âƒ–âƒ–âƒ–âƒ–âƒ–âƒ– â„ğ‘—] âˆ€ğ‘– = 1 â€¦ ğ‘‡ğ‘¥
Attention energies:
ğ‘’ğ‘–ğ‘— = ğ‘£âŠ¤
ğ‘ tanh (ğ‘Šğ‘ğ‘ ğ‘–âˆ’1 + ğ‘ˆğ‘â„ğ‘— + ğ‘ğ‘)
Attention distribution:
ğ›¼ğ‘–ğ‘— = exp(ğ‘’ğ‘–ğ‘—)
âˆ‘ğ‘‡ğ‘¥
ğ‘˜=1 exp(ğ‘’ğ‘–ğ‘˜)
Context vector:
ğ‘ğ‘– = âˆ‘ğ‘‡ğ‘¥
ğ‘—=1 ğ›¼ğ‘–ğ‘—â„ğ‘—
30/40
Attention Mechanism in Equations (2)
Decoder state:
ğ‘ ğ‘– = tanh(ğ‘ˆğ‘‘ğ‘ ğ‘–âˆ’1 + ğ‘Šğ‘‘ğ¸ğ‘‘Ì‚ ğ‘¦ğ‘–âˆ’1 + ğ¶ğ‘ğ‘– + ğ‘ğ‘‘)
Output projection:
ğ‘¡ğ‘– = tanh (ğ‘ˆğ‘œğ‘ ğ‘– + ğ‘Šğ‘œğ¸ğ‘‘Ì‚ ğ‘¦ğ‘–âˆ’1 + ğ¶ğ‘œğ‘ğ‘– + ğ‘ğ‘œ)
â€¦context vector is mixed with the hidden state
Output distribution:
ğ‘ (ğ‘¦ğ‘– = ğ‘˜ |ğ‘ ğ‘–, ğ‘¦ğ‘–âˆ’1, ğ‘ğ‘–) âˆ exp (ğ‘Šğ‘œğ‘¡ğ‘–)ğ‘˜ + ğ‘ğ‘˜
31/40
Attention Visualization
32/40
Attentive Sequence-to-Sequence Learning
Attention vs. Alignment
Attention vs. Alignment
Differences between attention model and word alignment used for
phrase table generation:
33/40
Attention vs. Alignment
Differences between attention model and word alignment used for
phrase table generation:
Attention (NMT) Alignment (SMT)
33/40
Attention vs. Alignment
Differences between attention model and word alignment used for
phrase table generation:
Attention (NMT) Alignment (SMT)
Probabilistic Discrete
33/40
Attention vs. Alignment
Differences between attention model and word alignment used for
phrase table generation:
Attention (NMT) Alignment (SMT)
Probabilistic Discrete
Declarative Imperative
33/40
Attention vs. Alignment
Differences between attention model and word alignment used for
phrase table generation:
Attention (NMT) Alignment (SMT)
Probabilistic Discrete
Declarative Imperative
LM generates LM discriminates
33/40
Attention vs. Alignment
Differences between attention model and word alignment used for
phrase table generation:
Attention (NMT) Alignment (SMT)
Probabilistic Discrete
Declarative Imperative
LM generates LM discriminates
Learnt with translation Prerequisite
33/40
Attention Off by Onedas
VerhÂ¨altnis
zwischen
Obama
und
Netanyahu
ist
seit
Jahren
gespannt
.
the
relationship
between
Obama
and
Netanyahu
has
been
stretched
for
years
. 11
47
81
72
87
93
95
38
21
17
16
14
38
19
33
90
32
26
54
77
12
17
â€¢ Attention can appear on the neighbouring token.
Philipp Koehn and Rebecca Knowles (2017). Six Challenges for Neural Machine Translation. NMT workshop.
34/40
Attending to Two at Once
â€¢ To benefit from PBMT, append its output to NMT input.
â€¢ Standard attentional model will learns to follow both.
Jan Niehues, Eunah Cho, Thanh-Le Ha, and Alex Waibel. 2016. Pre-translation for neural machine translation.
35/40
Image Captioning
Attention over CNN for image classification:
Source: Xu, Kelvin, et al. â€Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.â€ ICML. Vol. 14. 2015.
36/40
Encoder-Decoder vs.
Attentive Models
Sutskever+ (2014) Ã— Bahdanau+ (2014)
Two key papers on NMT in 2014:
â€¢ Bahdanau et al. (2015) Attention model,
â€¢ Sutskever et al. (2014) Seq2seq impressive empirical results:
â€¢ Made researchers believe NMT is the way to go.
â€¢ (Used reversed input.)
Evaluation on WMT14 EN â†’ FR test set:
Model BLEU score
vanilla SMT 33.0
tuned SMT 37.0
Sutskever et al.: reversed 30.6
â€“â€â€“: ensemble + beam search 34.8
â€“â€â€“: vanilla SMT rescoring 36.5
Bahdanauâ€™s attention 28.5
37/40
Sutskever+ (2014) Ã— Bahdanau+ (2014)
Two key papers on NMT in 2014:
â€¢ Bahdanau et al. (2015) Attention model,
â€¢ Sutskever et al. (2014) Seq2seq impressive empirical results:
â€¢ Made researchers believe NMT is the way to go.
â€¢ (Used reversed input.)
Evaluation on WMT14 EN â†’ FR test set:
Model BLEU score
vanilla SMT 33.0
tuned SMT 37.0
Sutskever et al.: reversed 30.6
â€“â€â€“: ensemble + beam search 34.8
â€“â€â€“: vanilla SMT rescoring 36.5
Bahdanauâ€™s attention 28.5 â† Why worse?
37/40
Sutskever+ (2014) Ã— Bahdanau+ (2014)
Sutskever et al. Bahdanau et al.
vocabulary 160k enc, 80k dec 30k both
encoder 4Ã— LSTM, 1,000 units bidi GRU, 2,000
decoder 4Ã— LSTM, 1,000 units GRU, 1,000 units
word embeddings 1,000 dimensions 620 dimensions
training time 7.5 epochs 5 epochs
38/40
Sutskever+ (2014) Ã— Bahdanau+ (2014)
Sutskever et al. Bahdanau et al.
vocabulary 160k enc, 80k dec 30k both
encoder 4Ã— LSTM, 1,000 units bidi GRU, 2,000
decoder 4Ã— LSTM, 1,000 units GRU, 1,000 units
word embeddings 1,000 dimensions 620 dimensions
training time 7.5 epochs 5 epochs
Comparison with Bahdanauâ€™s model size:
method BLEU score
encoder-decoder 13.9
attention model 28.5
38/40
Summary
We discussed:
â€¢ Basic building blocks of NN for NMT.
â€¢ Fully-connected, RNN, LSTM and GRU.
â€¢ Output softmax.
â€¢ Neural LM.
â€¢ Sequence-to-sequence (two RNNs attached).
â€¢ Architecture.
â€¢ Training.
â€¢ Decoding (Greedy vs. Beam)
â€¢ Attention (decoder attends to a mix on encoder states).
39/40
References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align
and translate. In Proceedings of ICLR.
Junyoung Chung, Ã‡aglar GÃ¼lÃ§ehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated recurrent
neural networks on sequence modeling. CoRR, abs/1412.3555.
JindÅ™ich Helcl and OndÅ™ej Bojar. 2017. Deep Learning in MT / NMT. Tutorial at RANLP 2017, August.
Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long short-term memory. Neural Comput., 9(8):1735â€“1780,
November.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector
space. CoRR, abs/1301.3781.
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword
units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pages 1715â€“1725, Berlin, Germany, August. Association for Computational Linguistics.
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to Sequence Learning with Neural Networks. In
Advances in neural information processing systems, pages 3104â€“3112.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun,
Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser,
Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang,
Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean.
2016. Googleâ€™s neural machine translation system: Bridging the gap between human and machine translation. CoRR,
abs/1609.08144.
40/40


Alignment
OndÅ™ej Bojar
March 19, 2020
NPFL087 Statistical Machine Translation
Charles University
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics unless otherwise stated
Outline
â€¢ CzEng (http://ufal.mff.cuni.cz/czeng)
â€¢ Sources of (Czech-English) Parallel Texts.
â€¢ Licensing Issues.
â€¢ Impact of Data Type on MT Quality Gain.
â€¢ Mining the Web.
â€¢ Document Alignment.
â€¢ Sentence Alignment.
â€¢ Word Alignment.
â€¢ IBM Model 1 and the Expectation-Maximization Loop.
â€¢ Problems of Word Alignment.
â€¢ Tectogrammatical Alignment.
1/44
Overview of Phrase-Based MT
This time around = NynÃ­
they â€™re moving = zareagovaly
even = dokonce jeÅ¡tÄ›
â€¦ = â€¦
This time around, they â€™re moving = NynÃ­ zareagovaly
even faster = dokonce jeÅ¡tÄ› rychleji
â€¦ = â€¦
1. Given parallel word-aligned corpus,
2. Extract phrases consistent with word
alignment,
3. Translate by replacing phrases.
â€¦but how to do 1? 2/44
Data Acquisition
Sources of Texts in CzEng 0.7
Legal texts:
â€¢ Acquis Communautaire Parallel Corpus
â€¢ The European Constitution proposal from the OPUS corpus
â€¢ samples from the Official Journal of the European Union
Stories and Commentaries:
â€¢ Readersâ€™ Digest stories
â€¢ e-books: Project Gutenberg and Palmknihy.cz and a subset of the KaÄenka
parallel corpus
â€¢ articles from Project Syndicate
User-supplied data: â€¦not always complete sentences
â€¢ Czech localization of KDE and GNOME open-source projects
â€¢ user-contributed translations from the Navajo project
3/44
Texts in CzEng 0.7 â€“ Data Sizes
Sentences Tokens
Acquis Communautaire 64.1% 69.0%
Readersâ€™ Digest 8.6% 8.6%
Project Syndicate 6.5% 8.9%
KDE Messages 6.2% 1.9%
GNOME Messages 5.7% 1.9%
KaÄenka 4.2% 4.9%
Navajo User Translations 2.3% 2.1%
E-Books 1.2% 1.6%
European Constitution 0.8% 0.7%
Samples from European Journal 0.4% 0.5%
Total 1.4 mil. 21 mil.
Community-supplied data in bold.
4/44
Community-Supplied Data (1/2)
The Navajo Project
â€¢ Anonymous contributors correct MT output of Wikipedia texts.
â€¢ About 2,000 segments used to be generated each month.
â€¢ Manual evaluation of 1,000 randomly selected segments:
Translation Quality Proportion in the Sample
precise, flawless 69.0%
not translated 6.8%
incomplete 6.6%
imprecise 5.8%
precise, almost flawless 4.5%
machine-generated 4.4%
vandalism 2.7%
other 0.2%
5/44
Community-Supplied Data (2/2)
KDE and GNOME Localizations
â€¢ Two major open-source software projects,
â€¢ Contributors not anonymous â‡’ the quality considerably higher
(almost professional)
â€¢ Only rarely full sentences, mostly short system messages and user
interface elements e.g. â€œOKâ€, â€œYesâ€ or â€œDelete fileâ€
6/44
Licensing Issues
â€¢ Much more data are available on the Internet,
â€¢ Only a fraction labelled for reuse.
Tokens Available
Source of Texts and Translation cs en cs en
Community Transl. of Proprietary Texts 19.5M 25.3M 37.8% 41.1%
Professional 21.3M 23.9M 41.2% 38.9%
Proprietary 9.6M 10.9M 18.6% 17.7%
Community 1.2M 1.4M 2.4% 2.3%
Total 51.6M 61.5M 100.0% 100.0%
CzEng 0.7 â‰ˆ Professional + Community sources; in bold
7/44
Enâ†’Cs Data in 2008P
X
D
C
Training Data Composition
In-domain
Professional Translation
Community-supplied
with proper copyright
Community-supplied,
copyright unclear Out-of-domain
Professional
Translation
8/44
OOV and PBMT Quality In/Out of Domain1
2
3
4
5
6
7
5 6 7 8 9 10 11 12 13 14
OOV Rate
BLEU
Proprietary
comm, no copy: X
in D, prof.
CC
domain-D
out-of-domain-D
9/44
Community Data Out-of-Domain1
2
3
4
5
6
7
5 6 7 8 9 10 11 12 13 14
OOV Rate
BLEU
Proprietary
comm, no copy: X
in D, prof.
CC
domain-D
out-of-domain-D
C
10/44
Community Data Out-of-Domain1
2
3
4
5
6
7
5 6 7 8 9 10 11 12 13 14
OOV Rate
BLEU
Proprietary
comm, no copy: X
in D, prof.
CC
domain-D
out-of-domain-D
C
CX
11/44
Professional Out-of-Domain1
2
3
4
5
6
7
5 6 7 8 9 10 11 12 13 14
OOV Rate
BLEU
Proprietary
comm, no copy: X
in D, prof.
CC
domain-D
out-of-domain-D
C
CX
P
12/44
Everything Out-of-Domain1
2
3
4
5
6
7
5 6 7 8 9 10 11 12 13 14
OOV Rate
BLEU
Proprietary
comm, no copy: X
in D, prof.
CC
domain-D
out-of-domain-D
C
CX
P
CPX
13/44
Similar Volume of in-Domain: Much Better1
2
3
4
5
6
7
5 6 7 8 9 10 11 12 13 14
OOV Rate
BLEU
Proprietary
comm, no copy: X
in D, prof.
CC
domain-D
out-of-domain-D
C
CX
P
CPX
D
14/44
Additional Data Improve Coverage1
2
3
4
5
6
7
5 6 7 8 9 10 11 12 13 14
OOV Rate
BLEU
Proprietary
comm, no copy: X
in D, prof.
CC
domain-D
out-of-domain-D
C
CX
P
CPX
D
DP
15/44
But Out-of-Domain Can Decrease Quality1
2
3
4
5
6
7
5 6 7 8 9 10 11 12 13 14
OOV Rate
BLEU
Proprietary
comm, no copy: X
in D, prof.
CC
domain-D
out-of-domain-D
C
CX
P
CPX
D
DPDCPX
16/44
Applying Out of Domain? Much Worse.1
2
3
4
5
6
7
5 6 7 8 9 10 11 12 13 14
OOV Rate
BLEU
Proprietary
comm, no copy: X
in D, prof.
CC
domain-D
out-of-domain-D
C
CX
P
CPX
D
DPDCPX
C
D
17/44
More Data â†’ Better Coverage1
2
3
4
5
6
7
5 6 7 8 9 10 11 12 13 14
OOV Rate
BLEU
Proprietary
comm, no copy: X
in D, prof.
CC
domain-D
out-of-domain-D
C
CX
P
CPX
D
DPDCPX
C
D
DP
CX
18/44
â€¦But Not Much Better Quality1
2
3
4
5
6
7
5 6 7 8 9 10 11 12 13 14
OOV Rate
BLEU
Proprietary
comm, no copy: X
in D, prof.
CC
domain-D
out-of-domain-D
C
CX
P
CPX
D
DPDCPX
C
D
DP
CX
DCPX
19/44
CzEng Releases 2006â€“2020
â€¢ Reached 180M million sentence pairs:
â€¢ 0.6 cs / 0.7 en gigawords of genuine parallel text (61M sentpairs)
â€¢ 2.0 cs / 2.3 en gigawords of synthetic text (127M sentpairs)
Ver. S. Pairs Main Focus Details in
0.5 0.9M Sentence alignment, common format Bojar and Å½abokrtskÃ½ (2006)
0.7 1.0M Used in WMT06 and WMT07 Bojar et al. (2008)
0.9 8.0M Automatic annotation up to t-layer Bojar and Å½abokrtskÃ½ (2009)
â€“ â€“ Sentence-level filtering Bojar et al. (2010)
1.0 15.0M Improving monolingual annotation
through parallel data
Bojar et al. (2012)
1.6 62.5M Processing tools dockered Bojar et al. (2016)
1.7 57.1M Block-level filtering â€“
2.0 188.0M Filtering + Synthetic data â€“
20/44
Methods
Mining the Web
Goal: Given two language names, find parallel texts.
â€¢ HervÃ© Saint-Amandâ€™s masterâ€™s thesis (SaarbrÃ¼cken).
â€¢ Train language identification on Wikipedia.
â€¢ Search for pages in English containing the word Äesky.
â€¢ Bitextor: EsplÃ -Gomis and Forcada (2010)
â€¢ PANACEA tools (http://myexperiment.elda.org/workflows/7)
â€¢ Studentsâ€™ project ParaSite: proof of concept, fixes needed.
Quasi-comparable sources (incl. Wikipedia):
â€¢ Texts on the same topic but written independently.
â€¢ Can hope to find parallel sentences but no longer segments.
â€¢ BUCC workshops 2008â€“2020: https://comparable.limsi.fr/bucc2020/
â€¢ â€œLightly supervised trainingâ€ (Schwenk, 2008) = basis of unsupervised MT. 21/44
Document Alignment Attempted Many Times
Goal: Given bag of texts in two languages, find pairs.
â€¢ A project at this very seminar at FJFI: (Jahoda et al., 2007)
â€¢ A project at MFF: (KlempovÃ¡ et al., 2009)
â€¢ Evaluation suggested that the first step is tricky: finding source URLs.
â€¢ VÃ¡clav NovÃ¡k (ÃšFAL, âˆ¼2009): aligning subtitles.
â€¢ Proper minimum pairing algorithm.
â€¢ Not generic enough: focus on named entities at the beg. and end only.
â€¢ ParaSite: probably good, re-evaluation would be useful.
â€¢ Problem: Based on libraries with conflicting licenses (GPL 2.0 vs 3.0).
â€¢ Parallel Paragraphs from CommonCrawl (KÃºdela et al., 2017)
â€¢ Recall 63%, precision 94% when re-aligning shuffled CzEng.
â€¢ 149TB of CommonCrawl â‡ 115k en-cs sentpairs from 2k webdomains.
â€¢ Targetted re-crawl would be highly desirable (project suggestion).
â€¢ paracrawl.eu large but noisy. Aligns documents, not paragraphs. 22/44
Sentence Alignment
Goal: Given a text in two languages, align sentences.
23/44
From Aligned Documents
24/44
We Want Sentence Alignment
25/44
Sentence Alignment
Goal: Given a text in two languages, align sentences.
Assume: Sentences hardly ever reordered.
â€¢ Classical algorithm: Gale and Church (1993).
â€¢ Based on similar character length of aligned sentences, no words examined.
â€¢ Dynamic-programming search for the best alignment.
â€¢ Allows 0 to 2 sentences in a group: 0-1, 1-0, 1-1, 2-1, 1-2, 2-2.
â€¢ Several algorithms for English-Czech evaluated by Rosen (2005).
â€¢ Nearly perfect alignment possible by a combination of aligners.
â€¢ The â€œstandard toolâ€: Hunalign (Varga et al., 2005).
â€¢ Another option: Gargantua (Braune and Fraser, 2010).
Illustration: MT Talk #7 (https://youtu.be/_4lnyoC3mtQ)
26/44
Word Alignment
Goal: Given a sentence in two languages, align words (tokens).
State of the art: GIZA++ (Och and Ney, 2000):
â€¢ Unsupervised, only sentence-parallel texts needed.
â€¢ Word alignments formally restricted to a function:
src token â†¦ tgt token or NULL
â€¢ A cascade of models refining the probability distribution:
â€¢ IBM1: only lexical probabilities: ğ‘ƒ (koÄka = cat)
â€¢ IBM2: absolute reordering added (not used in practice now)
â€¢ IBM3: adds fertility: 1 word generates several others
â€¢ IBM4/HMM: to account for relative reordering
â€¢ Only many-to-one links created â‡’ used twice, in both directions. 27/44
IBM Model 1
Lexical probabilities:
â€¢ Disregard the position of words in sentences.
â€¢ Estimated using Expectation-Maximization Loop.
See the slides by Philipp Koehn for:
â€¢ Formulas of both expectation and maximization step.
â€¢ The trick in expectation step, swapping sum and product by
rearranging the sum.
â€¢ Pseudocode.
Illustration: MT Talk #8 (https://youtu.be/mqyMDLu5JPw)
28/44
The Trick Illustrated
Sum of pairs:
Can be rearranged: = =
29/44
EM Loop in IBM1 Illustration from Bojar (2012)white
white
white
white
house
house
house
house
bÃ­lÃ½
dÅ¯m
white
white
white
white
dog
dog
dog
dog
bÃ­lÃ½
pes
black
black
black
black
dog
dog
dog
dog
ÄernÃ½
pes
bÃ­lÃ½
ÄernÃ½
dÅ¯m
pes
Iteration 0
black
dog
house
white
1
black
dog
house
white
2
black
dog
house
white
3
black
dog
house
white
4
black
dog
house
white
p( )â†’â†“|
Î£
Î£
Î£
Î£
Î£
Î£
Î£
Î£
Î£
Î£
Î£
Î£
Î£
Î£
Î£
Î£
30/44
Symmetrization
â€œSymmetrizationâ€ of two GIZA++ runs:
â€¢ intersection: high precision, too low recall.
â€¢ popular: heuristical (something between intersection and union).
â€¢ minimum-weight edge cover (Matusov et al., 2004). 31/44
Popular Symmetrization Heuristic
32/44
Troubles with Word Alignment
â€¢ Humans have troubles aligning word for word.
â€¢ Mismatch in alignments points 9â€“18%. (Bojar and ProkopovÃ¡, 2006)
Top Problematic Words Top Problematic Parts of Speech
English Czech English Czech
361 to 319 , 679 IN 1348 N
259 the 271 se 519 DT 1283 V
159 of 146 v 510 NN 661 R
143 a 112 na 386 PRP 505 P
124 , 74 o 361 TO 448 Z
107 be 61 Å¾e 327 VB 398 A
99 it 55 . 310 JJ 280 D
95 that 47 a 245 RB 192 J
33/44
Limits of Automatic W.A.
Baseline Improved
Humans GIZA++ en cs en cs
Problems Problems 14.3 15.5 14.3 15.5
Problems OK 0.1 0.1 0.2 0.1
OK Problems 38.6 35.7 25.2 25.0
OK OK 46.9 48.7 60.4 59.4
Percentage of English (en) and Czech (cs) tokens where the alignment was difficult for
humans and/or for GIZA++. (Humans against each other, GIZA++ against merged
humans.)
â€¢ Where GIZA++ had problems, humans often disagreed, too.
â€¢ Improving automatic alignment keeps the problematic part intact.
34/44
Partial Fix: â€œPossibleâ€ Alignments
Chinese-English from
DeNero and Klein (2010). 35/44
A Czech-English Example
NemyslÃ­m o o o * - - - - - - - -
, - - - - - - - o - - - -
Å¾e - - - - - - - o - - - -
by - - - - - - - o - - - -
se - - - - - - - o - - - -
to - - - - - - - - * - - -
jejich - - - - * - - - - - - -
zÃ¡kaznÃ­kÅ¯m - - - - - * - - - - - -
moc - - - - - - - - - * * -
lÃ­bilo - - - - - - - * - - - -
. - - - - - - - - - - - *
I do think would very
n't their like much
customers .
it
â€¢ Two papers
independently published
the same work and on
the same dataset.
â€¢ Kruijff-KorbayovÃ¡ et
al. (2006)
â€¢ Bojar and ProkopovÃ¡
(2006)
â€¢ The both defined
essentially the same
rules.
36/44
T-Layer to the Rescue
â€¢ Only content-bearing words have a node.
â€¢ Auxiliary words hidden, dropped pronouns added..
.
#
SENT
jÃ¡
ACT
myslit PROC
PRED
ten
PAT
jeho
APP
zÃ¡kaznÃ­k
ACT
moc
EXT
lÃ­bit_se PROC
EFF.
.
.
#11
SENT
I
ACT
not
RHEM
think CPL
PRED
he
APP
customer
ACT
like CPL
PAT
it
PAT
much
MANN
very
EXT NIL
(jÃ¡) NemyslÃ­m , Å¾e by se to jejich I do nâ€™t think their
zÃ¡kaznÃ­kÅ¯m moc lÃ­bilo . customers would like it very much .
37/44
Tectogrammatical Alignment
â€¢ MareÄek et al. (2008) align t-nodes, not words.
â‡’ Auxiliary words do not clutter the task.
â€¢ Improves human agreement from 91% to 94.7%.
â€¢ Application to phrase-based MT: (MareÄek, 2009)
â€¢ Improved alignment error rate on content words.
â€¢ Minor improvements in BLEU when combined with GIZA++.
â€¢ Main use: Extraction of t-lemma dictionaries for e.g. TectoMT.
Main disadvantage:
â€¢ Language-dependent.
â€¢ Heavy use of tools (tagging, parsing, deep parsing).
38/44
Related: Fraser and Marcu (2007)
â€¢ A generative story called â€œLEAFâ€ divides:
â€¢ Source words into classes: head, non-head, deleted.
â€¢ Target words into classes: head, non-head, spurious.
â€¢ Heads connected across languages, non-heads within languages.
â€¢ Probabilities in the generative story learnt unsupervised:
â€¢ Starting from GIZA++ outputs.
â€¢ Greedy local updates of alignments to increase the likelihood of the data.
Project suggestions: (1) Revive LEAF, (2) Your own NN version of LEAF. 39/44
Using Alignment in PBMT
Phrase extraction based on word alignments is wrong:
â€¢ From statistical point of view:
â€¢ No link to the decoding, i.e. the use of the phrases in MT.
â€¢ Wuebker et al. (2010) run â€œforcedâ€ or â€œconstraintâ€ decoding on the
training data to obtain phrasal alignments.
â€¢ The overfitting to long phrases is avoided by â€œleaving-one-outâ€ (Ney et al., 1995).
â€¢ From linguistic point of view:
â€¢ Fraser and Marcu (2007) allow for M-to-N non-consecutive translation
units.
â€¢ DeNero and Klein (2010) train on manual word alignments and handle
â€œpossibleâ€ links specifically.
40/44
Better Translation â‡ Uglier Ali. (1)
The better (more fluent) translation, the harder to align:
to o * - - - - - - - - - -
get - - * - - - - - - - - -
in - - - - - - - @ O O O -
shape - - - - - - - O O O @ -
for - - - * - - - - - - - -
the - - - - - - o - - - - -
1990s - - - - * * * - - - - -
. - - - - - - - - - - - *
, aby do . let co formÄ›
vstoupila v nejlepÅ¡Ã­
90 .
41/44
Better Translation â‡ Uglier Ali. (2)
T-layer to no rescue:.
.
.
#
SENT
teÄ
TWHEN
zdÃ¡t_se PROC
PRED
&Gen;
ACT
tento
RSTR
trasa
ACT
zaÄÃ­t PROC
PAT
fungovat PROC
PAT
&Cor;
ACT
aÅ¾
RHEM
leden
TWHEN.
.
.
#4
SENT
&Gen;
ACT
now
TWHEN NIL
that
RSTR
route
PAT
not
RHEM
expect CPL
PRED
&Cor;
ACT
begin CPL
EFF
Jan
TTILL
TeÄ se zdÃ¡ , Å¾e tyto trasy Now , those routes
zaÄnou fungovat aÅ¾ v lednu . are nâ€™t expected to begin until Jan . 42/44
Summary
â€¢ Paralel data are vital for MT.
The more and better, the better.
â€¢ Several projects for document alignment.
Project suggestion: Targeted re-crawl based on KÃºdela et al. (2017).
â€¢ Sentence alignment â€œsolvedâ€.
â€¢ Word alignment ill-defined but used to be very important.
Plus all the funny heuristicsâ€¦
â€¢ Beyond word alignment:
â€¢ Phrase alignment never got wide-spread; too tied to PBMT anyway.
â€¢ T-Alignment costly (T-layer needed).
â€¢ Project suggestion: NN LEAF.
43/44
References
OndÅ™ej Bojar and Magdalena ProkopovÃ¡. 2006. Czech-English Word Alignment. In Proceedings of the Fifth
International Conference on Language Resources and Evaluation (LREC 2006), pages 1236â€“1239. ELRA, May.
OndÅ™ej Bojar and ZdenÄ›k Å½abokrtskÃ½. 2006. CzEng: Czech-English Parallel Corpus, Release version 0.5. Prague
Bulletin of Mathematical Linguistics, 86:59â€“62.
OndÅ™ej Bojar and ZdenÄ›k Å½abokrtskÃ½. 2009. CzEng 0.9: Large Parallel Treebank with Rich Annotation. Prague Bulletin
of Mathematical Linguistics, 92:63â€“83.
OndÅ™ej Bojar, Miroslav JanÃ­Äek, ZdenÄ›k Å½abokrtskÃ½, Pavel ÄŒeÅ¡ka, and Peter BeÅˆa. 2008. CzEng 0.7: Parallel Corpus
with Community-Supplied Translations. In Proceedings of the Sixth International Language Resources and Evaluation
(LRECâ€™08), Marrakech, Morocco, May. ELRA.
OndÅ™ej Bojar, Adam LiÅ¡ka, and ZdenÄ›k Å½abokrtskÃ½. 2010. Evaluating Utility of Data Sources in a Large Parallel
Czech-English Corpus CzEng 0.9. In Proceedings of the Seventh International Language Resources and Evaluation
(LRECâ€™10), pages 447â€“452, Valletta, Malta, May. ELRA, European Language Resources Association.
OndÅ™ej Bojar, ZdenÄ›k Å½abokrtskÃ½, OndÅ™ej DuÅ¡ek, Petra GaluÅ¡ÄÃ¡kovÃ¡, Martin MajliÅ¡, David MareÄek, JiÅ™Ã­ MarÅ¡Ã­k, Michal
NovÃ¡k, Martin Popel, and AleÅ¡ Tamchyna. 2012. The Joy of Parallelism with CzEng 1.0. In Proceedings of the Eighth
International Language Resources and Evaluation Conference (LRECâ€™12), pages 3921â€“3928, Istanbul, Turkey, May.
ELRA, European Language Resources Association.
OndÅ™ej Bojar, OndÅ™ej DuÅ¡ek, Tom Kocmi, JindÅ™ich LibovickÃ½, Michal NovÃ¡k, Martin Popel, Roman Sudarikov, and
DuÅ¡an VariÅ¡. 2016. CzEng 1.6: Enlarged Czech-English Parallel Corpus with Processing Tools Dockered. In Petr Sojka,
AleÅ¡ HorÃ¡k, Ivan KopeÄek, and Karel Pala, editors,
Text, Speech, and Dialogue: 19th International Conference, TSD 2016, number 9924 in Lecture Notes in Computer
Science, pages 231â€“238, Cham / Heidelberg / New York / Dordrecht / London. Masaryk University, Springer
International Publishing. 44/44


Phrase-Based MT
OndÅ™ej Bojar
March 26, 2020
NPFL087 Statistical Machine Translation
Charles University
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics unless otherwise stated
Outline
â€¢ PBMT Overview.
â€¢ Reminder: Log-linear model.
â€¢ PBMT Model.
â€¢ Features Used.
â€¢ Traditional PBMT â€œPipelineâ€
â€¢ Translating with PBMT (Decoding)
â€¢ Translation Options and Stack-Based Beam Search
â€¢ MT is NP-Hard.
â€¢ Pruning, Future Cost Estimation.
â€¢ Local and Non-Local Features.
â€¢ Minimum Error-Rate Training.
â€¢ Moses as the implementation.
1/26
Overview: Phrase-Based MT
This time around = NynÃ­
they â€™re moving = zareagovaly
even = dokonce jeÅ¡tÄ›
â€¦ = â€¦
This time around, they â€™re moving = NynÃ­ zareagovaly
even faster = dokonce jeÅ¡tÄ› rychleji
â€¦ = â€¦
Phrase-based MT: choose such segmentation
of input string and such phrase â€œreplacementsâ€
to make the output sequence â€œcoherentâ€
(3-grams most probable).
2/26
Reminder: Log-Linear Model
â€¢ ğ‘(ğ‘’ğ¼
1|ğ‘“ğ½
1 ) is modelled as a weighted combination of models, called
â€œfeature functionsâ€: â„1(â‹…, â‹…) â€¦ â„ğ‘€ (â‹…, â‹…)
ğ‘(ğ‘’ğ¼
1|ğ‘“ğ½
1 ) = exp(âˆ‘ğ‘€
ğ‘š=1 ğœ†ğ‘šâ„ğ‘š(ğ‘’ğ¼
1, ğ‘“ğ½
1 ))
âˆ‘ğ‘’â€²ğ¼â€²
1
exp(âˆ‘ğ‘€
ğ‘š=1 ğœ†ğ‘šâ„ğ‘š(ğ‘’â€²ğ¼â€²
1 , ğ‘“ğ½
1 )) (1)
â€¢ The constant denominator not needed in maximization:Ì‚
ğ‘’Ì‚
ğ¼
1 = argmaxğ¼,ğ‘’ğ¼
1
exp(âˆ‘ğ‘€
ğ‘š=1 ğœ†ğ‘šâ„ğ‘š(ğ‘’ğ¼
1, ğ‘“ğ½
1 ))
âˆ‘ğ‘’â€²ğ¼â€²
1
exp(âˆ‘ğ‘€
ğ‘š=1 ğœ†ğ‘šâ„ğ‘š(ğ‘’â€²ğ¼â€²
1 , ğ‘“ğ½
1 ))
= argmaxğ¼,ğ‘’ğ¼
1
exp(âˆ‘ğ‘€
ğ‘š=1 ğœ†ğ‘šâ„ğ‘š(ğ‘’ğ¼
1, ğ‘“ğ½
1 ))
(2)
3/26
Phrase-Based Translation Model
â€¢ Captures the basic assumption of phrase-based MT:
1. Segment source sentence ğ‘“ğ½
1 into ğ¾ phrasesÌƒ ğ‘“1 â€¦Ìƒ ğ‘“ğ¾.
2. Translate each phrase independently:Ìƒ ğ‘“ğ‘˜ â†’Ìƒ ğ‘’ğ‘˜.
3. Concatenate translated phrases (with possible reordering ğ‘…):Ìƒ
ğ‘’ğ‘…(1) â€¦Ìƒ ğ‘’ğ‘…(ğ¾)
â€¢ In theory, the segmentation ğ‘ ğ¾
1 is a hidden variable in the maximization, we should be
summing over all segmentations: (Note the three args in â„ğ‘š(â‹…, â‹…, â‹…) now.)Ì‚
ğ‘’Ì‚
ğ¼
1 = argmaxğ¼,ğ‘’ğ¼
1
âˆ‘ğ‘ ğ¾
1
exp(âˆ‘ğ‘€
ğ‘š=1 ğœ†ğ‘šâ„ğ‘š(ğ‘’ğ¼
1, ğ‘“ğ½
1 , ğ‘ ğ¾
1 )) (3)
â€¢ In practice, the sum is approximated with a max (the biggest element only):Ì‚
ğ‘’Ì‚
ğ¼
1 = argmaxğ¼,ğ‘’ğ¼
1
maxğ‘ ğ¾
1 exp(âˆ‘ğ‘€
ğ‘š=1 ğœ†ğ‘šâ„ğ‘š(ğ‘’ğ¼
1, ğ‘“ğ½
1 , ğ‘ ğ¾
1 )) (4)
4/26
Commonly Used Features of PBMT
â€¢ Phrase translation probability:
â„Phr(ğ‘“ğ½
1 , ğ‘’ğ¼
1, ğ‘ ğ¾
1 ) = log âˆğ¾
ğ‘˜=1 ğ‘(Ìƒğ‘“ğ‘˜|Ìƒğ‘’ğ‘˜) where ğ‘(Ìƒğ‘“ğ‘˜|Ìƒğ‘’ğ‘˜) = count(Ìƒğ‘“,Ìƒ ğ‘’)
count(Ìƒğ‘’)
â‡’ Are all used unitsÌƒ ğ‘“ â†”Ìƒ ğ‘’ likely translations?
â€¢ Word count/penalty: â„wp(ğ‘’ğ¼
1, â‹…, â‹…) = ğ¼
â‡’ Do we prefer longer or shorter output?
â€¢ Phrase count/penalty: â„pp(â‹…, â‹…, ğ‘ ğ¾
1 ) = ğ¾
â‡’ Do we prefer translation in more or fewer less-dependent bits?
â€¢ Reordering model: different basic strategies (Lopez, 2009)
â‡’ Which source spans can provide continuation at a moment?
â€¢ ğ‘›-gram LM: â„LM(â‹…, ğ‘’ğ¼
1, â‹…) = log âˆğ¼
ğ‘–=1 ğ‘(ğ‘’ğ‘–|ğ‘’ğ‘–âˆ’1
ğ‘–âˆ’ğ‘›+1)
â‡’ Is output ğ‘›-gram-wise coherent?
5/26
Traditional PBMT â€œPipelineâ€
â€œTraining the Translation Modelâ€
1. Find relevant parallel texts.
2. Align at the level of sentences.
3. Align at the level of words.
4. Extract translation units, with scores (co-oc. stats.).
(Language Model similar, â€œsimpleâ€ words co-oc. stats, no alignment.)
â€œTuningâ€ (â€œMERTâ€) = Actual training in the ML sense
5. Identify TM/LM/other model component weights.
Translation: = Inference in the ML sense
6. Decompose input into known units.
7. Search for best combinations of units. 6/26
Estimating Phrase Translation Probs
The most important feature: phrase-to-phrase translation:
â„Phr(ğ‘“ğ½
1 , ğ‘’ğ¼
1, ğ‘ ğ¾
1 ) = log
ğ¾
âˆ
ğ‘˜=1
ğ‘(Ìƒğ‘“ğ‘˜|Ìƒğ‘’ğ‘˜) (5)
The conditional probability of phraseÌƒ ğ‘“ğ‘˜ given phraseÌƒ ğ‘’ğ‘˜ is estimated from relative
frequencies:
ğ‘(Ìƒğ‘“ğ‘˜|Ìƒğ‘’ğ‘˜) = count(Ìƒğ‘“,Ìƒ ğ‘’)
count(Ìƒğ‘’) (6)
â€¢ count(Ìƒğ‘“,Ìƒ ğ‘’) is the number of co-occurrences of a phrase pair (Ìƒğ‘“,Ìƒ ğ‘’) that are consistent with
the word alignment
â€¢ count(Ìƒğ‘’) is the number of occurrences of the target phraseÌƒ ğ‘’ in the training corpus.
â€¢ â„Phr usually used twice, in both directions: ğ‘(Ìƒğ‘“ğ‘˜|Ìƒğ‘’ğ‘˜) and ğ‘(Ìƒğ‘’ğ‘˜|Ìƒğ‘“ğ‘˜) 7/26
â€œTrainingâ€ = Phrase Extraction
This time around = NynÃ­
they â€™re moving = zareagovaly
even = dokonce jeÅ¡tÄ›
â€¦ = â€¦
This time around, they â€™re moving = NynÃ­ zareagovaly
even faster = dokonce jeÅ¡tÄ› rychleji
â€¦ = â€¦
Extract all phrases (up to max-phrase-len)
consistent with the word alignment.
â€¢ Long and short.
â€¢ Overlapping in all ways.
Score them (Eq. 6) â‡’ Phrase Table 8/26
Phrase Table in Moses
Given parallel training corpus, phrases are extracted and scored:
in europa ||| in europe ||| 0.829007 0.207955 0.801493 0.492402 2.718
europas ||| in europe ||| 0.0251019 0.066211 0.0342506 0.0079563 2.718
in eu ||| in europe ||| 0.018451 0.00100126 0.0319584 0.0196869 2.718
The scores are: (ğœ™(â‹…) = log ğ‘(â‹…))
â€¢ phrase translation probabilities: ğœ™phr(ğ‘“|ğ‘’) and ğœ™phr(ğ‘’|ğ‘“)
â€¢ lexical weighting: ğœ™lex(ğ‘“|ğ‘’) and ğœ™lex(ğ‘’|ğ‘“) (Koehn, 2003)
ğœ™lex(ğ‘“|ğ‘’) = log max
ğ‘âˆˆalignments
of (ğ‘“,ğ‘’)
|ğ‘“|
âˆ
ğ‘–=1
1
|{ğ‘—|(ğ‘–, ğ‘—) âˆˆ ğ‘| âˆ‘
âˆ€(ğ‘–,ğ‘—)âˆˆğ‘
ğ‘(ğ‘“ğ‘–|ğ‘’ğ‘—) (7)
â€¢ phrase penalty (always ğ‘’1 = 2.718)
9/26
Translation with PBMT
Translation with phrase-based model has two main stages:
1. Translation Options Preparation.
â€¢ Search the phrase table for all phrases applicable to the input sentence.
2. Decoding (Main Search).
â€¢ Gradual hypothesis expansion.
â€¢ Output produced left-to-right.
â€¢ Input consumed in any order.
10/26
Stage 1: Translation Optionshe
er geht ja nicht nach hause
it
, it
, he
is
are
goes
go
yes
is
, of course
not
do not
does not
is not
after
to
according to
in
house
home
chamber
at home
not
is not
does not
do not
home
under house
return home
do not
it is
he will be
it goes
he goes
is
are
is after all
does
to
following
not after
not to
not
is not
are not
is not a
11/26
Stage 2: Decoding (Beam Search)er geht ja nicht nach hauseconsult phrase translation table for all input phrases
12/26
Stage 2: Decoding (Beam Search)er geht ja nicht nach hauseinitial hypothesis: no input words covered, no output produced
13/26
Stage 2: Decoding (Beam Search)er geht ja nicht nach hause
arepick any translation option, create new hypothesis
14/26
Stage 2: Decoding (Beam Search)er geht ja nicht nach hause
are
it
hecreate hypotheses for all other translation options
15/26
Stage 2: Decoding (Beam Search)er geht ja nicht nach hause
are
it
he goes
does not
yes
go
to
home
homealso create hypotheses from created partial hypothesis
16/26
Stage 2: Decoding (Beam Search)er geht ja nicht nach hause
are
it
he goes
does not
yes
go
to
home
homebacktrack from highest scoring complete hypothesis
17/26
Interlude: MT is NP-Hard (1/2)
â€¢ Translation options lead to exponentially many hypotheses.
â€¢ Indeed, MT is NP-hard for at least two reasons:
â€¢ Finding the best word ordering.
â€¢ Covering with multi-word units.
â€¢ Remember the NP-hardness proof strategy:
â€¢ Use MT as a black box to solve an NP-complete task.
With a 2-gram language model, find-
ing the best word ordering solves the
Hamilton Circuit or Travelling Sales-
man Problem. (Knight, 1999)
18/26
Interlude: MT is NP-Hard (2/2)
Selecting a set of multi-word
translations to cover the whole
sentence solves Minimum Set Cover
Problem. (Knight, 1999)
The sentence is:
(However, she cooked and left.)
19/26
Fighting the Complexity
See slides 17â€“32 by Barry Haddow.
â€¢ Hypothesis Recombination.
â€¢ Stack-based Pruning.
â€¢ Future Cost Estimation.
20/26
Local and Non-Local FeaturesWord penalty
Peter left for home .
Petr odeÅ¡el domÅ¯ .
Bigram log. prob.
1,0 2,0 1,0
Phrase penalty 1,0 1,0 1,0
Phrase log. prob. 0,0 -0,69 -1,39
Total
4,0
3,0
-2,08
-2,50 -3,61 -0,39 -10,59
Weight
-0,5
-1,0
2,0
1,0
Weighted
-2,0
-3,0
-4,16
-10,59
Total -19,75
â—
-4,02
â—
-0,08
â€¢ Local features decompose along hypothesis construction.
â€¢ Phrase- and word-based features.
â€¢ Non-local features span the boundaries (e.g. LM). 21/26
Weight Optimization: MERT LoopCurrent weights
1 1 1
Word Translation
Language Model
Phrase Translation
Translate input
Hypothesis \ Weight 1 1 1
MluvÃ­me nahoru ! 2 0 2
Nahlas ! 0 2 1 3
Mluv nahlas ! 1 1 2 4
ProsÃ­m mluvte nahlas . 1 2 3
Word Translation
Language Model
Phrase Translation
Weighter internal score
0
2
1
3
Evaluate candidates using an external score.
External score
2 1
2 0 4 0
0 2 1 7 2
1 1 2 7 1
1 2 8 3
Word Translation
Language Model
Phrase Translation
Weighted internal score
External score
Find new
weights for
a better
match of
external and
internal score.
Weights same?
Stop.
Weights differ?
Loop.
Speak up !
0
0 0
3
0
Minimum Error Rate Training (Och, 2003)
22/26
Effects of WeightsdojÃ­t k jinÃ©mu roz
dojÃ­t k jinÃ©mu rozsud
dojÃ­t k jinÃ©mu rozsudku
dojÃ­t k jinÃ©mu rozsudku , | p
dojÃ­t k jinÃ©mu rozsudku , | poro
dojÃ­t k jinÃ©mu rozsudku , | porovn
dojÃ­t k jinÃ©mu rozsudku , | porovnÃ¡vÃ¡
pivovarnÃ­ci , dvojÄata , balÃ­rny , VikingovÃ©
dojÃ­t k jinÃ©mu rozsudku , | ventilace , klimati
( od naÅ¡eho zvlÃ¡Å¡tnÃ­ho zpravodaje ) - | je . . je . .
verdikt | jeÅ¡tÄ› nenÃ­ koneÄnÃ½ , | a soud | bude | proj
rozsudek | jeÅ¡tÄ› nenÃ­ koneÄnÃ½ | , | soud | s | TymoÅ¡en
rozsudek | soudu | , | tak | se | proti | TymoÅ¡enkovÃ© | .
rozsudek | soudu | , | Å¾e | se | proti | TymoÅ¡enkovÃ© | .
rozsudek | soudu | , | Å¾e | se | proti | TymoÅ¡enkovÃ© | .
i | pÅ™esto | , | Å¾e | si | v | TymoÅ¡enkovÃ© | .
i | pÅ™esto | , | Å¾e | si | v | TymoÅ¡enkovÃ© | .
i | pÅ™esto | , | Å¾e | se | proti | TymoÅ¡enkovÃ© | .
na tom | je | , | Å¾e | se | s | TymoÅ¡enkovou | .
na tom | je | , | Å¾e | se | s | TymoÅ¡enkovou | .
na tom | je | , | Å¾e | se | s | TymoÅ¡enkovou | .1
Language Model Score
-1
verdikt | jeÅ¡tÄ› nenÃ­ koneÄnÃ½ , | a soud | ...
verdikt | jeÅ¡tÄ› nenÃ­ | koneÄnÃ½ | , | soud | ...
verdikt | jeÅ¡tÄ› nenÃ­ | koneÄnÃ½ | , | soud | ...
verdikt | jeÅ¡tÄ› nenÃ­ | koneÄnÃ½ | , | soud | ...
verdikt | je | zatÃ­m | poslednÃ­ | ; | soud | ...
verdikt | je | zatÃ­m | poslednÃ­ | ; | soud | ...
verdikt | je | zatÃ­m | poslednÃ­ | ; | soud | ...
jeho | verdikt | je | zatÃ­m | poslednÃ­ | ; | soud | ...
na | verdikt | je | to | jeÅ¡tÄ› | zÃ¡vÄ›reÄnÃ© | ; | ten | ...
na | verdikt | je | to | jeÅ¡tÄ› | zÃ¡vÄ›reÄnÃ© | ; | ten | ...
0
1 Phrase Penalty
â€¢ Higher phrase penalty chops sentence into more segments.
â€¢ Too strong LM weight leads to words dropped.
â€¢ Negative LM weight leads to obscure wordings. 23/26
Moses Decoder
â€¢ http://www.statmt.org/moses
â€¢ Moses Installation Tutorial.
24/26
Phrase-Based MT Summary
Phrase-based MT:
â€¢ is a log-linear model
â€¢ decomposes sentence into contiguous phrases (=MTU)
â€¢ assumes phrases relatively independent of each other
â€¢ search has two parts:
â€¢ lookup of all relevant translation options
â€¢ stack-based beam search, gradually expanding hypotheses
To train a PBMT system:
1. Align words.
2. Extract (and score) phrases consistent with word alignment.
3. Optimize weights (MERT).
Best implementation: Moses Decoder. 25/26
References
Kevin Knight. 1999. Decoding complexity in word-replacement translation models. Comput. Linguist., 25(4):607â€“615.
Philipp Koehn. 2003. Noun Phrase Translation. Ph.D. thesis, University of Southern California.
Adam Lopez. 2009. Translation as weighted deduction. In Proceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009), pages 532â€“540, Athens, Greece, March. Association for Computational Linguistics.
Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proc. of the Association
for Computational Linguistics, Sapporo, Japan, July 6-7.
26/26


Morphology in MT
OndÅ™ej Bojar
April 2, 2020
NPFL087 Statistical Machine Translation
Charles University
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics unless otherwise stated
Outline
â€¢ Problems caused by rich morphology.
â€¢ Morphological richness of Czech.
â€¢ Combinatorial explosion of Czech word forms.
â€¢ Margin for improvement in BLEU.
â€¢ Morphology in PBMT:
â€¢ Factored PBMT.
â€¢ Reverse self-training.
â€¢ Morphology in NMT.
â€¢ Subword units, BPE.
1/38
Morphological Richness (in Czech)
Czech English
Rich morphology â‰¥ 4,000 tags possible 50 used
â‰¥ 2,300 tags seen
Word order free rigid
News Commentary Corpus Czech English
Sentences 55,676
Tokens 1.1M 1.2M
Vocabulary (word forms) 91k 40k
Vocabulary (lemmas) 34k 28k
Czech tagging and lemmatization: HajiÄ and HladkÃ¡ (1998)
English tagging (Ratnaparkhi, 1996) and lemmatization (Minnen et al., 2001). 2/38
Morphological Explosion in Czech
MT chooses output words in a form:
â€¢ Czech nouns and adjs.: 7 cases, 4 genders, 3 numbers, â€¦
â€¢ Czech verbs: gender, number, aspect (im/perfective), â€¦
I saw two green striped cats .
jÃ¡ pila dva zelenÃ½ pruhovanÃ½ koÄky .
pily dvÄ› zelenÃ¡ pruhovanÃ¡ koÄek
â€¦ dvou zelenÃ© pruhovanÃ© koÄkÃ¡m
vidÄ›l dvÄ›ma zelenÃ­ pruhovanÃ­ koÄkÃ¡ch
vidÄ›la dvÄ›mi zelenÃ©ho pruhovanÃ©ho koÄkami
â€¦ zelenÃ½ch pruhovanÃ½ch
uvidÄ›l zelenÃ©mu pruhovanÃ©mu
uvidÄ›la zelenÃ½m pruhovanÃ½m
â€¦ zelenou pruhovanou
vidÄ›l jsem zelenÃ½mi pruhovanÃ½mi
vidÄ›la jsem â€¦ â€¦
3/38
Morphological Explosion Elsewhere
Compounding in German:
â€¢ RindfleischetikettierungsÃ¼berwachungsaufgabenÃ¼bertragungs-
gesetz.
â€œbeef labelling supervision duty assignment lawâ€
Agglutination in Hungarian or Finnish:
istua â€œto sit downâ€ (istun = â€œI sit downâ€)
istahtaa â€œto sit down for a whileâ€
istahdan â€œIâ€™ll sit down for a whileâ€
istahtaisin â€œI would sit down for a whileâ€
istahtaisinko â€œshould I sit down for a while?â€
istahtaisinkohan â€œI wonder if I should sit down for a whileâ€
4/38
Margin: Lemmatized BLEU
â€¢ Lemmatized BLEU:
â€¢ Lemmatized MT output against lemmatized references.
â€¢ Does not penalize errors in word forms.
â‡’ An indication of achievable BLEU score.
Englishâ†’Czech phrase-based translation:
PCEDT Project Syndicate
Regular BLEU, lowercase 25.2 ~12
Lemmatized BLEU 33.6 ~20
â€¢ Margin for improvement: ~8 points in both experiments.
5/38
Morphology in
Phrase-Based MT
PBMT Main ComponentsInput Global Search
for sentence with highest probability Output
Parallel Texts
Translation Model
Monolingual Texts
Language Model
6/38
LM over Forms Insufficient
Possible translations differring in morphology:
two green striped cats
dvou zelenÃ¡ pruhovanÃ½ koÄkÃ¡ch â† garbage
dva zelenÃ© pruhovanÃ© koÄky â† 3grams ok, 4gram bad
dvÄ› zelenÃ© pruhovanÃ© koÄky â† correct nominative/accusative
dvÄ›ma zelenÃ½m pruhovanÃ½m koÄkÃ¡m â† correct dative
â€¢ 3-gram LM too weak to ensure agreement.
â€¢ 3-gram LM possibly already too sparse!
7/38
Explicit Morphological Target Factor
â€¢ Add morphological tag to each output token:
two green striped cats
dvou zelenÃ¡ pruhovanÃ½ koÄkÃ¡ch â† garbage
fem-loc neut-acc masc-nom-sg fem-loc
dva zelenÃ© pruhovanÃ© koÄky â† 3-grams ok, 4-gram bad
masc-nom masc-nom masc-nom
fem-nom fem-nom fem-nom
dvÄ› zelenÃ© pruhovanÃ© koÄky â† correct nominative/accusative
fem-nom fem-nom fem-nom fem-nom
fem-acc fem-acc fem-acc fem-acc
dvÄ›ma zelenÃ½m pruhovanÃ½m koÄkÃ¡m â† correct dative
fem-dat fem-dat fem-dat fem-dat
8/38
Advantages of Explicit Morphology for LM
â€¢ LM over morphological tags generalizes better.
â€¢ p(dvÄ› koÄkÃ¡ch) < p(dvÄ› koÄky) â€¦surely
But we would need to see all combinations of pruhovanÃ½ and koÄka!
â‡’ Better to ask if p(fem-nom fem-loc) < p(fem-nom fem-nom)
which is trained on any feminine adj+noun.
â€¢ But still does not solve everything.
â€¢ p(dvÄ› zelenÃ©) â‰· p(dva zelenÃ©) â€¦ bad question anyway!
Not solved by asking if p(fem-nom fem-nom) â‰· p(masc-nom masc-nom).
â€¢ Tagset size smaller than vocabulary.
â‡’ can afford e.g. 7-grams:
p(masc-nom fem-nom fem-nom) < p(fem-nom fem-nom fem-nom)
9/38
Motivation from Translation Model
Availability of translations of â€œknee capsâ€ in parallel data:
Case Surface form 50K 500K 5M 50M
nom ÄÃ©Å¡ky â€¢ â€¢ â€¢ â€¢
gen ÄÃ©Å¡ek â€“ â€¢ â€¢ â€¢
dat ÄÃ©Å¡kÃ¡m â€“ â€“ â€¢ â€¢
acc ÄÃ©Å¡ky âˆ˜ âˆ˜ â€¢ â€¢
voc ÄÃ©Å¡ky âˆ˜ âˆ˜ âˆ˜ âˆ˜
loc ÄÃ©Å¡kÃ¡ch â€“ â€¢ â€¢ â€¢
instr ÄÃ©Å¡kami â€“ â€“ â€“ â€¢
â‡’ You need to have 50M parallel sentences to translate:
â€œWhatâ€™s wrong with my knee caps?â€
â€œâ€¢â€ â€¦ the word was seen in the particular case,
â€œâˆ˜â€ â€¦ the surface form was seen but in a different case.
Reproduced from Huck et al. (2017b). 10/38
Advantages of Explicit Morphology for TM
Main idea:
â€¢ separate translation of lemmas and morphology:
J src lemma K â‡’ J tgt lemma K
J src morphology K â‡’ J tgt morphology K
â€¢ generate target forms based on the lemma and morphology:
J tgt lemma Kâ‡’ J tgt surface form K
J tgt morphology K
11/38
Factored Phrase-Based MT
â€¢ Both input and output words can have more factors.
â€¢ Arbitrary number and order of:
Mapping steps (â†’)
Translate (phrases of) source factors to target factors.
two green â†’ dvÄ› zelenÃ©
Generation steps (â†“)
src tgt
ğ‘“1 ğ‘’1 +LM
ğ‘“2 ğ‘’2
Generate target factors from target factors.
dvÄ› â†’ fem-nom; dva â†’ masc-nom
â‡’ Ensures â€œverticalâ€ coherence.
Target-side language models (+LM)
Applicable to various target-side factors.
â‡’ Ensures â€œhorizontalâ€ coherence. (Koehn and Hoang, 2007)
12/38
Translation Process in Factored PBMT
Input: (knee caps, knee cap, Adj NN-plur)
1. Translation step: lemma â‡’ lemma:
(?, ÄÃ©Å¡ka, ?)
2. Generation step: lemma â‡’ morphological tag
(?, ÄÃ©Å¡ka, N1-sg), (?, ÄÃ©Å¡ka, N2-sg), (?, ÄÃ©Å¡ka, N1-pl), (?, ÄÃ©Å¡ka, N2-pl), â€¦
3. Translation step: morphological tag â‡’morphological tag
â€¦ This reorders the options, so that plural is more likely:
(?, ÄÃ©Å¡ka, N1-pl), (?, ÄÃ©Å¡ka, N2-pl), (?, ÄÃ©Å¡ka, N1-sg), (?, ÄÃ©Å¡ka, N2-sg), â€¦
4. Generation step: lemma, morphological tag â‡’ surface form
(ÄÃ©Å¡ky, ÄÃ©Å¡ka, N1-pl), (ÄÃ©Å¡ek, ÄÃ©Å¡ka, N2-pl), â€¦, (ÄÃ©Å¡kami, ÄÃ©Å¡ka, N7-pl)
This and several following slides reuse slides by Philipp Koehn, MT Marathon 2009. 13/38
Factored PBMT Model
â€¢ Extension of the phrase-based model:
1. Phrase Extraction for mapping steps.
2. Extraction for generation steps.
3. Decoding.
â€¢ Each step simply brings one or more feature functions:
â€¢ Fits nicely into the log-linear model,
â€¢ Weights trained by the discriminative training (MERT).
â€¢ The order of the operations is defined in configuration.
14/38
Factored Phrase Extraction (1/3)
As in standard phrase-based MT:
1. Run sentence and word alignment,
Extract all phrases consistent with word alignment.natÃ¼rlich
hat
john
spass
am
spiel
naturally
john
has
fun
with
the
game
â‡’ Extracted: natÃ¼rlich hat john â†’ naturally john has
15/38
Factored Phrase Extraction (2/3)
As in standard phrase-based MT:
1. Run sentence and word alignment,
2. Extract all phrases consistent with word alignment.natÃ¼rlich
hat
john
spass
am
spiel
naturally
john
has
fun
with
the
game
â‡’ Extracted: natÃ¼rlich hat john â†’ naturally john has
16/38
Factored Phrase Extraction (3/3)
As in standard phrase-based MT:
1. Run sentence and word alignment,
2. Extract same phrases, just another factor from each word.ADV
V
NNP
NN
P
NN
ADV
NNP
V
NN
P
DET
NN
â‡’ Extracted: ADV V NNP â†’ ADV NNP V
17/38
The Benefit Illustrated Once Moreold | old | ADJ
man | man | NN
starÃ©ho | starÃ½ | AAMS4
pÃ¡na | pÃ¡n | NNMS4
black | black | ADJ
dog | dog | NN
ÄernÃ©mu | ÄernÃ½ | AAMS3
psovi | pes | NNMS3
18/38
The Benefit Illustrated Once Moreold | old | ADJ
man | man | NN
starÃ©ho | starÃ½ | AAMS4
pÃ¡na | pÃ¡n | NNMS4
black | black | ADJ
dog | dog | NN
ÄernÃ©mu | ÄernÃ½ | AAMS3
psovi | pes | NNMS3
starÃ©ho pÃ¡na
starÃ©ho
pÃ¡na
...
ÄernÃ©mu psovi
ÄernÃ©mu
psovi
...
old man
old
man
...
black dog
black
dog
...
=
=
=
=
=
=
Instead of one phrase table based on word forms:
18/38
The Benefit Illustrated Once Moreold | old | ADJ
man | man | NN
starÃ©ho | starÃ½ | AAMS4
pÃ¡na | pÃ¡n | NNMS4
black | black | ADJ
dog | dog | NN
ÄernÃ©mu | ÄernÃ½ | AAMS3
psovi | pes | NNMS3
starÃ©ho pÃ¡na
starÃ©ho
pÃ¡na
...
ÄernÃ©mu psovi
ÄernÃ©mu
psovi
...
old man
old
man
...
black dog
black
dog
...
=
=
=
=
=
=
Instead of one phrase table based on word forms:
old man
old
man
...
black dog
black
dog
=
=
=
=
=
=
We extract separately a table of lemmas and a table of tags:
starÃ½ pÃ¡n
starÃ½
pÃ¡n
...
ÄernÃ½ pes
ÄernÃ½
pes
18/38
The Benefit Illustrated Once Moreold | old | ADJ
man | man | NN
starÃ©ho | starÃ½ | AAMS4
pÃ¡na | pÃ¡n | NNMS4
black | black | ADJ
dog | dog | NN
ÄernÃ©mu | ÄernÃ½ | AAMS3
psovi | pes | NNMS3
starÃ©ho pÃ¡na
starÃ©ho
pÃ¡na
...
ÄernÃ©mu psovi
ÄernÃ©mu
psovi
...
old man
old
man
...
black dog
black
dog
...
=
=
=
=
=
=
Instead of one phrase table based on word forms:
old man
old
man
...
black dog
black
dog
=
=
=
=
=
=
We extract separately a table of lemmas and a table of tags:
starÃ½ pÃ¡n
starÃ½
pÃ¡n
...
ÄernÃ½ pes
ÄernÃ½
pes
AAMS4 NNMS4
AAMS4
NNMS4
...
AAMS3 NNMS3
AAMS3
NNMS3
ADJ NN
ADJ
NN
...
ADJ NN
ADJ
NN
=
=
=
=
=
=
We extract separately a table of lemmas and a table of tags:
18/38
The Benefit Illustrated Once Moreold | old | ADJ
man | man | NN
starÃ©ho | starÃ½ | AAMS4
pÃ¡na | pÃ¡n | NNMS4
black | black | ADJ
dog | dog | NN
ÄernÃ©mu | ÄernÃ½ | AAMS3
psovi | pes | NNMS3
starÃ©ho pÃ¡na
starÃ©ho
pÃ¡na
...
ÄernÃ©mu psovi
ÄernÃ©mu
psovi
...
old man
old
man
...
black dog
black
dog
...
=
=
=
=
=
=
Instead of one phrase table based on word forms:
old man
old
man
...
black dog
black
dog
=
=
=
=
=
=
We extract separately a table of lemmas and a table of tags:
starÃ½ pÃ¡n
starÃ½
pÃ¡n
...
ÄernÃ½ pes
ÄernÃ½
pes
AAMS4 NNMS4
AAMS4
NNMS4
...
AAMS3 NNMS3
AAMS3
NNMS3
ADJ NN
ADJ
NN
...
ADJ NN
ADJ
NN
=
=
=
=
=
=
We extract separately a table of lemmas and a table of tags:
black dog=ÄernÃ½ pes
AAMS4 NNMS4 ADJ NN=
18/38
Factored Phrase-Based MT
See the following slides by Philipp Koehn (Fri Jan 30, 2009, pp. 49â€“75):
â€¢ Decoding
â€¢ Reminder of standard phrase-based decoding
â€¢ Factored model decoding
â€¢ Experiments
â€¢ esp. Alternative decoding paths
19/38
48
Translation optionshe
er geht ja nicht nach hause
it
, it
, he
is
are
goes
go
yes
is
, of course
not
do not
does not
is not
after
to
according to
in
house
home
chamber
at home
not
is not
does not
do not
home
under house
return home
do not
it is
he will be
it goes
he goes
is
are
is after all
does
to
following
not after
not to
not
is not
are not
is not a
â€¢ Many translation options to choose from
â€“ in Europarl phrase table: 2727 matching phrase pairs for this sentence
â€“ by pruning to the top 20 per phrase, 202 translation options remain
MT Marathon Winter School, Lecture 5 30 January 2009
49
Translation optionshe
er geht ja nicht nach hause
it
, it
, he
is
are
goes
go
yes
is
, of course
not
do not
does not
is not
after
to
according to
in
house
home
chamber
at home
not
is not
does not
do not
home
under house
return home
do not
it is
he will be
it goes
he goes
is
are
is after all
does
to
following
not after
not to
not
is not
are not
is not a
â€¢ The machine translation decoder does not know the right answer
â†’ Search problem solved by heuristic beam search
MT Marathon Winter School, Lecture 5 30 January 2009
50
Decoding process: precompute translation optionser geht ja nicht nach hause
MT Marathon Winter School, Lecture 5 30 January 2009
51
Decoding process: start with initial hypothesiser geht ja nicht nach hause
MT Marathon Winter School, Lecture 5 30 January 2009
52
Decoding process: hypothesis expansioner geht ja nicht nach hause
are
MT Marathon Winter School, Lecture 5 30 January 2009
53
Decoding process: hypothesis expansioner geht ja nicht nach hause
are
it
he
MT Marathon Winter School, Lecture 5 30 January 2009
54
Decoding process: hypothesis expansioner geht ja nicht nach hause
are
it
he goes
does not
yes
go
to
home
home
MT Marathon Winter School, Lecture 5 30 January 2009
55
Decoding process: find best pather geht ja nicht nach hause
are
it
he goes
does not
yes
go
to
home
home
MT Marathon Winter School, Lecture 5 30 January 2009
56
Factored model decoding
â€¢ Factored model decoding introduces additional complexity
â€¢ Hypothesis expansion not any more according to simple translation table, but
by executing a number of mapping steps, e.g.:
1. translating of lemma â†’ lemma
2. translating of part-of-speech, morphology â†’ part-of-speech, morphology
3. generation of surface form
â€¢ Example: haus|NN|neutral|plural|nominative
â†’ { houses|house|NN|plural, homes|home|NN|plural,
buildings|building|NN|plural, shells|shell|NN|plural }
â€¢ Each time, a hypothesis is expanded, these mapping steps have to applied
MT Marathon Winter School, Lecture 5 30 January 2009
57
Efficient factored model decoding
â€¢ Key insight: executing of mapping steps can be pre-computed and stored as
translation options
â€“ apply mapping steps to all input phrases
â€“ store results as translation options
â†’ decoding algorithm unchanged... haus | NN | neutral | plural | nominative ...
houses|house|NN|plural
homes|home|NN|plural
buildings|building|NN|plural
shells|shell|NN|plural
...
...
...
...
...
...
...
...
...
... ...
...
MT Marathon Winter School, Lecture 5 30 January 2009
58
Efficient factored model decoding
â€¢ Problem: Explosion of translation options
â€“ originally limited to 20 per input phrase
â€“ even with simple model, now 1000s of mapping expansions possible
â€¢ Solution: Additional pruning of translation options
â€“ keep only the best expanded translation options
â€“ current default 50 per input phrase
â€“ decoding only about 2-3 times slower than with surface model
MT Marathon Winter School, Lecture 5 30 January 2009
59
Factored Translation Models
â€¢ Motivation
â€¢ Example
â€¢ Model and Training
â€¢ Decoding
â€¢ Experiments
MT Marathon Winter School, Lecture 5 30 January 2009
60
Adding linguistic markup to outputword word
part-of-speech
OutputInput
â€¢ Generation of POS tags on the target side
â€¢ Use of high order language models over POS (7-gram, 9-gram)
â€¢ Motivation: syntactic tags should enforce syntactic sentence structure model
not strong enough to support major restructuring
MT Marathon Winter School, Lecture 5 30 January 2009
61
Some experiments
â€¢ Englishâ€“German, Europarl, 30 million word, test2006
Model BLEU
best published result 18.15
baseline (surface) 18.04
surface + POS 18.15
â€¢ Germanâ€“English, News Commentary data (WMT 2007), 1 million word
Model BLEU
Baseline 18.19
With POS LM 19.05
â€¢ Improvements under sparse data conditions
â€¢ Similar results with CCG supertags [Birch et al., 2007]
MT Marathon Winter School, Lecture 5 30 January 2009
63
Local agreement (esp. within noun phrases)word word
part-of-speech
OutputInput
morphology
â€¢ High order language models over POS and morphology
â€¢ Motivation
â€“ DET-sgl NOUN-sgl good sequence
â€“ DET-sgl NOUN-plural bad sequence
MT Marathon Winter School, Lecture 5 30 January 2009
65
Morphological generation modellemma lemma
part-of-speech
OutputInput
morphology
part-of-speech
word word
â€¢ Our motivating example
â€¢ Translating lemma and morphological information more robust
MT Marathon Winter School, Lecture 5 30 January 2009
66
Initial results
â€¢ Results on 1 million word News Commentary corpus (Germanâ€“English)
System In-doman Out-of-domain
Baseline 18.19 15.01
With POS LM 19.05 15.03
Morphgen model 14.38 11.65
â€¢ What went wrong?
â€“ why back-off to lemma, when we know how to translate surface forms?
â†’ loss of information
MT Marathon Winter School, Lecture 5 30 January 2009
67
Solution: alternative decoding pathslemma lemma
part-of-speech
OutputInput
morphology
part-of-speech
word word
or
â€¢ Allow both surface form translation and morphgen model
â€“ prefer surface model for known words
â€“ morphgen model acts as back-off
MT Marathon Winter School, Lecture 5 30 January 2009
68
Results
â€¢ Model now beats the baseline:
System In-doman Out-of-domain
Baseline 18.19 15.01
With POS LM 19.05 15.03
Morphgen model 14.38 11.65
Both model paths 19.47 15.23
MT Marathon Winter School, Lecture 5 30 January 2009
69
Adding annotation to the source
â€¢ Source words may lack sufficient information to map phrases
â€“ English-German: what case for noun phrases?
â€“ Chinese-English: plural or singular
â€“ pronoun translation: what do they refer to?
â€¢ Idea: add additional information to the source that makes the required
information available locally (where it is needed)
â€¢ see [Avramidis and Koehn, ACL 2008] for details
MT Marathon Winter School, Lecture 5 30 January 2009
70
Case Information for Englishâ€“GreekOutputInput
case
word word
subject/object
â€¢ Detect in English, if noun phrase is subject/object (using parse tree)
â€¢ Map information into case morphology of Greek
â€¢ Use case morphology to generate correct word form
MT Marathon Winter School, Lecture 5 30 January 2009
71
Obtaining Case Information
â€¢ Use syntactic parse of English input
(method similar to semantic role labeling)
MT Marathon Winter School, Lecture 5 30 January 2009
72
Results English-Greek
â€¢ Automatic BLEU scores System devtest test07
baseline 18.13 18.05
enriched 18.21 18.20
â€¢ Improvement in verb inflection
System Verb count Errors Missing
baseline 311 19.0% 7.4%
enriched 294 5.4% 2.7%
â€¢ Improvement in noun phrase inflection
System NPs Errors Missing
baseline 247 8.1% 3.2%
enriched 239 5.0% 5.0%
â€¢ Also successfully applied to English-Czech
MT Marathon Winter School, Lecture 5 30 January 2009
Translation Scenarios for Enâ†’Cs
Vanilla Translate+Check (T+C)
English Czech
form form +LM
lemma lemma
morphology morphology
English Czech
form form +LM
lemma lemma
morphology morphology +LM
Translate+2â‹…Check (T+C+C) 2â‹…Translate+Generate (T+T+G)
English Czech
form form +LM
lemma lemma +LM
morphology morphology +LM
English Czech
form form +LM
lemma lemma +LM
morphology morphology +LM
20/38
Details on Translate+Check
â€¢ Drawback: Morphological tags increase target-side complexity:
word form â†’ word form word form â†’ word form
morphological tag
green striped
zelenÃ½ pruhovanÃ½
zelenÃ© pruhovanÃ©
zelenÃ­ pruhovanÃ­
zelenÃ½ch pruhovanÃ½ch
zelenÃ½m pruhovanÃ½m
green striped
zelenÃ½sg,masc,nom pruhovanÃ½sg,masc,nom
zelenÃ©sg,fem,gen pruhovanÃ©sg,fem,gen
zelenÃ©sg,fem,dat pruhovanÃ©sg,fem,dat
zelenÃ©pl,fem,nom pruhovanÃ©pl,fem,nom
zelenÃ­pl,masc,nom pruhovanÃ­pl,masc,nom
zelenÃ½chpl,masc,loc pruhovanÃ½chpl,masc,loc
zelenÃ½m pruhovanÃ½m
â€¢ Benefit: more robust LMs, e.g. trained on morphological tags only.
â€¢ p(fem,nom masc,loc) < p(fem,nom fem,nom) â€¦ observed on all adjectives.
â€¢ p(zelenÃ© pruhovanÃ½ch) < p(zelenÃ© pruhovanÃ©) â€¦ much sparser.
21/38
Factored Attempts (WMT09)
Sents System BLEU NIST Sent/min
2.2M Vanilla PBMT 14.24 5.175 12.0
2.2M T+C 13.86 5.110 2.6
84k Vanilla PBMT 10.52 4.506 â€“
84k T+C+C&T+T+G 10.01 4.360 4.0
â€¢ In WMT07, T+C worked best.
+ fine-tuned tags helped with small data (Bojar, 2007).
â€¢ In WMT08, T+C was worth the effort (Bojar and HajiÄ, 2008).
â€¢ In WMT09, our computers could handle 7-grams of forms.
â‡’ No gain from T+C.
â€¢ T+T+G too big to fit and explodes the search space.
â‡’ Worse than Vanilla trained on the same dataset. 22/38
T+T+G Failure Explained
â€¢ Factored models are â€œsynchronousâ€, i.e. Moses:
1. Generates fully instantiated â€œtranslation optionsâ€.
2. Appends translation options to extend â€œpartial hypothesisâ€.
3. Applies LM to see how well the option fits the previous words.
â€¢ There are too many possible combinations of lemma+tag.
â‡’ Less promising ones must be pruned.
! Pruned before the linear context is available.
â€¢ Hieu Hoang wasted a year on trying asynchronous factors.
â€¢ Pruning hard to design (no clear comparison for partial translation options).
â€¢ In a completely different decoder Bojar and TÃ½novskÃ½ (2009) use â€œdelayed
factorsâ€.
â€¢ The final value generated only after the full hypothesis is ready.
23/38
Big / Long / Morphological LMs
â€¢ Our best setups used four LMs:
LM ID Factor Order # Training Tokens
long word form 7 685M
big word form 4 3903M
morph morph. tag 10 817M
longm morph. tag 15 817M
â€¢ â€¦ with complementary benefits:
long big long morph big long big morph big long morph all + longm
21.32 22.00 22.01 22.26 22.21 22.48 22.59
24/38
Tentative Summary
â€¢ Target-side rich morphology causes data sparseness.
â€¢ Factored setups compact the sparseness.
â€¦ but the search space is likely to explode at runtime.
â€¢ Explosion can be contained by pruning.
â€¦ but the pruning happens without linear context
â‡’ high risk of search errors.
Two promising techniques for handling sparseness
and avoiding the explosion:
â€¢ Two-step translation (Bojar and Kos, 2010).
â€¢ Reverse self-training (Bojar and Tamchyna, 2011).
25/38
Two-Step Attempts (WMT10) 1/2
1. English â†’ lemmatized Czech
â€¢ meaning-bearing morphology preserved
â€¢ max phrase len 10, distortion limit 6
â€¢ large target-side (lemmatized LM)
2. Lemmatized Czech â†’ Czech
â€¢ trained on much more data
â€¢ max phrase len 1, monotone
Src after a sharp drop
Mid po+6 ASA1.prudkÃ½ NSA-.pokles
Gloss after+voc adj+sg...sharp noun+sg...drop
Out po prudkÃ©m poklesu
26/38
Two-Step Attempts (WMT10) 2/2
Training Sents Vanilla Two-Step Diff
Parallel Mono BLEU SemPOS BLEU SemPOS B. S.
126k 126k 10.28Â±0.40 29.92 10.38Â±0.38 30.01 â†—â†—
27/38
Two-Step Attempts (WMT10) 2/2
Training Sents Vanilla Two-Step Diff
Parallel Mono BLEU SemPOS BLEU SemPOS B. S.
126k 126k 10.28Â±0.40 29.92 10.38Â±0.38 30.01 â†—â†—
126k 13M 12.50Â±0.44 31.01 12.29Â±0.47 31.40 â†˜â†—
27/38
Two-Step Attempts (WMT10) 2/2
Training Sents Vanilla Two-Step Diff
Parallel Mono BLEU SemPOS BLEU SemPOS B. S.
126k 126k 10.28Â±0.40 29.92 10.38Â±0.38 30.01 â†—â†—
126k 13M 12.50Â±0.44 31.01 12.29Â±0.47 31.40 â†˜â†—
7.5M 13M 14.17Â±0.51 33.07 14.06Â±0.49 32.57 â†˜â†˜
27/38
Two-Step Attempts (WMT10) 2/2
Training Sents Vanilla Two-Step Diff
Parallel Mono BLEU SemPOS BLEU SemPOS B. S.
126k 126k 10.28Â±0.40 29.92 10.38Â±0.38 30.01 â†—â†—
126k 13M 12.50Â±0.44 31.01 12.29Â±0.47 31.40 â†˜â†—
7.5M 13M 14.17Â±0.51 33.07 14.06Â±0.49 32.57 â†˜â†˜
Manual micro-evaluation of â†˜â†—, i.e. 12.50Â±0.44 vs. 12.29Â±0.47:
Two-Step Both Fine Both Wrong Vanilla Total
Two-Step 23 4 8 - 35
Both Fine 7 14 17 5 43
Both Wrong 8 1 28 2 39
Vanilla - 3 7 23 33
Total 38 22 60 30 150
â€¢ Each annotator weakly prefers Two-step
â€¢ but they donâ€™t agree on individual sentences. 27/38
Reverse Self-Training
Goal: Learn from monolingual data to produce new target-side
word forms in correct contexts.
Source English Target Czech
Small Parallel a cat chasedâ€¦ = koÄka honilaâ€¦
I saw a cat = vidÄ›l jsem koÄku
Big Monolingual Äetl jsem o koÄce
28/38
Reverse Self-Training
Goal: Learn from monolingual data to produce new target-side
word forms in correct contexts.
Source English Target Czech
Small Parallel a cat chasedâ€¦ = koÄka honilaâ€¦
I saw a cat = vidÄ›l jsem koÄku
Big Monolingual ? Äetl jsem o koÄce
28/38
Reverse Self-Training
Goal: Learn from monolingual data to produce new target-side
word forms in correct contexts.
Source English Target Czech
Small Parallel a cat chasedâ€¦ = koÄka honilaâ€¦
I saw a cat = vidÄ›l jsem koÄku
Big Monolingual ? Äetl jsem o koÄce
Use reverse translation
28/38
Reverse Self-Training
Goal: Learn from monolingual data to produce new target-side
word forms in correct contexts.
Source English Target Czech
Small Parallel a cat chasedâ€¦ = koÄka honilaâ€¦
koÄka honitâ€¦ (lem.)
I saw a cat = vidÄ›l jsem koÄku
vidÄ›t bÃ½t koÄka (lem.)
Big Monolingual ? Äetl jsem o koÄce
ÄÃ­st bÃ½t o koÄka (lem.)
Use reverse translation
backed-off by lemmas.
28/38
Reverse Self-Training
Goal: Learn from monolingual data to produce new target-side
word forms in correct contexts.
Source English Target Czech
Small Parallel a cat chasedâ€¦ = koÄka honilaâ€¦
koÄka honitâ€¦ (lem.)
I saw a cat = vidÄ›l jsem koÄku
vidÄ›t bÃ½t koÄka (lem.)
Big Monolingual ? Äetl jsem o koÄce
ÄÃ­st bÃ½t o koÄka (lem.)
Use reverse translation
I read about a cat â† backed-off by lemmas.
28/38
Reverse Self-Training
Goal: Learn from monolingual data to produce new target-side
word forms in correct contexts.
Source English Target Czech
Small Parallel a cat chasedâ€¦ = koÄka honilaâ€¦
koÄka honitâ€¦ (lem.)
I saw a cat = vidÄ›l jsem koÄku
vidÄ›t bÃ½t koÄka (lem.)
Big Monolingual ? Äetl jsem o koÄce
ÄÃ­st bÃ½t o koÄka (lem.)
Use reverse translation
I read about a cat â† backed-off by lemmas.
â‡’ A new phrase learned: â€œabout a catâ€ = â€œo koÄceâ€.
28/38
The Back-off to Lemmas
â€¢ The key distinction from self-training used for domain adaptation
(Bertoldi and Federico, 2009; Ueffing et al., 2007).
â€¢ We use simply â€œalternative decoding pathsâ€ in Moses:
Czech English
form form +LM or Czech English
lemma form +LM
â€¢ Other languages (e.g. Turkish, German) need different back-off
techniques:
â€¢ Split German compounds.
â€¢ Separate and allow to ignore Turkish morphology.
29/38
Small Parallel, Increasing Monolingual
1 2 3 4 5
26
28
30
32
Monolingual Data Size (millions of sentences)
Translation Quality (Automatic; BLEU)
LM only
30/38
Small Parallel, Increasing Monolingual
1 2 3 4 5
26
28
30
32
Monolingual Data Size (millions of sentences)
Translation Quality (Automatic; BLEU)
LM only
LM and TM
30/38
Increasing Para, Fixed Mono26
28
30
32
34
36
38
0 0.5 1 1.5 2 2.5 3 3.5 4 4.5
BLEU
%
Parallel data (mils of sents.)
Mono LM and TM
Mono LM
31/38
Morphology in
Neural MT
NMT: â€œSolvedâ€ by Segmentation
â€¢ SMT struggled with productive morphology (>1M wordforms).
nejneobhodpodaÅ™ovÃ¡vatelnÄ›jÅ¡Ã­mi, DonaudampfschifffahrtsgesellschaftskapitÃ¤n
â€¢ NMT can handle only 30â€“80k dictionaries.
â‡’ Resort to sub-word units.
Orig ÄeskÃ½ politik svezl migranty
Syllables Äes kÃ½ âŠ” po li tik âŠ” sve zl âŠ” mig ran ty
Morphemes Äesk Ã½ âŠ” politik âŠ” s vez l âŠ” migrant y
Char Pairs Äe sk Ã½ âŠ” po li ti k âŠ” sv ez l âŠ” mi gr an ty
Chars Ä e s k Ã½ âŠ” p o l i t i k âŠ” s v e z l âŠ” m i g r a n t y
BPE 30k ÄeskÃ½ politik s@@ vez@@ l mi@@ granty
32/38
Byte Pair Encoding (Sennrich et al., 2016)
â€¢ Given a dictionary of token types and frequences.
1. Replace the most frequent pair of characters with a new unit .
(Record this â€œmergeâ€ operation.)
2. Repeat until the desired number of merge operations is reached.
33/38
Byte Pair Encoding (Sennrich et al., 2016)
â€¢ Given a dictionary of token types and frequences.
1. Replace the most frequent pair of characters with a new unit .
(Record this â€œmergeâ€ operation.)
2. Repeat until the desired number of merge operations is reached.
Current vocabulary The new merge
lower lowest newer widest
33/38
Byte Pair Encoding (Sennrich et al., 2016)
â€¢ Given a dictionary of token types and frequences.
1. Replace the most frequent pair of characters with a new unit .
(Record this â€œmergeâ€ operation.)
2. Repeat until the desired number of merge operations is reached.
Current vocabulary The new merge
lower lowest newer widest we â†’ we
33/38
Byte Pair Encoding (Sennrich et al., 2016)
â€¢ Given a dictionary of token types and frequences.
1. Replace the most frequent pair of characters with a new unit .
(Record this â€œmergeâ€ operation.)
2. Repeat until the desired number of merge operations is reached.
Current vocabulary The new merge
lower lowest newer widest we â†’ we
lo we r lo we st ne we r widest
33/38
Byte Pair Encoding (Sennrich et al., 2016)
â€¢ Given a dictionary of token types and frequences.
1. Replace the most frequent pair of characters with a new unit .
(Record this â€œmergeâ€ operation.)
2. Repeat until the desired number of merge operations is reached.
Current vocabulary The new merge
lower lowest newer widest we â†’ we
lo we r lo we st ne we r widest we r â†’ we r
33/38
Byte Pair Encoding (Sennrich et al., 2016)
â€¢ Given a dictionary of token types and frequences.
1. Replace the most frequent pair of characters with a new unit .
(Record this â€œmergeâ€ operation.)
2. Repeat until the desired number of merge operations is reached.
Current vocabulary The new merge
lower lowest newer widest we â†’ we
lo we r lo we st ne we r widest we r â†’ we r
lo we r lo we st ne we r widest
33/38
Byte Pair Encoding (Sennrich et al., 2016)
â€¢ Given a dictionary of token types and frequences.
1. Replace the most frequent pair of characters with a new unit .
(Record this â€œmergeâ€ operation.)
2. Repeat until the desired number of merge operations is reached.
Current vocabulary The new merge
lower lowest newer widest we â†’ we
lo we r lo we st ne we r widest we r â†’ we r
lo we r lo we st ne we r widest st â†’ st
33/38
Byte Pair Encoding (Sennrich et al., 2016)
â€¢ Given a dictionary of token types and frequences.
1. Replace the most frequent pair of characters with a new unit .
(Record this â€œmergeâ€ operation.)
2. Repeat until the desired number of merge operations is reached.
Current vocabulary The new merge
lower lowest newer widest we â†’ we
lo we r lo we st ne we r widest we r â†’ we r
lo we r lo we st ne we r widest st â†’ st
â€¢ New input: Apply the recorded sequence of merges:
newest â†’ ne we st â†’ ne we st â‡’ n@@ e@@ we@@ st
â€¢ Ensures that vocabulary size = alphabet + merge ops.
33/38
Flavours of Subword Units
â€¢ Byte Pair Encoding (BPE, Sennrich et al. (2016))
http://github.com/rsennrich/subword-nmt/
â€¢ Google Wordpieces (Wu et al., 2016)
Code probably unavailable, used in speech.
â€¢ SubwordTextEncoder in Tensor2tensor (Vaswani et al., 2017)
https://github.com/tensorflow/tensor2tensor
STE BlÃ­Å¾Ã­_ se_ k_ tobÄ›_ tramvaj _ ._
Z_ tramvaj e_ nevysto upil i_ ._
BPE BlÃ­Å¾Ã­ se k tobÄ› tramvaj .
Z tramva@@ je nevy@@ stoupili .
BPE underscore BlÃ­Å¾Ã­_ se_ k_ tobÄ›_ tramvaj@@ _ ._
Z_ tramvaj@@ e_ nevy@@ stoupili_ ._
The best now is SentencePiece: https://github.com/google/sentencepiece 34/38
Performance of STE and BPE
â€¢ Germanâ†’Czech T2T experiments (MachÃ¡Äek et al., 2018).
â€¢ The underscore trick:
â€¢ Append â€œ_â€ to tokens before learning splits.
split underscore BLEU
STE after every token 18.58Â±0.06
BPE after non-final tokens 18.24Â±0.08
BPE after every token 13.88Â±0.18
BPE - 13.69Â±0.66
â€¢ +5(!) BLEU points from the underscore trick.
â€¢ If not attached at the end of the sentence.
35/38
Room for Linguistics
â€¢ Ataman et al. (2017) use a new Morfessor model Flatcat
(GrÃ¶nroos et al., 2014) for Turkish.
â€¢ Considerably better than BPE.
â€¢ Huck et al. (2017a) examine Englishâ†’German:
â€¢ Compound, suffix, prefix and BPE splitting, or a cascade.
â€¢ Suffix+BPE or Compound+suffix+BPE best.
â€¢ MachÃ¡Äek et al. (2018) for Germanâ†’Czech:
â€¢ Unsupervised (Morfessor) and supervised (DeriNet).
â€¢ STE worked best.
36/38
Summary
â€¢ Rich morphology causes serious problems to token-based MT.
â€¢ Factors in PBMT allow to capture additional info.
â€¢ Rich annotation is dangerous when not treated carefully.
Occamâ€™s razor: think twice before adding an attribute.
â€¢ Avoid data sparseness, always provide a back-off.
â€¢ Avoid complex models:
- They are hard to tune (set parameters).
- They tend to explode at runtime.
â€¢ Promising 2-step translation.
â€¢ Reverse self-training good for small data.
â€¢ NMT with subword units resolves problems with morphology.
â€¢ Still room for linguistically-adequate solutions.
37/38
References
Duygu Ataman, Matteo Negri, Marco Turchi, and Marcello Federico. 2017. Linguistically motivated vocabulary
reduction for neural machine translation from turkish to english. The Prague Bulletin of Mathematical Linguistics,
108:331, Jan.
Nicola Bertoldi and Marcello Federico. 2009. Domain adaptation for statistical machine translation with monolingual
resources. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 182â€“189, Athens, Greece,
March. Association for Computational Linguistics.
OndÅ™ej Bojar and Jan HajiÄ. 2008. Phrase-Based and Deep Syntactic English-to-Czech Statistical Machine Translation.
In Proceedings of the Third Workshop on Statistical Machine Translation, pages 143â€“146, Columbus, Ohio, June.
Association for Computational Linguistics.
OndÅ™ej Bojar and Kamil Kos. 2010. 2010 Failures in English-Czech Phrase-Based MT. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and MetricsMATR, pages 60â€“66, Uppsala, Sweden, July. Association for
Computational Linguistics.
OndÅ™ej Bojar and AleÅ¡ Tamchyna. 2011. Improving Translation Model by Monolingual Data. In Proceedings of the
Sixth Workshop on Statistical Machine Translation, pages 330â€“336, Edinburgh, Scotland, July. Association for
Computational Linguistics.
OndÅ™ej Bojar and Miroslav TÃ½novskÃ½. 2009. Evaluation of Tree Transfer System. Project Euromatrix - Deliverable 3.4,
ÃšFAL, Charles University, March.
OndÅ™ej Bojar. 2007. English-to-Czech Factored Machine Translation. In Proceedings of the Second Workshop on
Statistical Machine Translation, pages 232â€“239, Prague, Czech Republic, June. Association for Computational
Linguistics.
Stig-Arne GrÃ¶nroos, Sami Virpioja, Peter Smit, and Mikko Kurimo. 2014. Morfessor FlatCat: An HMM-Based Method
for Unsupervised and Semi-Supervised Learning of Morphology. In Proceedings of COLING 2014, the 25th International38/38


Syntax in Pre-Neural
Statistical MT
OndÅ™ej Bojar
April 9, 2020
NPFL087 Statistical Machine Translation
Charles University
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics unless otherwise stated
Outline
â€¢ Motivation for grammar in MT.
â€¢ Hierarchical Model.
â€¢ Proper syntax: Constituency vs. dependency trees.
Constituency Syntax:
â€¢ Context Free Grammars.
â€¢ MT as parsing.
â€¢ Synchronous CFG, LM integration.
â€¢ Why real source/target parse trees make it harder.
Dependency Syntax:
â€¢ Surface syntax (STSG), problems.
â€¢ Deep syntax (t-layer); TectoMT, HMTM.
1/43
Motivation
Simple phrase-based models:
â€¢ Donâ€™t check for grammatical coherence.
â‡’ 3-grams fluent, overall word salad.
â€¢ Donâ€™t allow gaps in phrases:
I do not knowâ€¦ â†” Je ne sais pasâ€¦
â€œdo notâ€ â†” â€œne pasâ€
â€¢ Reordering models have little explicit knowledge:
â€¢ No movement of longer blocks.
â€¢ No grammar constraints possible.
2/43
Hierarchical Model
Hierarchical Phrase-Based Model
Hierarchical model (Chiang, 2005): rough approximation of syntax.
Hiero addresses only the gaps in phrases:
â€¢ Gaps donâ€™t have labels â‡’ any phrase fits.
â€œdo not Xâ€ â†” â€œne X pasâ€
â€œdo not catâ€ â†” â€œne chat pasâ€
â‡’ Not really a grammar, but not an issue for correct input.
â€¢ Phrase extraction similar to phrase-based:
1. Extract all (non-gappy) phrases consistent with alignment.
2. If a subphrase is also extraced, make a synchronous gap.
â‡’ Much higher number of rules extracted.
3/43
Hierarchical Phrase Extraction
4/43
Hierarchical Phrase Extraction
4/43
Challenges of Hierarchical Model
â€¢ Very high number of extractable rules.
â€¢ Limited by ad-hoc constraints:
â€¢ Initial phrases â‰¤ 10 words.
â€¢ Rules â‰¤ 6 symbols.
â€¢ At least one aligned terminal required.
â€¢ At most two non-terminals, non-adjacent.
â€¢ Spurious ambiguity.
= many ways to obtain the same output.
â€¢ Pollutes n-best lists.
â€¢ LM is a non-local feature.
â€¢ Causes serious state splitting â‡’ Search space much larger.
â€¦ Plus hierarchical model is not a syntactic model.
â€¢ With a special treatment of unaligned words, a regular PBMT system can
reach most of hierarchical hypotheses (Auli et al., 2009).
5/43
Proper Syntax
Constituency vs. Dependency Trees
SPPPP

NP
John
VP
HHH

V
loves
NP
Mary
Constituency trees = syntactic structure of a sentence as â€œbrackettingâ€:
(ğ‘† (ğ‘ ğ‘ƒ John) (ğ‘‰ ğ‘ƒ (ğ‘‰ loves) (ğ‘ ğ‘ƒ Mary) ) )
6/43
Constituency vs. Dependency Trees
SPPPP

NP
John
VP
HHH

V
loves
NP
Mary
Constituency trees = syntactic structure of a sentence as â€œbrackettingâ€:
(ğ‘† (ğ‘ ğ‘ƒ John) (ğ‘‰ ğ‘ƒ (ğ‘‰ loves) (ğ‘ ğ‘ƒ Mary) ) )
6/43
Constituency vs. Dependency Trees
SPPPP

NP
John
VP
HHH

V
loves
NP
Mary
loves
bbb
"
""
John Mary
or
John loves Mary
Constituency trees = syntactic structure of a sentence as â€œbrackettingâ€:
(ğ‘† (ğ‘ ğ‘ƒ John) (ğ‘‰ ğ‘ƒ (ğ‘‰ loves) (ğ‘ ğ‘ƒ Mary) ) )
6/43
Contituency Syntax
See MT Talk #10:
http://youtu.be/y_9SEdG1u3U
Context Free Grammar
Context-free grammar (CFG) describes infinite set of valid trees
using a finite set of rules, e.g.:
S â†’ NP VP
Probabilistic CFG assign weights to rules, e.g.:
VP â†’ {
V 0.1
V NP 0.5
V NP NP 0.4
(1)
Generative model for probabilitic CFG:
ğ‘(tree ğ‘‡ |sentence ğ‘ ) = âˆ
ğ‘‹â†’ğ›¼âˆˆğ‘‡
ğ‘(ğ›¼|ğ‘‹) (2)
7/43
Parsing
Parsing is finding the most probable constituency tree:Ì‚
ğ‘‡ = argmax
ğ‘‡ âˆˆ{trees of sentence ğ‘ }
ğ‘(ğ‘‡ |ğ‘ ) (3)
CKY (CYK) algorithm for ğ‘‚(ğ‘›3) parsing. (â€œdynamic programmingâ€):
length: 7 S
6 VP
5
4 S
3 VP PP
2 S NP NP
1 NP V, VP Det N P Det N
0she1 1eats2 2a3 3fish4 4with5 6a6 6fork7 8/43
Synchronous CFG
â€¢ Synchronous CFG capture the â€œdouble generationâ€.
â€¢ Nonterminals must map 1-1.
9/43
Synchronous CFG
â€¢ Synchronous CFG capture the â€œdouble generationâ€.
â€¢ Nonterminals must map 1-1.
SXXXXX

NP
John
VP``````
V
fell
PP
cc##
P
in
N
love
PP
bbb
"
""
P
with
N
Mary
SPPPP

NP
Jan
VP
HHH

V
miluje
NP
Marii
9/43
Synchronous CFG
â€¢ Synchronous CFG capture the â€œdouble generationâ€.
â€¢ Nonterminals must map 1-1.
â€¢ Synchronous Tree Substitution Grammars (STSG)
â€¢ Whole subtrees are attached â‡’ structural changes ok.
SXXXXX

NP
John
VP``````
V
fell
PP
cc##
P
in
N
love
PP
bbb
"
""
P
with
N
Mary
SPPPP

NP
Jan
VP
HHH

V
miluje
NP
Marii
9/43
Synchronous CFG
â€¢ Synchronous CFG capture the â€œdouble generationâ€.
â€¢ Nonterminals must map 1-1.
â€¢ Synchronous Tree Substitution Grammars (STSG)
â€¢ Whole subtrees are attached â‡’ structural changes ok.
â€¢ In fact equivalent to SCFG.
SXXXXX

NP
John
VP``````
V
fell
PP
cc##
P
in
N
love
PP
bbb
"
""
P
with
N
Mary
SXXXXX

NP
John
VP+ğ‘ƒ ğ‘ƒ +ğ‘ƒ +ğ‘XXXXXX

V
fell
in love PP+ğ‘ƒ +ğ‘
bbb
"
""
with Mary
SPPPP

NP
Jan
VP
HHH

V
miluje
NP
Marii
9/43
MT as Parsing: While Parsing, Translate
Picture from slides by Li et al. (2009).
10/43
State Splitting for LM
11/43
Proper Syntax is Hard
See slides by Chiang (2010):
â€¢ The source and target trees constrain too much.
â‡’ Too few rules extracted.
â‡’ Coverage lost.
â€¢ Labelled non-terminals do not always match.
â‡’ Some valid translations not admissible.
Relaxation of the hard constraints needed:
â€¢ Allow breaking trees into smaller fragments (e.g. binarization).
â€¢ Allow attachment of non-matching non-terminals.
12/43
the
DT JJ NN
green witch
la bruja verde
NP
DT NN JJ
P
a
VB NP
VP
bofetada
slap
PP
DTdiÃ³
una
NN
NPV
VP
STSG
extraction
1. Phrases
âœ¤ respect word alignments
âœ¤ are syntactic constituents on
both sides
2. Phrase pairs form rules
3. Subtract phrases to form rules
VB NP
VP
bofetada
slap
PP
DTdiÃ³
una
NN
NPV
VP
STSG
extraction
1. Phrases
âœ¤ respect word alignments
âœ¤ are syntactic constituents on
both sides
2. Phrase pairs form rules
3. Subtract phrases to form rules
Why is tree-to-tree hard?


13





116
 
 SRLQWV


11
  FKHFN

    116

    11



43





&'
  




,1
  WKDQ


--5
  PRUH
  FKHFN SR


13

    116



43





&'
  




,1
  WKDQ


--5
  PRUH
too few derivations
NP
the
DT JJ NN
green witch
a la bruja verde
too few rules
Why is tree-to-tree hard?
NP
the
DT JJ NN
green witch
a la bruja verde
too few rules



,3





93
  






13



11
  ã™«â˜




33
  ãµ€â¼‰â‰¤ã»ã¯ãº²



13



15
  ãœã ™
  13
    33
  LQWUDGHEHWZHHQWKHWZRVKRUHV
  13
    11
 
 VXUSOXV
  13
  7DLZDQÂ·V



,3





93
  






13



11
  ã™«â˜




33
  ãµ€â¼‰â‰¤ã»ã¯ãº²



13



15
  ãœã ™
  13
    33
  LQWUDGHEHWZHHQWKHWZRVKRUHV
  13
    11
 
 VXUSOXV
  13
  7DLZDQÂ·V


,3





93
  






13



11
  ã™«â˜




33
  ãµ€â¼‰â‰¤ã»ã¯ãº²



13



15
  ãœã ™
,3
 ,3
 ,3
 9313


33
  ãµ€â¼‰â‰¤ã»ã¯ãº²


13



15
  ãœã ™
Extracting more rules
binarize head-out


,3





93
  






13



11
  ã™«â˜




33
  ãµ€â¼‰â‰¤ã»ã¯ãº²



13



15
  ãœã ™


,3





,3





,3





93
  




13



11
  ã™«â˜



33
  ãµ€â¼‰â‰¤ã»ã¯ãº²


13



15
  ãœã ™
Extracting more rules
Syntax-Augmented Machine
Translation (Venugopal & Zollmann)
Tree-Sequence Substitution Grammar
(M. Zhang et al., 2008)
  
   




11
  ã™«â˜


,3





93
  






13



13



11
  ã™«â˜




33



33
  ãµ€â¼‰â‰¤ã»ã¯ãº²



13



13



15
  ãœã ™


,3





93
  




133313



133313





13



11
  ã™«â˜




33
  ãµ€â¼‰â‰¤ã»ã¯ãº²



13



15
  ãœã ™
Why is tree-to-tree hard?
13
 116
SRLQWV
11
FKHFN

    116

    11



43





&'
  




,1
  WKDQ


--5
  PRUH
  FKHFN  
 SRLQWV


13

    116



43





&'
  




,1
  WKDQ


--5
  PRUH
too few derivations
Allow more derivations

    116

    11



43





&'
  




,1
  WKDQ


--5
  PRUH
  FKHFN  
 SRLQWV


13

    116



43





&'
  




,1
  WKDQ


--5
  PRUH
âœ¤ STSG: allow only matching
substitutions
âœ¤ Hiero-like: allow any
substitutions
âœ¤ Let the model learn to choose:
âœ¤ matching substitutions
âœ¤ mismatching substitutions
âœ¤ monotone phrase-based


13





116
 
 SRLQWV


11
  FKHFN
Summary of Constituency Syntax in MT
â€¢ CFG capture syntax of some natural languages.
â€¢ Translating while chart parsing.
â€¢ SCFG/STSG parse input
and construct syntactically-parallel output.
â€¢ Hierarchical model: a simplification.
â€¢ Only a single nonterminal used.
â€¢ LM integrated by state splitting.
â€¢ When real source and/or target parse trees are used:
â€¢ Tricks/binarization necessary to extract non-isomorphic trees.
â€¢ Fuzzy matching must be allowed during decoding.
13/43
Dependency Syntax in MT
See MT Talk #11:
http://youtu.be/xauhVtfXbDQ
Constituency vs. Dependency Again
Constituency trees (CFG) represent only bracketing:
= which adjacent constituents are glued tighter to each other.
Dependency trees represent which words depend on which.
+ usually, some agreement/conditioning happens along the edge.
Constituency Dependency
John (loves Mary)
John VP(loves Mary) loves
bbb
"
""
John Mary
SPPPP

NP
John
VP
HHH


V
loves
NP
Mary John loves Mary 14/43
What Dependency Trees Tell Us
Input: The grass around your house should be cut soon.
Google: TrÃ¡vu kolem vaÅ¡eho domu by se mÄ›l snÃ­Å¾it brzy.
Long-distance between grass and cut:
â€¢ Can â€œpumpâ€ many words in between â‡’ phrase limit exceeded.
Two errors caused by independent translation of grass and cut:
â€¢ Bad lexical choice for cut = sekat/snÃ­Å¾it/krÃ¡jet/Å™ezat/â€¦
â€¢ Bad case of trÃ¡va.
â€¢ Depends on the chosen active/passive form:
activeâ‡’accusative passiveâ‡’nominative
trÃ¡vu â€¦ byste se mÄ›l posekat trÃ¡va â€¦ by se mÄ›la posekat
trÃ¡va â€¦ by mÄ›la bÃ½t posekÃ¡na
Examples by ZdenÄ›k Å½abokrtskÃ½, Karel Oliva and others.
15/43
Tree vs. Linear Context
The grass around your house should be cut soon
â€¢ Tree context (neighbours in the dependency tree):
â€¢ is better at predicting lexical choice than ğ‘›-grams.
â€¢ often equals linear context:
Czech manual trees: 50% of edges link neighbours,
80% of edges fit in a 4-gram.
â€¢ Phrase-based MT is a very good approximation.
16/43
Hiero Can Cover Long-Distance DependencytrÃ¡vu
the
grass
around
your
house
should
be
cut
soon
brzy
kolem
vaÅ¡eho
domu
byste
mÄ›l
posekat
the grass X1 should be cut = trÃ¡vu X1 byste mÄ›l posekat 17/43
â€œCrossing Bracketsâ€
â€¢ Constituent outside its fatherâ€™s span causes â€œcrossing brackets.â€
â€¢ Linguists use â€œtracesâ€ ( 1 ) to represent this.
â€¢ Sometimes, this is not visible in the dependency tree:
â€¢ There is no â€œhistory of bracketingâ€.
â€¢ See Holan et al. (1998) for dependency trees including derivation history.
Sâ€™PPPPP

TOPIC
Mary 1
Saaaa
!!!!
NP
John
VP
bb
"
"
V
loves
NP
1
Mary John loves
Despite this shortcoming, CFGs are popular and â€œtheâ€ formal grammar for many. Possibly due to the charm of the
father of linguistics, or due to the abundance of dependency formalisms with no clear winner (Nivre, 2005).
18/43
Non-Projectivity
= a gap in a subtree span, filled by a node higher in the tree.
Ex. Dutch â€œcross-serialâ€ dependencies, a non-projective tree with one
gap caused by saw within the span of swim.
â€¦dat
â€¦that
Jan
John
kinderen
children
zag
saw
zwemmen
swim
â€¦that John saw children swim.
â€¢ 0 gaps â‡’ projective tree â‡’ can be represented in a CFG.
â€¢ â‰¤ 1 gap & â€œwell-nestedâ€ â‡’ mildly context sentitive (TAG).
See Kuhlmann and MÃ¶hl (2007) and Holan et al. (1998). 19/43
Why Non-Projectivity Matters?
â€¢ CFGs cannot handle non-projective constructions:
Think in Dutch that John grass saw being-cut!
20/43
Why Non-Projectivity Matters?
â€¢ CFGs cannot handle non-projective constructions:
Think in Dutch that John grass saw being-cut!
â€¢ No way to glue these crossing dependencies together:
â€¢ Lexical choice:
ğ‘‹ â†’< grass ğ‘‹ being-cut, trÃ¡vu ğ‘‹ sekat >
â€¢ Agreement in gender:
ğ‘‹ â†’< John ğ‘‹ saw, Jan ğ‘‹ vidÄ›l >
ğ‘‹ â†’< Mary ğ‘‹ saw, Marie ğ‘‹ vidÄ›la >
â€¢ Phrasal chunks can memorize fixed sequences containing:
â€¢ the non-projective construction
â€¢ and all the words in between! (â‡’ extreme sparseness)
20/43
Is Non-Projectivity Severe?
In principle:
â€¢ Czech allows long gaps as well as many gaps in a subtree.
Proti odmÃ­tnutÃ­
Against dismissal
se
aux-refl
zÃ­tra
tomorrow
Petr
Peter
v prÃ¡ci
at work
rozhodl
decided
protestovat
to object
Peter decided to object against the dismissal at work tomorrow.
In treebank data:
âŠ– 23% of Czech sentences contain a non-projectivity.
âŠ• 99.5% of Czech sentences are well nested with â‰¤ 1 gap.
21/43
Summary of Dependency Trees
â€¢ More appropriate for Czech (frequent non-projectivity).
â€¢ Exhibit less divergence across languages (Fox, 2002).
â€¢ Dependency context more relevant than adjacency context.
So letâ€™s look if we can apply them in MT.
22/43
Idea: Observe a Pair of Treesâ€¦
# Asociace uvedla , Å¾e domÃ¡cÃ­ poptÃ¡vka v zÃ¡Å™Ã­ stoupla .
# The association said domestic demand grew in September .
23/43
â€¦Decompose into Treeletsâ€¦
# Asociace uvedla , Å¾e domÃ¡cÃ­ poptÃ¡vka v zÃ¡Å™Ã­ stoupla .
# The association said domestic demand grew in September .
24/43
â€¦Collect Dict. of Treelet Pairs
_Predğ‘ğ‘ 
_Sbğ‘ğ‘  uvedla , Å¾e _Predğ‘ğ‘ 
=
_Predğ‘’ğ‘›
_Sbğ‘’ğ‘› said _Predğ‘’ğ‘›
_Sbğ‘ğ‘ 
asociace =
_Sbğ‘’ğ‘›
The association
_Sbğ‘ğ‘ 
_Adjğ‘ğ‘  poptÃ¡vka
=
_Sbğ‘’ğ‘›
_Adjğ‘’ğ‘› demand
â€¦Synchronous Tree Substitution Grammar,
e.g. ÄŒmejrek (2006). 25/43
Transfer at Various Layers
â€¢ My thesis main goal: Transfer at deep-syntactic layer.
â€¢ Implementation to be applicable anywhere with dependency trees.
26/43
Deep Syntax
See MT Talk #14:
http://youtu.be/lJwCW2mFk2M
Going Deeper
â€¢ Motivation for tectogrammatical layer.
â€¢ T-Layer in STSG:
â€¢ Complexity of t-layer attributes.
â€¢ Factorization inevitable, but how to factorize?
â€¢ Empirical evaluation.
â€¢ TectoMT Transfer.
â€¢ Hidden Markov Tree Model.
27/43
Tectogrammatics: Deep Syntax Culminating
Background: Prague Linguistic Circle (since 1926).
Theory: Sgall (1967), PanevovÃ¡ (1980), Sgall et al. (1986).
Materialized theory â€” Treebanks:
â€¢ Czech: PDT 1.0 (2001), PDT 2.0 (2006)
â€¢ Czech-English: PCEDT 1.0 (2004), PCEDT 2.0 (2012)
â€¢ Arabic: PADT (2004)
Practice â€” Tools:
â€¢ parsing Czech to surface: McDonald et al. (2005)
â€¢ parsing Czech to deep: KlimeÅ¡ (2006)
â€¢ parsing English to surface: well studied (+rules convert to dependency trees)
â€¢ parsing English to deep: heuristic rules (manual annotation in progress)
â€¢ generating Czech surface from t-layer: PtÃ¡Äek and Å½abokrtskÃ½ (2006)
28/43
Layers in PDT
29/43
Analytical vs. Tectogrammatical
â€¢ hide auxiliary words, add nodes
for â€œdeletedâ€ participants
â€¢ resolve e.g. active/passive voice,
analytical verbs etc.
â€¢ â€œfullâ€ t-layer resolves much more,
e.g. topic-focus articulation or
anaphora
30/43
Czech and English A-Layer
31/43
Czech and English T-Layer
Predicate-argument structure: changeshould(ACT: someone, PAT: it)
32/43
The Tectogrammatical Hope
Transfer at t-layer should be easier than direct translation:
â€¢ Reduced structure size (auxiliary words disappear).
â€¢ Long-distance dependencies (non-projectivites) solved at t-layer.
â€¢ Word order ignored / interpreted as information structure
(given/new).
â€¢ Reduced vocabulary size (Czech morphological complexity).
â€¢ Czech and English t-trees structurally more similar
â‡’less parallel data might be sufficient (but more monolingual).
â€¢ Ready for fancy t-layer features: co-reference.
33/43
Reminder: STSG
# Asociace uvedla , Å¾e domÃ¡cÃ­ poptÃ¡vka v zÃ¡Å™Ã­ stoupla .
# The association said domestic demand grew in September .
1. Decompose input tree into treelets.
2. Replace treelets with their translations.
3. Join output treelets.
34/43
Reminder: STSG
# Asociace uvedla , Å¾e domÃ¡cÃ­ poptÃ¡vka v zÃ¡Å™Ã­ stoupla .
# The association said domestic demand grew in September .
1. Decompose input tree into treelets.
2. Replace treelets with their translations.
3. Join output treelets.
Real t-nodes have 25 attributes! â‡’ Canâ€™t treat them as atomic.
34/43
Structure vs. Attributes
Factorization = introduction of independence assumptions.
â€¢ STSG factorizes along structure (input into treelets).
â€¢ T-layer requires factorization along attributes.
Which should go first?
â€¢ Treelets of attributes?
â€¢ Similar to phrases of factors, synchronous approach.
â€¢ Can easily fill up stacks with treelets differing too little.
â€¢ Layers of trees?
â€¢ Would be hard to ensure matching tree structure.
â€¢ Rather use a few attributes to construct structure
and postpone the choice of others until the tree is finished.
35/43
Transfer at Various Layers
Layers \ Language Models no LM ğ‘›-gram/binode
epcp, no factors 8.65Â±0.55 10.90Â±0.63
eaca, no factors 6.59Â±0.52 8.75Â±0.61
etct 2009; 43k - 7.39Â±0.52
etca, no factors - 6.30Â±0.57
etct factored, preserving structure 5.31Â±0.53 5.61Â±0.50
eact, source factored, output atomic - 3.03Â±0.32
etct, no factors, all attributes 1.61Â±0.33 2.56Â±0.35
etct, no factors, just t-lemmas 0.67Â±0.19 -
etct 2009: strall + wfwindep. LM rescoring. Formemes (not functors) as frontier labels.
Improved node-to-node alignment (MareÄek et al., 2008). New generation pipeline.
t: a: p: (WMT07 DevTest) 36/43
Reasons of STSG Bad Performance
â€¢ Cumulation of Errors in annotation pipeline.
â€¢ Data Loss due to incompatible structures:
â€¢ Error in parses or word-alignment prevents treelet pair extraction.
â€¢ Combinatorial Explosion of factored output:
â€¢ Abundance of t-node attribute combinations
â‡’ e.g. lexically different translation options pushed off the stack
â‡’ ğ‘›-bestlist varies in unimportant attributes.
â€¢ Too Strong Independence Assumtions:
â€¢ Should never analyze and factorize phrases seen often enough.
â€¢ Complex models hard to tune:
â€¢ More models â‡’ Minimum error rate training has harder time.
37/43
â€œTectoMT Transferâ€ (1/3)         
   
    
     
    
      
 
 
 
 
  
38/43
â€œTectoMT Transferâ€ (2/3)
             
   
     
      
 
 
 
  
 
  
   
    
    !
 "  
     
 
""        
#!
$ 
 
"      
    
"   
 
 " 
   
         
39/43
â€œTectoMT Transferâ€ (3/3)
Slides 6â€“28 by Martin Popel (2009):
â€¢ Illustration of TectoMT transfer.
â€¢ Hidden Markov Tree Model (HMTM).
40/43
Demo Translation â€“ Analysis
Machine translation should be easy.
machine translation should be easy .
NN NN MD VB JJ .
raw text
m-layer
a-layer
Atr
machine
Sb
translation
Pred
should
Obj
be
AuxK
.
Pnom
easy
Demo Translation â€“ Analysis
Machine translation should be easy.
machine translation should be easy .
NN NN MD VB JJ .
raw text
m-layer
a-layer
Atr
machine
Sb
translation
Pred
should
Obj
be
AuxK
.
Pnom
easy
Mark functional words
Demo Translation â€“ Analysis
Machine translation should be easy.
machine translation should be easy .
NN NN MD VB JJ .
raw text
m-layer
a-layer
Atr
machine
Sb
translation
Pred
should
Obj
be
AuxK
.
Pnom
easy
Mark edges to be contracted
Demo Translation â€“ Analysis
Machine translation should be easy.
machine translation should be easy .
NN NN MD VB JJ .
raw text
m-layer
t-layer
machine
translation
be
easy
Build t-tree (backbone)
Demo Translation â€“ Analysis
Machine translation should be easy.
machine translation should be easy .
NN NN MD VB JJ .
raw text
m-layer
t-layer
machine
translation easy
n:attr n:subj
v:fin
adj:compl
Fill formems
be
Demo Translation â€“ Analysis
Machine translation should be easy.
machine translation should be easy .
NN NN MD VB JJ .
raw text
m-layer
t-layer
machine
translation easy
n:attr n:subj
v:fin
adj:compl
Fill grammatemes tense = simple,
modality, conditional
number = singular degree of comparison
= positive
be
Demo Translation â€“ Transfer
source t-layer
machine
translation
be
easy
n:attr
n:subj
v:fin
adj:compl
Build target t-tree by cloning
target t-layer
machine
translation
be
easy
n:attr
n:subj
v:fin
adj:compl
Demo Translation â€“ Transfer
source t-layer
machine
translation
be
easy
n:attr
n:subj
v:fin
adj:compl
Get translation variants
for lemmas and formems
target t-layer
poÄÃ­taÄ
stroj
strojovÃ½
pÅ™eklad
pÅ™evod snadnÃ½
jednoduchÃ½
n:2
n:attr
adj:attr
n:1
v:fin
v:inf
adj:compl
n:1
adv:
bÃ½t
mÃ­t
Demo Translation â€“ Transfer
source t-layer
machine
translation
be
easy
n:attr
n:subj
v:fin
adj:compl
Select the best combination
of lemmas and formems
target t-layer
poÄÃ­taÄ
stroj
strojovÃ½
pÅ™eklad
pÅ™evod snadnÃ½
jednoduchÃ½
n:2
n:attr
adj:attr
n:1
v:fin
v:inf
adj:compl
n:1
adv:
bÃ½t
mÃ­t
Demo Translation â€“ Synthesis
Build target a-layer by cloning
target t-layer
strojovÃ½
pÅ™eklad snadnÃ½
adj:attr
n:1
v:fin
adj:compl
bÃ½t
strojovÃ½
pÅ™eklad snadnÃ½
bÃ½ttarget a-layer
Demo Translation â€“ Synthesis
Fill morphological categories
target t-layer
strojovÃ½
pÅ™eklad snadnÃ½
adj:attr
n:1
v:fin
adj:compl
bÃ½t
strojovÃ½
pÅ™eklad snadnÃ½
bÃ½ttarget a-layer
number = singular
gender = masc. inanim.
case = nominativedegree = positive
degree = positive
Demo Translation â€“ Synthesis
Impose agreement
target t-layer
strojovÃ½
pÅ™eklad snadnÃ½
adj:attr
n:1
v:fin
adj:compl
bÃ½t
strojovÃ½
pÅ™eklad snadnÃ½
bÃ½ttarget a-layer
number = singular
gender = masc. inanim.
case = nominative
number = singular
case = nominative
gender = masc. inanim.
degree = positive
degree = positive
number = singular
case = nominative
gender = masc. inanim.
number = singular
gender = masc. Inanim.
Demo Translation â€“ Synthesis
Add functional words
target t-layer
strojovÃ½
pÅ™eklad snadnÃ½
adj:attr
n:1
v:fin
adj:compl
bÃ½t
strojovÃ½
pÅ™eklad
snadnÃ½
mÃ­ttarget a-layer
bÃ½tby
.
Demo Translation â€“ Synthesis
Reorder clitics
target t-layer
strojovÃ½
pÅ™eklad snadnÃ½
adj:attr
n:1
v:fin
adj:compl
bÃ½t
strojovÃ½
pÅ™eklad
snadnÃ½
mÃ­ttarget a-layer
bÃ½tby
.
Demo Translation â€“ Synthesis
Generate wordforms
target t-layer
strojovÃ½
pÅ™eklad snadnÃ½
adj:attr
n:1
v:fin
adj:compl
bÃ½t
strojovÃ½
pÅ™eklad
snadnÃ½
mÄ›ltarget a-layer
bÃ½tby
.
Demo Translation â€“ Synthesis
Concatenate tokens for output
target t-layer
strojovÃ½
pÅ™eklad snadnÃ½
adj:attr
n:1
v:fin
adj:compl
bÃ½t
strojovÃ½
pÅ™eklad
snadnÃ½
mÄ›ltarget a-layer
bÃ½tby
StrojovÃ½ pÅ™eklad by mÄ›l bÃ½t snadnÃ½.
.
HMTM â€“ Motivation
source t-layer
machine
translation
be
easy
n:attr
n:subj
v:fin
adj:compl
Select the best label
for each node
target t-layer
poÄÃ­taÄ|n:2,
poÄÃ­taÄ|n:attr,
strojovÃ½|adj:attr, ...
pÅ™eklad|n:1,
pÅ™evod|n:1
bÃ½t|v:fin, bÃ½t|v:inf,
mÃ­t|v:fin, mÃ­t|v:inf
snadnÃ½|adj:compl,
jednoduchÃ½|adj:compl, ...
Hidden Markov Tree Modelmachine engine
translation arcade
be have
easy simple
strojovÃ½
pÅ™eklad
bÃ½t
snadnÃ½
ROOT
PE (strojovÃ½ | engine) = 0.5
P E(strojovÃ½ | machine) = 0.4
PE (pÅ™eklad | translation) = 0.6
PE (pÅ™eklad | arcade) = 0.7
1Ã—10 -8
P T(machine | translation) = 0.02
1Ã—10 -8
1Ã—10-10
0.002
0.001
0.01
P E (bÃ½t | be) = 0.8
P E (bÃ½t | have) = 0.01
1Ã—10 -8
Source tree (Czech) Target tree (English)
ANALYSIS
TRANSFER
SYNTHESIS
ROOT
Source sentence:
StrojovÃ½ pÅ™eklad by mÄ›l bÃ½t snadnÃ½.
Target sentence:
Machine translation should be easy.
P(optimal_tree) = P E (strojovÃ½ | machine) Â· P T (machine | translation)Â·
P E (pÅ™eklad | translation) Â· P T (translation | be)Â·
P E (snadnÃ½ | easy) Â· P T (easy | be)Â·
P E (bÃ½t | be) Â· P T (be | ROOT)
0.0001
41/43
Summary
â€¢ Dependency trees linguistically more promising.
â€¢ Tree context vs. linear context. Non-projectivity. T-layer.
â€¢ STSG to transfer dependency trees:
â€¢ Severe issues of sparseness, i.a. due to missing adjunction.
â€¢ TectoMT system with HMTM transfer.
Rich annotation hurts if not backed-off.
â€¢ Due to sparser data (incompatible trees).
â€¢ Due to cummulation of errors.
â€¢ Due to too strong independence assumptions.
â€¢ Due to harder optimization problem for MERT.
42/43
References
Michael Auli, Adam Lopez, Hieu Hoang, and Philipp Koehn. 2009. A systematic analysis of translation model search
spaces. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 224â€“232, Athens, Greece,
March. Association for Computational Linguistics.
David Chiang. 2005. A Hierarchical Phrase-Based Model for Statistical Machine Translation. In Proceedings of the
43rd Annual Meeting of the Association for Computational Linguistics (ACLâ€™05), pages 263â€“270, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
David Chiang. 2010. Learning to translate with source and target syntax. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics, pages 1443â€“1452, Uppsala, Sweden, July. Association for
Computational Linguistics.
Martin ÄŒmejrek. 2006. Using Dependency Tree Structure for Czech-English Machine Translation. Ph.D. thesis, ÃšFAL,
MFF UK, Prague, Czech Republic.
Heidi J. Fox. 2002. Phrasal cohesion and statistical machine translation. In EMNLP â€™02: Proceedings of the ACL-02
conference on Empirical methods in natural language processing, pages 304â€“3111. Association for Computational
Linguistics.
TomÃ¡Å¡ Holan, Vladislav KuboÅˆ, Karel Oliva, and Martin PlÃ¡tek. 1998. Two Useful Measures of Word Order
Complexity. In A. Polguere and S. Kahane, editors, Proceedings of the Coling â€™98 Workshop: Processing of
Dependency-Based Grammars, Montreal. University of Montreal.
VÃ¡clav KlimeÅ¡. 2006. Analytical and Tectogrammatical Analysis of a Natural Language. Ph.D. thesis, ÃšFAL, MFF UK,
Prague, Czech Republic.
Marco Kuhlmann and Mathias MÃ¶hl. 2007. Mildly context-sensitive dependency languages. In Proceedings of the 45th
Annual Meeting of the Association of Computational Linguistics, pages 160â€“167, Prague, Czech Republic, June.
Association for Computational Linguistics. 43/43


Transformer and
Syntax in NMT
OndÅ™ej Bojar
April 16, 2020
NPFL087 Statistical Machine Translation
Charles University
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics unless otherwise stated
Overview
â€¢ Reminder: Seq2seq with Attention.
â€¢ Transformer Architecture.
â€¢ Focus on Self-Attention.
â€¢ Explicit Syntax in NMT.
â€¢ In Network Structure.
â€¢ At Each Token.
â€¢ In Attention.
Some images due to JindÅ™ich Helcl and/or JindÅ™ich LibovickÃ½.
1/43
Reminder: Seq2seq with Attention<s> x 1 x 2 x 3 x 4
~y i ~y i+1
h 1h 0 h 2 h 3 h 4
...
+
Ã—
Î± 0
Ã—
Î± 1
Ã—
Î± 2
Ã—
Î± 3
Ã—
Î± 4
s is i-1 s i+1
+
2/43
Attention â€“ Formal Notation
Inputs:
decoder state ğ‘ ğ‘–
encoder states â„ğ‘— = [âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—â„ğ‘—;âƒ–âƒ–âƒ–âƒ–âƒ–âƒ–âƒ–âƒ–âƒ– â„ğ‘—] âˆ€ğ‘– = 1 â€¦ ğ‘‡ğ‘¥
whereâƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ— â„ğ‘— = RNNenc(â„ğ‘—âˆ’1, ğ‘¥ğ‘—) = tanh(ğ‘ˆğ‘’âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ— â„ğ‘—âˆ’1 + ğ‘Šğ‘’ğ¸ğ‘’ğ‘¥ğ‘— + ğ‘ğ‘’)
Attention energies: ğ‘’ğ‘–ğ‘— = ğ‘£âŠ¤
ğ‘ tanh (ğ‘Šğ‘ğ‘ ğ‘–âˆ’1 + ğ‘ˆğ‘â„ğ‘— + ğ‘ğ‘)
Attention distribution: ğ›¼ğ‘–ğ‘— = exp(ğ‘’ğ‘–ğ‘—)
âˆ‘ğ‘‡ğ‘¥
ğ‘˜=1 exp(ğ‘’ğ‘–ğ‘˜)
Context vector: ğ‘ğ‘– = âˆ‘ğ‘‡ğ‘¥
ğ‘—=1 ğ›¼ğ‘–ğ‘—â„ğ‘—
3/43
Attention Mechanism in Equations (2)
Decoder state:
ğ‘ ğ‘– = tanh(ğ‘ˆğ‘‘ğ‘ ğ‘–âˆ’1 + ğ‘Šğ‘‘ğ¸ğ‘‘Ì‚ ğ‘¦ğ‘–âˆ’1 + ğ¶ğ‘ğ‘– + ğ‘ğ‘‘)
Output projection:
ğ‘¡ğ‘– = tanh (ğ‘ˆğ‘œğ‘ ğ‘– + ğ‘Šğ‘œğ¸ğ‘‘Ì‚ ğ‘¦ğ‘–âˆ’1 + ğ¶ğ‘œğ‘ğ‘– + ğ‘ğ‘œ)
â€¦context vector is mixed with the hidden state
Output distribution:
ğ‘ (ğ‘¦ğ‘– = ğ‘˜ |ğ‘ ğ‘–, ğ‘¦ğ‘–âˆ’1, ğ‘ğ‘–) âˆ exp (ğ‘Šğ‘œğ‘¡ğ‘–)ğ‘˜ + ğ‘ğ‘˜
4/43
Transformer
Attention is All You Need (Vaswani et al., 2017)
5/43
Transformer Detailed Walkthroughs
Transformer Illustrated:
â€¢ http://jalammar.github.io/illustrated-transformer/
Amazingly simple description! (I am reusing the pictures.)
Transformer paper annotated with PyTorch code:
â€¢ http://nlp.seas.harvard.edu/2018/04/03/attention.html
â€¢ PyTorch by examples:
https://github.com/jcjohnson/pytorch-examples
Summary at Medium:
â€¢ https://medium.com/@adityathiruvengadam/
transformer-architecture-attention-is-all-you-need-aeccd
6/43
Transformer = 6 Layers Enc + 6 Dec
7/43
Composition of One Layer
8/43
Word Vectors in Encoder
9/43
FF Is Actually Position-Independent
10/43
Positional Encoding
â€¢ Encode token position directly in the word vector:
â€¢ Positional embedding can be random, or â€œfrequency-likeâ€:
11/43
Self-Attention
Self-Attention Motivation (1/2)
â€¢ Sequences of arbitrary length ğ‘› need to be processed.
â€¢ RNNs make the (time-unrolled) network as deep as ğ‘›.
â€¢ CNNs allow to trade kernel size ğ‘˜ and depth for a target â€œreceptive
fieldâ€:
embeddings x = (ğ‘¥1, â€¦ , ğ‘¥ğ‘ )ğ‘¥0 =âƒ— 0 ğ‘¥ğ‘› =âƒ— 0
12/43
Self-Attention Motivation (2/2)
â€¢ SANs (Self-Attentive Networks) can access any position in
constant time.
Operations Sequential Steps Memory
Recurrent ğ‘‚(ğ‘› â‹… ğ‘‘2) ğ‘‚(ğ‘›) ğ‘‚(ğ‘› â‹… ğ‘‘)
Convolutional ğ‘‚(ğ‘˜ â‹… ğ‘› â‹… ğ‘‘2) ğ‘‚(1) ğ‘‚(ğ‘› â‹… ğ‘‘)
Self-attentive ğ‘‚(ğ‘›2 â‹… ğ‘‘) ğ‘‚(1) ğ‘‚(ğ‘›2 â‹… ğ‘‘)
â€¢ Sequence length ğ‘›, state dimensionality ğ‘‘, kernel size ğ‘˜.
â€¢ Assuming infinitely many GPU cores (or rather ALU),
operations can be run in parallel,
but may depend on each other, needing some Sequential Steps.
13/43
Self-Attention
â€¢ Goal: Aggregate arbitary-length input to fixed-size vector.
Goal: Allow data-driven, trainable aggregation.
14/43
Self-Attention
â€¢ Goal: Aggregate arbitary-length input to fixed-size vector.
Goal: Allow data-driven, trainable aggregation.
Given the sequence of inputs ğ‘¥1, â€¦ , ğ‘¥ğ‘›:
â€¢ Create three â€œviewsâ€ of
them: queries, keys,
values.
â€¢ Using trained matrices
ğ‘Š ğ‘„, ğ‘Š ğ¾, ğ‘Š ğ‘‰ .
14/43
Match All Queries with All Keys
15/43
Normalize Scores
16/43
Aggregate Values Accordingly
17/43
Self-Attention as Matrix Calculation
18/43
Multi-Head Attention
19/43
Multi-Head Attention
20/43
Self-Attention Summary
21/43
Self-Attention in Transformer
Three uses of multi-head attention in Transformer
â€¢ Encoder-Decoder Attention:
â€¢ Q: previous decoder layers; K = V: outputs of encoder
â‡’ Decoder positions attend to all positions of the input.
â€¢ Encoder Self-Attention:
â€¢ Q = K = V: outputs of the previous layer of the encoder
â‡’ Encoder positions attend to all positions of previous layer.
â€¢ Decoder Self-Attention:
â€¢ Q = K = V: outputs of the previous decoder layer.
â€¢ Masking used to prevent depending on future outputs.
â‡’ Decoder attends to all its previous outputs.
22/43
Self-Attention at Enc Layer #5: 1 Head
23/43
Self-Attention at Enc Layer #5: 2 Heads
24/43
Self-Attention at Enc Layer #5: 8 Heads
25/43
Explicit Linguistic
Information in NMT
Ways of Adding Linguistic Annotation
â€¢ Construct network structure along linguistic structure.
âˆ¼ What we discussed in Syntax in SMT.
â€¢ Tree-LSTMs.
â€¢ Graph-Convolutional Networks.
â€¦ Source information only.
â€¢ Enrich information at each token.
âˆ¼ What we discussed in Morphology in SMT.
â€¢ Factors on the source side.
â€¢ Multi-Task on the target side.
â€¢ Improve attention using linguistic annotation.
â€¢ Attention calculation respecting syntax.
â€¢ Attention forced to reflect syntax in multi-task.
26/43
Linguistics in NN Structure
Tree-LSTMs (Tai et al., 2015)
Memory comes from:
the single predecessor childrenx 1 x 2 x 3 x 4
y 1 y 2 y 3 y 4x 1
x 2
x 4 x 5 x 6
y 1
y 2 y 3
y 4 y 6
plain LSTM Tree-LSTM
â€¢ Two flavors:
â€¢ Dependency trees: Sum over all children.
â€¢ Constituency trees: Up to N children, respecting order. 27/43
Chen et al. (2017a) (1/2)x1 x2 x3 x4 x5 x6
âˆ’â†’
h 1
âˆ’â†’
h 2
âˆ’â†’
h 3
âˆ’â†’
h 4
âˆ’â†’
h 5
âˆ’â†’
h 6
âˆ’â†’
h 1
â†âˆ’
h 2
â†âˆ’
h 3
â†âˆ’
h 4
â†âˆ’
h 5
â†âˆ’
h 6
h â†‘
7
hâ†‘
8
hâ†‘
9
hâ†‘
10
h â†‘
11
Tree-GRU Encoder:
â€¢ Constituency syntax of the tree provides additional states. 28/43
Chen et al. (2017a) (2/2)x1 x2 x3 x4 x5 x6
h â†‘
1 hâ†‘
2 h â†‘
3 hâ†‘
4 h â†‘
5 hâ†‘
6hâ†“
1 h â†“
2 hâ†“
3 hâ†“
4 h â†“
5 h â†“
6
h â†‘
7 hâ†“
7
h â†‘
8 h â†“
8
hâ†‘
9 h â†“
9
hâ†‘
10 h â†“
10
h â†‘
11 h â†“
11
Bidirectional tree encoder.
â€¢ Can be seen as many RNNs running from each word up to the root
and back to the word. 29/43
Graph-Convolutional Networks (Bastings et al., 2017)W ( 0)
det
W ( 0)
nsubj
W ( 0)
dobj
W ( 0)
det
W ( 1)
det
W ( 1)
nsubj
W ( 1)
dobj
W ( 1)
det
*PAD* The monkey eats a banana *PAD*
h (0)
h (1)
h (2)
GCNCNN
Figure 2: A 2-layer syntactic GCN on top of a convolutional encoder. Loop connections are depicted
with dashed edges, syntactic ones with solid (dependents to heads) and dotted (heads to dependents)
edges. Gates and some labels are omitted for clarity.
30/43
Linguistics at Each Token
CCGs to Encode Syntax at Each Token
Syntax reflects long-distance dependencies.
â€¢ What city is the Taj Mahal in?
â€¢ Where is the Taj Mahal âˆ…?
The need to produce in depends on the What/Where.
â€¢ CCG tags for is differ â‡’ dependency highlighted.
â€¢ Following CCG tags, the decoder can know if in is needed.
â€¢ CCG tags are denser than words â‡’ better generalization.
31/43
CCGs to Encode Syntax at Each Token
Syntax reflects long-distance dependencies.
â€¢ What city is the Taj Mahal in?
â€¢ Where is the Taj Mahal âˆ…?
The need to produce in depends on the What/Where.
â€¢ CCG tags for is differ â‡’ dependency highlighted.
â€¢ Following CCG tags, the decoder can know if in is needed.
â€¢ CCG tags are denser than words â‡’ better generalization.
â€¢ What(S[wq]/(S[q]/NP))/N city is(S[q]/P P)/NP the Taj Mahal in?
â€¢ WhereS[wq]/(S[q]/NP) is(S[q]/NP)/NP the Taj Mahal?
31/43
Factors in NMT
â€¢ Source word factors easy to incorporate:
â€¢ Concatenate embeddings of the various factors.
â€¢ POS tags, morph. features, source dependency labels help enâ†”de and
enâ†’ro (Sennrich and Haddow, 2016).
â€¢ Target word factors:
â€¢ Interleave for morphology: (Tamchyna et al., 2017)
Src there are a million different kinds of pizza .
Baseline (BPE) existujÃ­ miliony druhÅ¯ piz@@ zy .
Interleave VB3P existovat NNIP1 milion NNIP2 druh NNFS2 pizza Z: .
â€¢ Interleave for syntax: (Nadejde et al., 2017)
Src BPE Obama receives Net+ an+ yahu in the capital of USA
Tgt NP Obama ((S[dcl]\NP)/PP)/NP receives NP Net+ an+ yahu PP/NP in NP/N the N cap
â€¢ Multiple decoders, each predicting own sequence.
32/43
Predicting Target Syntax
My students Dan Kondratyuk and Ronald Cardenas retried Nadejde et
al. (2017) with:
â€¢ sequence-to-sequence model,
â€¢ Transformer model.
Predicting target syntax using:
â€¢ a secondary decoder
â€¢ interleaving.
As tags, they used:
â€¢ correct CCG tags, â€¢ random tags, â€¢ a single dummy tag.
(Kondratyuk et al. Replacing Linguists with Dummies. PBML 2019.)
33/43
Predicting Target Syntax (S2S)Training steps (millions)
Baseline CCG Random Same
0 2 4 6 8 10 12 14 16 18
0
5
10
15
20
25
Interleaved
Seq2seqTraining steps (millions)
Baseline CCG Random Same
Multi-Decoder
0 2 4 6 8 10 12 14 16 18
0
5
10
15
20
25
Seq2seq
34/43
Predicting Target Syntax (Transformer)Training steps (millions)
Baseline CCG Random Same
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28
0
5
10
15
20
25
30
Interleaved
TransformerTraining steps (millions)
Baseline CCG Random Same
Multi-Decoder
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28
0
5
10
15
20
25
Transformer
35/43
Syntax in Word Embeddings Chen et al. (2017b)SrcdepSrcU2=<x3, x1, x4, x7,Îµ>Dep TuplesU1EncoderVx1VU1h1Vx2VU2h2VxJVUJhJâ€¦â€¦â€¦DecoderUJCNNCNNCNNCNNx1x2x3x4x5x6x7rootâ€¦,1i,2iâ€¦,i J
â€¢ CNN-derived embeddings of nodesâ€™ syntactic neighbourhood
included: (parent, siblings).
â€¢ Two mechanisms:
â€¢ Concatenated to standard embeddings.
â€¢ Separate attention over these word-level annotiations
36/43
Linguistics in Attention
Tree Coverage in Attention Chen et al. (2017a)
Tree coverage model:
â€¢ Attention coverage depends on source syntax.
â€¢ Without it (left), output is repeated. 37/43
Multi-Task to Request Dependency Tree (1/2)
â€¢ Pham et al. (2019) noticed that
attention head could be interpreted as
dependency parse.
â€¢ Add secondary objective to require
head #1 to match source dependency
tree.
38/43
Multi-Task to Request Dependency Tree (2/2)
â€¢ Czech-to-English translation (BLEU).
â€¢ Czech dependency parse from head #1 (UAS).
BLEU UAS
Dev Test Dev Test
Transformer Baseline 37.28 36.66 â€“ â€“
Parse from layer 0 36.95 36.60 81.39 82.85
Parse from layer 1 38.51 38.01 90.17 90.78
Parse from layer 2 38.50 37.87 91.31 91.18
Parse from layer 3 38.37 37.67 91.43 91.43
Parse from layer 4 37.86 37.60 91.65 91.56
Parse from layer 5 37.63 37.67 91.44 91.46
39/43
Dummy Dependency Tree
I shot an elephant in my pajamas
I
shot
an
elephant
in
my
pajamas
ROOT
I shot
an
elephant
inmy
pajamas
40/43
Multi-Task to Request Dummy Tree
BLEU Precision
Dev Test Dev Test
Transformer Baseline 37.28 36.66 â€“ â€“
Dummy Parse from layer 0 38.68 38.14 99.97 99.96
Dummy Parse from layer 1 39.11 38.06 99.99 99.99
Dummy Parse from layer 2 37.85 37.85 99.98 99.98
Dummy Parse from layer 3 37.93 37.70 99.97 99.98
Dummy Parse from layer 4 37.68 37.47 99.98 99.96
Dummy Parse from layer 5 37.53 37.54 99.96 99.95
True Parse from layer 1 38.51 38.01 90.17 90.78
41/43
Summary
â€¢ Transformer is a great replacement for RNN.
â€¢ Constant-time processing.
â€¢ (CNNs can be comparable, but Gehring et al. (2016) was kind of missed.)
â€¢ Explicit syntax can be useful.
â€¢ Many options how to include it.
â€¢ Some gains hard to reproduce.
â€¢ Dummy information can be equally useful.
â€¢ Transformer seems to learn syntax for free.
42/43
References
Joost Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, and Khalil Simaan. 2017. Graph convolutional encoders
for syntax-aware neural machine translation. In Proceedings of the 2017 Conference on Empirical Methods in Natural
Language Processing, pages 1947â€“1957. Association for Computational Linguistics.
Huadong Chen, Shujian Huang, David Chiang, and Jiajun Chen. 2017a. Improved Neural Machine Translation with a
Syntax-Aware Encoder and Decoder. In Proceedings of the 55th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1936â€“1945, Vancouver, Canada, July. Association for Computational
Linguistics.
Kehai Chen, Rui Wang, Masao Utiyama, Lemao Liu, Akihiro Tamura, Eiichiro Sumita, and Tiejun Zhao. 2017b. Neural
Machine Translation with Source Dependency Representation. In Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing, pages 2846â€“2852, Copenhagen, Denmark, September. Association for
Computational Linguistics.
Jonas Gehring, Michael Auli, David Grangier, and Yann N. Dauphin. 2016. A convolutional encoder model for neural
machine translation. CoRR, abs/1611.02344.
Maria Nadejde, Siva Reddy, Rico Sennrich, Tomasz Dwojak, Marcin Junczys-Dowmunt, Philipp Koehn, and Alexandra
Birch. 2017. Predicting target language ccg supertags improves neural machine translation. In Proceedings of the
Second Conference on Machine Translation, Volume 1: Research Paper, pages 68â€“79, Copenhagen, Denmark,
September. Association for Computational Linguistics.
Thuong-Hai Pham, Dominik MachÃ¡Äek, and OndÅ™ej Bojar. 2019. Promoting the knowledge of source syntax in
transformer nmt is not needed. ComputaciÃ³n y Sistemas, 23(3):923â€“934.
Rico Sennrich and Barry Haddow. 2016. Linguistic input features improve neural machine translation. In Proceedings
of the First Conference on Machine Translation, pages 83â€“91, Berlin, Germany, August. Association for Computational
Linguistics. 43/43


Does MT Understand?
Word and Sentence Representations
OndÅ™ej Bojar
April 23, 2020
NPFL087 Statistical Machine Translation
Charles University
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics unless otherwise stated
Outline
â€¢ Introducing Semiotics.
â€¢ Do Current MT Systems Understand?
â€¢ Continuous Representations.
â€¢ What are Good Representations?
â€¢ Continuous Word Representations.
â€¢ Continuous Sentence Representations.
â€¢ Aspects of Meaning.
â€¢ Evaluating Sentence Representations
â€¢ How Meaningful is Seq2Seq Representation?
1/86
2/86
Semiotic TriangleThought or Reference
Symbol Referent
Correct symbol symbolises
Adequate thought refers to
True symbol stands for
Semiotic Triangle by Ogden and Richards (1923). 3/86
Semiotic TriangleThought or Reference
Symbol Referent
Correct symbol symbolises
Adequate thought refers to
True symbol stands for
Danny
approached
the chair with
a yellow bag.
Ambiguous sentenceâ€¦ 4/86
Semiotic TriangleThought or Reference
Symbol Referent
Correct symbol symbolises
Adequate thought refers to
True symbol stands for
Danny
approached
the chair with
a yellow bag.
Ambiguous sentence correspond to two situations. LavaCorpus (Berzak et al., 2015) 5/86
Semiotic TriangleThought or Reference
Symbol Referent
Correct symbol symbolises
Adequate thought refers to
True symbol stands for
Danny
approached
the chair with
a yellow bag.
Syntactic â€œmeaningâ€ distinguishes this already. 6/86
Semiotic TriangleThought or Reference
Symbol Referent
Correct symbol symbolises
Adequate thought refers to
True symbol stands for
Danny
approached
the chair with
a yellow bag.
Î»p.Î»c.Î»b.person(p)
âˆ§chair(c)âˆ§bag(b)
âˆ§yellow(b)âˆ§has(p,b)
âˆ§approach(p,c)
Î»p.Î»c.Î»b.person(p)
âˆ§chair(c)âˆ§bag(b)
âˆ§yellow(b)âˆ§has(c,b)
âˆ§approach(p,c)
Lambda calculus makes the difference clear. 7/86
Semiotic TriangleThought or Reference
Symbol Referent
Correct symbol symbolises
Adequate thought refers to
True symbol stands for
Danny
approached
the chair with
a yellow bag.
NN activations when processing the videos will somehow differ, too. 8/86
Why is Meaning Important in MT
Translation = expressing the same meaning in another language.
A meaning-aware translator (human or machine) will:
1. Use context to disambiguate as much as possible.
2. Ask around to learn about and understand the situation described.
3. Ideally warn the audience about unresolved ambiguities.
9/86
Recent Performance of NMT on News
Seg-Level Englishâ†’Czech 2018
Ave. % Ave. z System
1 84.4 0.667 CUNI-Transformer
2 79.8 0.521 uedin
78.6 0.483 Professional Translation
4 68.1 0.128 online-B
5 59.4 âˆ’0.178 online-A
6 54.1 âˆ’0.354 online-G
Doc-Aware Englishâ†’German 2019
Ave. Ave. z System
90.3 0.347 Facebook-FAIR
93.0 0.311 Microsoft-WMT19-sent-doc
92.6 0.296 Microsoft-WMT19-doc-level
90.3 0.240 Professional Translation
87.6 0.214 MSRA-MADL
88.7 0.213 UCAM
89.6 0.208 NEU
87.5 0.189 MLLP-UPV
87.5 0.130 eTranslation
86.8 0.119 dfki-nmt
84.2 0.094 online-B
â€¦ 10 more systems here â€¦
76.3 âˆ’0.400 online-X
43.3 âˆ’1.769 en-de-task
See lecture #1 for all caveats of MT evaluation.
10/86
Do Recent Best Systems Understand?
â€¢ NMT systems are trained on millions of documents.
â€¢ To read the source and target training data of CUNI-Transformer
you would need 50 years, 8 hours a day, no weekends.
(Only 40% of it was parallel.)
â€¢ NNs create internal representations.
â€¦ So perhaps these representations are meaningful?
Test it yourself (Englishâ†”Czech):
https://lindat.mff.cuni.cz/services/transformer/
11/86
Do Recent Best Systems Understand?
12/86
Do Recent Best Systems Understand?
13/86
Do Recent Best Systems Understand?
14/86
Do Recent Best Systems Understand?
15/86
Do Recent Best Systems Understand?
16/86
Representations
Defining Representations
Given:
â€¢ a neural network trained to predictÌ‚ ğ‘¦ğ‘– âˆˆ ğ’´ given ğ‘¥ğ‘– âˆˆ ğ’³,
â€¢ and a cut ğ¶ of that network
â€¢ (a set of neurons s.t. every path from input to output has to intersect it),
a representation is the mapping from ğ’³ to â„‹, where
â€¢ â„‹ is the vector space of observed activations of neurons in ğ¶
(in some arbitrary fixed order).
17/86
Two Cuts Here: (1) Input, (2) Hidden Layer
18/86
The Learned Representation
Original space allows to:
â€¢ plot input data,
â€¢ visualize separation boundaries
for the first as well as
subsequent layers.
Hidden space â„‹ allows to:
â€¢ to linearly separate the classes.
â€¦ but is it good for anything
else?
19/86
Good Representations (1/2)
20/86
Good Representations (1/2)
(ğ‘, ğ‘, ğ‘) is a good representation
because it separates border from face features
20/86
Good Representations (2/2)
21/86
Good Representations (2/2)
(ğ‘, ğ‘, ğ‘) is a good representation
because it resembles a known picture
21/86
Which Representations Are Good?
Ideas for a start:
â€¢ Good representations allow to solve some main task.
â€¢ â€¦ but for this, the NN was trained in the first place.
â€¢ Good representations allow to solve some other task.
â€¢ Pretrained word embeddings may be useful for other tasks.
â€¢ Good representations serve well on task interfaces.
â€¢ Divide-and-conquer vs. end-to-end training.
â€¢ Must divide training to make use of different data sources
(e.g. spoken language translation needs ASR and MT data).
â€¢ Good representations â€œmake senseâ€.
â€¢ The representation of a specific test set resembles something known.
â€¢ Attaching a single layer to the representation gives a good accuracy in
something, e.g. part-of-speech tags from sentence embeddings.
22/86
Word Representations
Word Embeddings
â€¢ Map each word to a dense vector.
â€¢ In practice 300â€“2000 dimensions are used.
â€¢ The dimensions have no clear interpretation.
â€¢ Embeddings are trained for each particular task.
â€¢ NNs: The matrix that maps 1-hot input to the first layer.
â€¢ The famous word2vec (Mikolov et al., 2013):
â€¢ CBOW: Predict the word from its four neighbours.
â€¢ Skip-gram: Predict likely neighbours given the word.Input layerHidden layerOutput layerx1x2x3xkxVy1y2y3yjyVh1h2hihNWVÃ—N={wki}W'NÃ—V={w'ij}
Right: CBOW with just a single-word context (http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf)
23/86
Emergent Continuous Space of Words
Word2vec embeddings show interesting properties:
ğ‘£(king) âˆ’ ğ‘£(man) + ğ‘£(woman) â‰ˆ ğ‘£(queen) (1)
Illustrations from https://www.tensorflow.org/tutorials/word2vec 24/86
Testset by Mikolov et al. (2013)
Question Type Sample Pair
capital-countries Athens â€“ Greece
capital-world Abuja â€“ Nigeria
currency Algeria â€“ dinar
city-in-state Houston â€“ Texas
family boy â€“ girl
adjective-to-adverb calm â€“ calmly
opposite aware â€“ unaware
comparative bad â€“ worse
superlative bad â€“ worst
present-participle code â€“ coding
nationality-adjective Albania â€“ Albanian
past-tense dancing â€“ danced
plural banana â€“ bananas
plural-verbs decrease â€“ decreases
25/86
Caveat on Evaluation (1/2)
Consider word2vec â€œcomprehensiveâ€ test set (Mikolov et al., 2013):
â€¢ 8.8k â€œsemanticâ€ and 10.6k â€œsyntacticâ€ questions,
â€¢ w2v â€œaccuracy is quite goodâ€ (eyeballing)
â€¢ The authors do mention that exact-match is â€œonly about 60%â€).
Kocmi and Bojar (2016) carefully examined the test set:
â€¢ â€œSemanticâ€ questions cover only 3 question types:
â€¢ countryâ†’city, countryâ†’currency, masculine family memberâ†’ feminine
â€¢ Vylomova et al. (2016) test many other relations, e.g. walk-run,
dog-puppy, bark-dog, cook-eat.
â€¢ â€œSyntacticâ€ questions constructed by combinations:
â€¢ starting from only 313 distinct word pairs,
â€¢ (leading to only 35 different pairs per question on average),
â€¢ And of the 313 pairs, 286 are formed regularly.
26/86
Caveat on Evaluation (2/2)
Test Set by
Accuracy on â€œSynt Questionsâ€ Mikolov et al. Kocmi et al.
word2vec as released 62.5% 43.5%
27/86
Caveat on Evaluation (2/2)
Test Set by
Accuracy on â€œSynt Questionsâ€ Mikolov et al. Kocmi et al.
word2vec as released 62.5% 43.5%
word2vec trained on our data 42.5% 9.7%
SubGram trained on our data 42.3% 22.4%
27/86
Caveat on Evaluation (2/2)
Test Set by
Accuracy on â€œSynt Questionsâ€ Mikolov et al. Kocmi et al.
word2vec as released 62.5% 43.5%
word2vec trained on our data 42.5% 9.7%
SubGram trained on our data 42.3% 22.4%
Nine rules 71.9% 66.4%
27/86
Caveat on Ultimate Evaluation
Kocmi and Bojar (2016):
â€¢ submitted to TSD on March 22, 2016.
â€¢ appeared in TSD in September 2016.
â€¦ cited by 7.
Bojanowski et al. (2017):
â€¢ submitted to arxiv on July 15, 2016.
â€¢ appeared in TACL 2017.
â€¦ cited by 2994.
28/86
Caveat on Ultimate Evaluation
Kocmi and Bojar (2016):
â€¢ submitted to TSD on March 22, 2016.
â€¢ appeared in TSD in September 2016.
â€¦ cited by 7.
â€¢ No code released, no fast code implemented at all.
Bojanowski et al. (2017):
â€¢ submitted to arxiv on July 15, 2016.
â€¢ appeared in TACL 2017.
â€¦ cited by 2994.
â€¢ This is the FastText paper.
28/86
Evaluating Words against Human Assessment?
The whole idea of evaluating word vectors by relating to human
judgements is risky.
â€¢ Human-produced datasets are subjective.
â€¢ Similarity vs. relatedness.
â€¢ Relatedness: teacher â‰ˆ student, coffee â‰ˆ cup
â€¢ Similarity: teacher â‰ˆ professor, car â‰ˆ train
â€¢ Hill et al. (2017) observed a soft tendency:
â€¢ Monolingual models reflect non-specific relatedness,
â€¢ NMT models reflect conceptual similarity.
â€¢ We saw that too for English-Czech (Abdou et al., 2017).
â€¢ Even if we distinguish them, which should be reflected in embeddings?
Details: Faruqui et al. (2016); Survey of eval. methods: Bakarov (2018)
29/86
Sentence Representations
Encoder-Decoder Architecture
https://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-2/ 30/86
Continuous Space of Sentencesâˆ’15 âˆ’10 âˆ’5 0 5 10 15 20
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
5
10
15
I gave her a card in the garden
In the garden , I gave her a card
She was given a card by me in the garden
She gave me a card in the garden
In the garden , she gave me a card
I was given a card by her in the garden
2-D PCA projection of 8000-D space representing sentences (Sutskever et al., 2014). 31/86
Fixed-Length Representation of Sentences??
Raymond Mooney:
You canâ€™t cram the meaning of
a whole %&!$ing sentence into a single $&!*ing vector!
32/86
Aspects of Meaning
Aspects of Meaning (1/2)
â€¢ Meaning can be seen as a coarsening:
â€¢ Pictures: Semantic segmentation.
33/86
Aspects of Meaning (1/2)
â€¢ Meaning can be seen as a coarsening:
â€¢ Pictures: Semantic segmentation.
â€¢ Programs: The output they give (caveat: undecidable).
â€¢ Sentences: Reference to real world? Speakerâ€™s intention?
33/86
Aspects of Meaning (1/2)
â€¢ Meaning can be seen as a coarsening:
â€¢ Pictures: Semantic segmentation.
â€¢ Programs: The output they give (caveat: undecidable).
â€¢ Sentences: Reference to real world? Speakerâ€™s intention?
â€¢ Linguistic meaning captures the structure of expressions:
â€¢ Morphology, syntax, â€¦
â€¢ Units of each layer composed into higher units (FGD, Sgall et al. (1986))
Illustration from http://www.cs.toronto.edu/~tingwuwang/semantic_segmentation.pdf.
33/86
Aspects of Meaning (2/2)
Aspects of sentence meaning as listed by Bojar et al. (2019).
Symbolic Continuous
Aspect of Meaning Theories Representations
Abstraction 3 Ã—
Compositionality 3 âˆ¼
Learnability ? 3
Relatability (similarity, operations) âˆ¼ âˆ¼
Vagueness of Meaning Ã— 3
Ambiguity of Expressions 3 Ã—
Statefulness âˆ¼ 3
34/86
Compositionality of Meaning
Manning (2015):
Understanding novel and complex sentences crucially de-
pends on being able to construct their meaning compositionally
from smaller partsâ€”words and multiword expressionsâ€”of which
they are constituted.
35/86
Compositionality of Vector Representations
Karlgren and Kanerva (2019) show â€œHolographic Reduced Reprs.â€:
â€¢ Addition: Preserves similarity, useful to represent bag-of-â€¦
â€¢ Hadamard product (elem-wise multiplication),
â€¢ Invertible; product dissimilar to its operands: ğ´ âˆ— ğµ â‰ ğ´.
â€¢ Bipolar vectors ({âˆ’1, +1}ğ‘›) are inverse of themselves.
â€¢ Can represent variable assignment {ğ‘¥ = ğ‘, ğ‘¦ = ğ‘, ğ‘§ = ğ‘} using bipolar
vectors ğ‘‹, ğ‘Œ , and ğ‘ added into a vector (ğ‘‹ âˆ— ğ´) + (ğ‘Œ âˆ— ğµ) + (ğ‘ âˆ— ğ¶).
To recover the value of ğ‘¥, multiply by ğ‘‹:
ğ‘‹ âˆ— (ğ‘‹ âˆ— ğ´) + ğ‘‹ âˆ— (ğ‘Œ âˆ— ğµ) + ğ‘‹ âˆ— (ğ‘ âˆ— ğ¶)) = ğ´ + noise + noise âˆ¼ ğ´
â€¢ Vector elements permutation,
â€¢ Also invertible; dissimilar; enormous number of permutations.
â€¢ Useful to represent structures, e.g. lists: ğ›±1 for CAR ğ›±2 for CDR:
(ğ‘, ğ‘) represented with ğ›±1(ğ‘) + ğ›±2(ğ‘)
(In highly-dimensional spaces, most vectors are dissimilar; cosine or Pearson correlation of 0.25 indicate close similarity.) 36/86
Modelling Ambiguity?
Sentence-level embeddings always produced by an encoder.
â€¢ Encoder = A deterministic mapping from expression to meaning.
â€¢ Unclear how ambiguous expressions are and should be represented.
â€¢ Same problem with word vectors already:
Ideally, an expression would correspond to
a distribution over semantic space, not a single point. 37/86
Meaning Statefulness
Stateful Meaning Representation:
â€¢ Could be modelled by decoder state:
â‰ˆ â€œState of mind after reading the input and producing a partial output.â€
â€¢ Better reflected in models with attention.
â€¢ Btw needed to interpret humour (Gluscevskij, 2017).
Stateless Meaning Representation:
â€¢ Encoder state.
â€¢ Points correspond to expressions.
â€¢ Ambiguity representation unclear.
38/86
Is Sentence Meaning Continuous?
We know that one Arabic sentence can have dozens of thousands of
English translation (Dreyer and Marcu, 2012):
Premiere of Iraq Nuri al-Maliki was given an excuse by President Bush, who expressed his confidence
in him, and he stated that the circumstances are complicated.
President Bush said that he trusts in Nouri Maliki, head of government of Iraq, and he stated that
he finds an excuse for him â€because the situation is trickyâ€.
Head of cabinet of Iraq Nuri al-Maliki was given an excuse by President Bush, who expressed his
trust in him, and he indicated that the circumstances are difficult.
Iraqâ€™s head of cabinet Nuri al-Maliki was given a reason by President Bush, who expressed his trust
in him, and he indicated that the case is tricky.
President Bush said that he has faith in Iraqi head of cabinet Nouri al-Maliki, and he stated that he
finds an excuse for him â€for the case is complicatedâ€.
39/86
Is Sentence Meaning Continuous?
Similarly: 70k Czech translations of 1 English sentence (Bojar et al., 2013)
And even though he is a political veteran, the Councilor Karel Brezina responded similarly.
A aÄkoli ho lze povaÅ¾ovat za politickÃ©ho veterÃ¡na, radnÃ­ BÅ™ezina reagoval obdobnÄ›.
A i pÅ™estoÅ¾e je politickÃ½ matador, radnÃ­ Karel BÅ™ezina odpovÄ›dÄ›l podobnÄ›.
ByÅ¥ ho lze oznaÄit za politickÃ©ho veterÃ¡na, Karel BÅ™ezina reagoval podobnÄ›.
ByÅ¥ ho mÅ¯Å¾eme prohlÃ¡sit za politickÃ©ho veterÃ¡na, byla i odpovÄ›Ä K. BÅ™eziny velmi podobnÃ¡.
K. BÅ™ezina, i kdyÅ¾ ho lze prohlÃ¡sit za politickÃ©ho veterÃ¡na, odpovÄ›dÄ›l velmi obdobnÄ›.
OdpovÄ›Ä Karla BÅ™eziny byla podobnÃ¡, navzdory tomu, Å¾e je politickÃ½m veterÃ¡nem.
RadnÃ­ BÅ™ezina odpovÄ›dÄ›l velmi obdobnÄ›, navzdory tomu, Å¾e ho lze prohlÃ¡sit za politickÃ©ho veterÃ¡na.
Reakce K. BÅ™eziny, tÅ™ebaÅ¾e je politickÃ½ veterÃ¡n, byla velmi obdobnÃ¡.
Velmi obdobnÃ¡ byla i odpovÄ›Ä Karla BÅ™eziny, aÄkoli ho lze prohlÃ¡sit za politickÃ©ho veterÃ¡na.
Q: Are all these paraphrases close in sent embedding spaces?
Q: How entagled are manifolds of different sents?
â€¦ work in progress with Petra BaranÄÃ­kovÃ¡ 40/86
Examining Continuous Space
Proposed strategy:
1. Propose directions of exploration.
2. Generate seed pairs of sentences for each of the directions.
3. Collect specimens along the proposed directions:
â€¢ interpolation, a â€œsentence in betweenâ€,
â€¢ extrapolation, â€œa sentence further in the hinted directionâ€.
â€¢ Allow people to say â€œimpossibleâ€.
4. Validate the relations.
5. Create the partially ordered set.
6. Search for a manifold covering the ordered set.
Some first ideas explored with Chris Callison-Burch.
First dataset for Czech released (BaranÄÃ­kovÃ¡ and Bojar, 2020). 41/86
Directions of Exploration (1/2)
â€¢ Politeness
â€¢ Tense
â€¢ Verity: How much the speaker believes the message.
â€¢ Modality: Willingness/Ability of the speaker to do it.
â€¢ â€œCountingâ€ / Generic Numerals, Scalar adjectives
â€¢ I saw a handful of people there. / a big crowd / a massive crowd.
â€¢ freezing / cold / chilly
â€¢ â€œNegationâ€, but not only reversing the main predicate
â€¢ Complexity / simplicity, Length.
Thanks to Å Ã¡rka ZikÃ¡novÃ¡ for some of the ideas.
42/86
Directions of Exploration (2/2)
â€¢ Specificity / Generality, Vagueness.
â€¢ Geese fly / Geese migrate / Geese migrate south / The Canadian geese
flew over the pond at friendly Farms in their southward migration.
â€¢ Hammer the hook into the wall. / Put the hook on the wall. / Do the
thingy in there.
â€¢ Contextual boundness.
â€¢ Give it to him. / Give the parcel to the man at the counter. / Give your
parcel to the operator at the post office.
â€¢ High/low style/English/class.
â€¢ Hey yâ€™all itâ€™s a nice day ainâ€™t it?
â€¢ Greetings! Lovely weather we are having.
43/86
First Results of Getting Pairs
Can you please give me a minute? Could you leave me alone?
Close the door. Close the damn door man
Can you help me find something? I need you to help me get something.
May I talk to Mary? Is Mary here?
Iâ€™m sorry-I donâ€™t believe we have met. Who the hell are you?
Can you move so I can see the screen? You arenâ€™t made of glass, you know.
Will you kindly exit? I do not want you here!
Would you please get the mail? Get the mail!
Can I help you? What do you want?
Can you please help me with this? Get over here and help me!
Can you make me breakfast? Why are you not making me breakfast right now?
I tried to call were you busy? You never answer your phone.
44/86
First Results of Midpointing
Can you move so I can see the screen?
Blocking the view, friend.
Move your blocking the screen
Could you move a little bit, youâ€™re blocking the screen.
Can you please move?
I canâ€™t see, can you move a little?
Hey can you move.
Please move.
Can you move a bit?
You arenâ€™t made of glass, you know.
45/86
After the Midpointingâ€¦Can you hurry eating?
All done?
Are you almost done eating?
Are you done eating yet?
Are you finished with your food?
Are you finished with your food yet?
Done with the food?Finished yet?
When will you be done with your food?
You're still not done with your food?
46/86
Ask Crowd to Partially Sort ThemCan you hurry eating?
All done?
Are you almost done eating?
Are you done eating yet?
Are you finished with your food?
Are you finished with your food yet?
Done with the food?Finished yet?
When will you be done with your food?
You're still not done with your food?
Can you hurry eating?
All done?
Are you almost done eating?
Are you done eating yet?
Are you finished with your food?
Are you finished with your food yet?
Done with the food?Finished yet?
When will you be done with your food?
You're still not done with your food?
47/86
Find Methods for Manifold LearningCan you hurry eating?
All done?
Are you almost done eating?
Are you done eating yet?
Are you finished with your food?
Are you finished with your food yet?
Done with the food?Finished yet?
When will you be done with your food?
You're still not done with your food?
Can you hurry eating?
All done?
Are you almost done eating?
Are you done eating yet?
Are you finished with your food?
Are you finished with your food yet?
Done with the food?Finished yet?
When will you be done with your food?
You're still not done with your food?
Manifold learning
48/86
Match Posets with Learned ManifoldsCan you hurry eating?
All done?
Are you almost done eating?
Are you done eating yet?
Are you finished with your food?
Are you finished with your food yet?
Done with the food?Finished yet?
When will you be done with your food?
You're still not done with your food?
Can you hurry eating?
All done?
Are you almost done eating?
Are you done eating yet?
Are you finished with your food?
Are you finished with your food yet?
Done with the food?Finished yet?
When will you be done with your food?
You're still not done with your food?
Manifold learning
Can you hurry eating?
All done?
Are you almost done eating?
Are you done eating yet?
Are you finished with your food?
Are you finished with your food yet?
Done with the food?
Finished yet?
When will you be done with your food?
You're still not done with your food?
Can you hurry eating?
All done?
Are you almost done eating?
Are you done eating yet?
Are you finished with your food?
Are you finished with your food yet?
Done with the food?
Finished yet?
When will you be done with your food?
You're still not done with your food?
Manifold learning
semi-supervised.
49/86
Evaluating Sentence
Representations
Evaluating Sentence Representations
Conneau and Kiela (2018) introduce SentEval:
â€¢ Given a sentence representation function, assess the fitness of the
representation in multiple tasks.
https://github.com/facebookresearch/SentEval/
Conneau et al. (2018) and others then compare several reprs incl.:
â€¢ SkipThough (Kiros et al., 2015):
â€¢ Predict sentence given the surrounding sentences.
â€¢ InferSent (Conneau et al., 2017):
â€¢ Train sentence representations on predicting entailment.
Extremely active research area, see BlackboxNLP workshops.
50/86
How â€œSemanticâ€ are Seq2Seq Reprs?
CÃ­fka and Bojar (2018):
â€¢ Trained several variations of Cho et al. (2014).
â€¢ Extracted sentence representations.
â€¢ Related BLEU and â€œsemanticsâ€ of the representation:
â€¢ Evaluation through classification.
â€¢ Evaluation through similarity.
â€¢ Evaluation using paraphrases.
51/86

































Summary
â€¢ NMT systems can surpass humans within the given domain.
â€¢ We discussed learned representations.
â€¢ Illustrated word and sentence embeddings.
â€¢ We discussed aspects of meaning.
â€¢ Some level of â€œunderstandingâ€ can be found in the representations.
â€¢ Follow the BlackBoxNLP workshops:
â€¢ POS, Syntax, Word Derivations, Compositionalityâ€¦
â€¢ Still very far from human understanding.
â€¢ Big caveats need to be taken when interpreting results.
â€¢ The â€œutilityâ€ of syntax in NMT discussed last week.
â€¢ The exact composition of the task and the test set.
85/86
References
Mostafa Abdou, Vladan Gloncak, and OndÅ™ej Bojar. 2017. Variable mini-batch sizing and pre-trained embeddings. In
Proceedings of the Second Conference on Machine Translation, pages 680â€“686, Copenhagen, Denmark, September.
Association for Computational Linguistics.
Amir Bakarov. 2018. A survey of word embeddings evaluation methods. CoRR, abs/1801.09536.
Petra BaranÄÃ­kovÃ¡ and OndÅ™ej Bojar. 2020. COSTRA 1.0: A Dataset of Complex Sentence Transformations. In
Proceedings of the LREC 2020. ELRA.
Yevgeni Berzak, Andrei Barbu, Daniel Harari, Boris Katz, and Shimon Ullman. 2015. Do you see what I mean? visual
resolution of linguistic ambiguities. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language
Processing, pages 1477â€“1487, Lisbon, Portugal, September. Association for Computational Linguistics.
Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword
information. Transactions of the Association for Computational Linguistics, 5:135â€“146.
OndÅ™ej Bojar, MatouÅ¡ MachÃ¡Äek, AleÅ¡ Tamchyna, and Daniel Zeman. 2013. Scratching the Surface of Possible
Translations. In Proc. of TSD 2013, Lecture Notes in Artificial Intelligence, Berlin / Heidelberg. ZÃ¡padoÄeskÃ¡ univerzita
v Plzni, Springer Verlag.
OndÅ™ej Bojar, Raffaella Bernardi, and Bonnie Webber. 2019. Representation of sentence meaning (a jnle special issue).
Natural Language Engineering, 25(4).
Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. 2014. Learning phrase representations using rnn encoderâ€“decoder for statistical machine translation. In
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages
1724â€“1734, Doha, Qatar, October. Association for Computational Linguistics.
OndÅ™ej CÃ­fka and OndÅ™ej Bojar. 2018. Are BLEU and Meaning Representation in Opposition? In Proceedings of the
56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1362â€“1371. 86/86


Multilingual
Machine Translation
OndÅ™ej Bojar
April 30, 2020
NPFL087 Statistical Machine Translation
Charles University
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics unless otherwise stated
Outline
â€¢ Motivation for using more than 2 languages.
â€¢ Transfer Learning.
â€¢ Catastrophic Forgetting.
â€¢ Trivial Transfer Learning.
â€¢ Multi-Lingual NMT.
â€¢ Massively Multi-Lingual NMT.
Many slides on transfer learning by Tom Kocmi.
Many slides on multilingual models by Rico Sennrich and Adam Lopez.
1/70
Why Multilingual MT
â€¢ Help in low-resource settings.
â€¢ Words, morphemes or syntactic patterns common to more languages.
â€¢ Learning can reuse patterns seen in another dataset.
â€¢ Improve translation quality.
â€¢ Words are ambiguous, the third language can disambiguate.
â€¢ Truly multi-lingual environments.
â€¢ United Nations: 6 languages.
â€¢ EU official languages: 24.
â€¢ EUROSAI official languages: 43.
â€¢ INTOSAI official languagesâ€¦
2/70
Transfer Learning
Motivation for NN Transfer LearningPerformance
Training steps
Without transfer learning
3/70
Motivation for NN Transfer LearningPerformance
Training steps
Without transfer learning
With transfer learning
4/70
Motivation for NN Transfer LearningPerformance
Training steps
Without transfer learning
With transfer learning
Better initial performace
Steeper slope
Better final performance
5/70
Steps of Transfer Learning
6/70
Steps of Transfer Learning
7/70
Interlude: Catastrophic Forgetting
â€¢ Kocmi and Bojar (2017) explore curriculum learning:
â€¢ Start with simpler sentences first, add complex ones later.0
5
10
15
0 10 20 30 40 50
BLEU
Steps (in millions examples)
Baseline
8/70
Interlude: Catastrophic Forgetting
â€¢ Kocmi and Bojar (2017) explore curriculum learning:
â€¢ Start with simpler sentences first, add complex ones later.
â€¢ When â€œsimplerâ€ means â€œshorterâ€:0
5
10
15
0 10 20 30 40 50
BLEU
Steps (in millions examples)
Baseline
Sorted by length
8/70
Interlude: Catastrophic Forgetting
â€¢ Kocmi and Bojar (2017) explore curriculum learning:
â€¢ Start with simpler sentences first, add complex ones later.
â€¢ When â€œsimplerâ€ means â€œshorterâ€:
â€¢ Clear jumps in score as bins of longer sentences are allowed.0
5
10
15
0 10 20 30 40 50
BLEU
Steps (in millions examples)
Baseline
Sorted by length
Curriculum by target length
8/70
Interlude: Catastrophic Forgetting
â€¢ Kocmi and Bojar (2017) explore curriculum learning:
â€¢ Start with simpler sentences first, add complex ones later.
â€¢ When â€œsimplerâ€ means â€œshorterâ€:
â€¢ Clear jumps in score as bins of longer sentences are allowed.
â€¢ Reversed curriculum unlearns to produce long sentences.0
5
10
15
0 10 20 30 40 50
BLEU
Steps (in millions examples)
Baseline
Sorted by length
Curriculum by target length
Reversed Curriculum by target length
8/70
Trivial Transfer Learning
â€¢ Early works (Zoph et al., 2016; Nguyen and Chiang, 2017) target
one common language (English).
â€¢ Kocmi and Bojar (2018) try even unrelated languages.
The trivial procedure:
â€¢ Train on one pair (â€œparentâ€), switch corpus to another (â€œchildâ€).
â€¢ The only requirement: joint subword units across all langs.
9/70
Getting Balanced Vocabulary
10/70
Getting Balanced Vocabulary
11/70
Getting Balanced Vocabularythe_
Å¾e_
ying_
staying_
pra
pracovat_
...
12/70
English on Same SideParent model Corpus size
difference
Direction Baseline
(BLEU)
Transfer
(BLEU)
Î”
(BLEU)
Czech 9x from English 16.13 17.75 1.62 *
Czech 9x to English 19.19 22.42 3.23 *
Child model: Slovak
* statistically significant
13/70
English on Same SideParent model Corpus size
difference
Direction Baseline
(BLEU)
Transfer
(BLEU)
Î”
(BLEU)
Czech 9x from English 16.13 17.75 1.62 *
Czech 9x to English 19.19 22.42 3.23 *
Child model: Slovak
* statistically significant
14/70
English on Same SideParent model Corpus size
difference
Direction Baseline
(BLEU)
Transfer
(BLEU)
Î”
(BLEU)
Czech 9x from English 16.13 17.75 1.62 *
Czech 9x to English 19.19 22.42 3.23 *
Child model: Slovak
* statistically significant
Parent model Corpus size
difference
Direction Baseline
(BLEU)
Transfer
(BLEU)
Î”
(BLEU)
Finnish 3.5x from English 17.03 19.74 2.71 *
Russian 16x from English 17.03 20.09 3.06 *
Czech 50x from English 17.03 20.41 3.38 *
Finnish 3.5x to English 21.74 24.18 2.44 *
Russian 16x to English 21.74 23.54 1.80 *
Child model: Estonian
15/70
English on Same SideParent model Corpus size
difference
Direction Baseline
(BLEU)
Transfer
(BLEU)
Î”
(BLEU)
Czech 9x from English 16.13 17.75 1.62 *
Czech 9x to English 19.19 22.42 3.23 *
Child model: Slovak
* statistically significant
Parent model Corpus size
difference
Direction Baseline
(BLEU)
Transfer
(BLEU)
Î”
(BLEU)
Finnish 3.5x from English 17.03 19.74 2.71 *
Russian 16x from English 17.03 20.09 3.06 *
Czech 50x from English 17.03 20.41 3.38 *
Finnish 3.5x to English 21.74 24.18 2.44 *
Russian 16x to English 21.74 23.54 1.80 *
Child model: Estonian
16/70
English on Same SideParent model Corpus size
difference
Direction Baseline
(BLEU)
Transfer
(BLEU)
Î”
(BLEU)
Czech 9x from English 16.13 17.75 1.62 *
Czech 9x to English 19.19 22.42 3.23 *
Child model: Slovak
* statistically significant
Parent model Corpus size
difference
Direction Baseline
(BLEU)
Transfer
(BLEU)
Î”
(BLEU)
Finnish 3.5x from English 17.03 19.74 2.71 *
Russian 16x from English 17.03 20.09 3.06 *
Czech 50x from English 17.03 20.41 3.38 *
Finnish 3.5x to English 21.74 24.18 2.44 *
Russian 16x to English 21.74 23.54 1.80 *
Child model: Estonian
Related
Cyrillic
Related
Related
Related
Cyrillic
Biggest
17/70
English on Same Side, Parent Low-ResourceParent model Corpus size
difference
Direction Baseline
(BLEU)
Transfer
(BLEU)
Î”
(BLEU)
Estonian 0.3x from English 19.50 20.07 0.57 *
Estonian 0.3x to English 24.40 23.95 -0.45
Parent model Corpus size
difference
Direction Baseline
(BLEU)
Transfer
(BLEU)
Î”
(BLEU)
Slovak 0.1x from English 23.48 22.99 -0.49 *
Slovak 0.1x to English 29.61 28.20 -1.41 *
Child model: Finnish
Child model: Czech
18/70
English on Same Side, Parent Low-ResourceParent model Corpus size
difference
Direction Baseline
(BLEU)
Transfer
(BLEU)
Î”
(BLEU)
Estonian 0.3x from English 19.50 20.07 0.57 *
Estonian 0.3x to English 24.40 23.95 -0.45
Parent model Corpus size
difference
Direction Baseline
(BLEU)
Transfer
(BLEU)
Î”
(BLEU)
Slovak 0.1x from English 23.48 22.99 -0.49 *
Slovak 0.1x to English 29.61 28.20 -1.41 *
Child model: Finnish
Child model: Czech
19/70
English on the Other SideParent
model
Child model Corpus size
amplification
Baseline
(BLEU)
Transfer
(BLEU)
Î”
(BLEU)
Parent
Aligned Î”
EN - Finnish Estonian - EN 3.5x 21.74 22.75 1.01 * 2.44 *
EN - Russian Estonian - EN 16x 21.74 23.12 1.38 * 1.80 *
EN - Czech Estonian - EN 50x 21.74 22.80 1.06 *
Finnish - EN EN - Estonian 3.5x 17.03 18.19 1.16 * 2.71 *
Russian - EN EN - Estonian 16x 17.03 18.16 1.13 * 3.06 *
20/70
No Language in CommonParent model Corpus size
amplification
Baseline
(BLEU)
Transfer
(BLEU)
Î”
(BLEU)
Arabic - Russian 12x 21.74 22.23 0.49
Spanish - French 12x 21.74 22.24 0.50 *
Spanish - Russian 12x 21.74 22.52 0.78 *
French - Russian 12x 21.74 22.40 0.66 *
Child model: Estonian to English
21/70
No Language in CommonParent model Corpus size
amplification
Baseline
(BLEU)
Transfer
(BLEU)
Î”
(BLEU)
Arabic - Russian 12x 21.74 22.23 0.49
Spanish - French 12x 21.74 22.24 0.50 *
Spanish - Russian 12x 21.74 22.52 0.78 *
French - Russian 12x 21.74 22.40 0.66 *
Child model: Estonian to English
CyrillicArabic
Cyrillic
Cyrillic
22/70
The Better the Parent, the Better the Child12
14
16
18
0 250 500 750 1000
BLEU
Steps (in thousands)
Baseline en-et only
en-et after 50k of en-f
en-et after 100k of en-f
en-et after 200k of en-f
en-et after 400k of en-f
en-et after 800k of en-f
Non-comparable English-Finnish
23/70
The Lesser the Child, the Bigger the Gain10k 50k 100k 200k 400k 800k
0
5
10
15
20
1.95
5.74
9.39
11.96
14.94
17.03
12.46
15.95 17.61 17.95 19.04 19.74
BLEU
24/70
Why it Helps? Not Really Vocabulary (1/2)
Length BLEU Components BP
Base ENET 35326 48.1/21.3/11.3/6.4 0.979
ENRU+ENET 35979 51.0/24.2/13.5/8.0 0.998
ENCS+ENET 35921 51.7/24.6/13.7/8.1 0.996
(The reference length in the matching tokenization was 36062.)
â€¢ Child models produce longer outputs â‡’ lower brevity penalty.
â€¢ But ğ‘›-gram precisions also better.
1-gram present in ENRU+ENET ENCS+ENET
Child, Base, Ref 15902 (44.2 %) 15924 (44.3 %)
Child only 9635 (26.8 %) 9485 (26.4 %)
Child, Base 7209 (20.0 %) 7034 (19.6 %)
Child, Ref 3233 (9.0 %) 3478 (9.7 %)
Total 35979 (100.0 %) 35921 (100.0 %)
â€¢ The 3k better toks are regular ET words, not NEs or numbers. 25/70
Why it Helps? Not Really Vocabulary (2/2)
26/70
Why it Helps? Sentence Lengths SomewhatParent Child
Sentence lengths BLEU Avg. words BLEU Avg. words
1-10 words 8.57 10.9 16.57 15.3
10-20 words 16.21 15.4 17.48 15.3
20-40 words 12.59 21.9 17.99 15.3
40-60 words 5.76 35.5 16.80 15.5
1-60 words 22.30 15.3 19.15 15.4
27/70
Why it Helps? Sentence Lengths SomewhatParent Child
Sentence lengths BLEU Avg. words BLEU Avg. words
1-10 words 8.57 10.9 16.57 15.3
10-20 words 16.21 15.4 17.48 15.3
20-40 words 12.59 21.9 17.99 15.3
40-60 words 5.76 35.5 16.80 15.5
1-60 words 22.30 15.3 19.15 15.4
28/70
Why it Helps? Sentence Lengths SomewhatParent Child
Sentence lengths BLEU Avg. words BLEU Avg. words
1-10 words 8.57 10.9 16.57 15.3
10-20 words 16.21 15.4 17.48 15.3
20-40 words 12.59 21.9 17.99 15.3
40-60 words 5.76 35.5 16.80 15.5
1-60 words 22.30 15.3 19.15 15.4
29/70
Multilingual MT
Multilingual MT Configurations
â€¢ Pivot translation (Cascading).
â€¢ Multi-lingual source (also called multi-way).
â€¢ Multi-lingual multi-source.
â€¢ Multi-lingual target.
â€¢ Multi-lingual multi-target.
â€¢ Both sides multi-lingual.
â€¢ (Both sides multi-lingual, multi-source, multi-target. ;-)
â€¢ Zero-shot training.
â€¢ i.e. translating an unseen pair when both the source and target langs were
covered in the training data in other pairs.
â€¢ â€œBeyond zero-shotâ€ is translating from an unseen language.
30/70
Multi-Target and Multi-Source MT
â€¢ Multi-Target focus: Efficiency
â€¢ Decrease hardware resources compared using many separate models.
â€¢ Multi-Source focus: Resolving ambiguity thanks to existing translation
â€¢ E.g. Translating German â€œSchlossâ€ to French is easier
if we can feed in the English translation (â€œcastleâ€ or â€œlockâ€).
â€¢ Training on: Multi-parallel or bi-parallel multilingual corpora.cs
â€¦
Neural Network
de
fr
en
Figure 1: Multi-Target MTNeural NetworkNeural Network
fr
en
cs
it
â€¦
+
+
+ Figure 2: Multi-Source MT 31/70
Ideal: Flexible Multi-Lingual MTru
Neural NetworkNeural Network
en
cs
it
â€¦
AND/OR
sk
de
fr
...
AND/OR
AND/OR
Figure 3: Flexible multilingual MT
32/70
Multi-source translation
Quite an old idea (e.g. Och & Ney 2001)33/70
Multi-source translation
â€¢ Assorted techniques to do this in IBM-style or phrase-
based MT.
â€¢ Difficult to model directly due to independence
assumptions of these models.
â€¢ Usually done as a kind of system combination
(merging the output of two MT systems).
â€¢ But this introduces other problems, e.g. decoding.
â€¢ Fundamentally, itâ€™s interpolation of conditional LMs.34/70
Direct multi-source
Zoph & Knight 2016
â€¢ Directly learns and uses p(English|French,German)
â€¢ For attention: two context vectors (uses p-local attention of
Luong, et al, but could use other methods).35/70
Multi-way MT
Firat et al. 2016 (two papers)
â€¢ Assume only many bilingual parallel corpora.
â€¢ For N languages: learn N encoders and N decoders.
â€¢ But what about attention?36/70
Multi-way MT
Firat et al. 2016 (two papers)
â€¢ Assume only many bilingual parallel corpora.
â€¢ For N languages: learn N encoders and N decoders.
â€¢ But what about attention?
p(fi|fiâˆ’1, ..., f1, e) = g(fiâˆ’1, si, ci)
ci =
|e|
âˆ‘
j=1
Î±ij hj
Î±ij = exp(aij )
âˆ‘|e|
k=1 exp(aik)
aij = a(siâˆ’1, hj )37/70
Multi-way MT
Firat et al. 2016 (two papers)
â€¢ As in Bahdanu et al. (2014), attention mechanism is
a feedforward function of both decoder hidden state
and encoder context vector.
â€¢ Shared between all encoders and decoders.
p(fi|fiâˆ’1, ..., f1, e) = g(fiâˆ’1, si, ci)
ci =
|e|
âˆ‘
j=1
Î±ij hj
Î±ij = exp(aij )
âˆ‘|e|
k=1 exp(aik)
aij = a(siâˆ’1, hj )
Everything
we need is
right here!38/70
Multi-way MT
Firat et al. 2016 (two papers)
Low-resource simulation
(using high-resource
European languages)39/70
Multi-way MT
Firat et al. 2016 (two papers)40/70
Multi-way MT
Firat et al. 2016 (two papers)
ok, but what about multi-source?41/70
Multi-way multi-source MT
Firat et al. 2016 (two papers)
â€¢ Still assumes only many bilingual parallel corpora.
â€¢ What to do if there are multiple input sentences?
â€¢ Early averaging (average context vectors).
â€¢ Late averaging (aka linear interpolation).
Early and late averaging are orthogonal, can be combined.42/70
43/70
Multi-way multi-source MT
Firat et al. 2016 (two papers)44/70
45/70
Zero-shot MT
Firat et al. 2016 (two papers)
â€¢ Suppose our bilingual parallel data include a pair of
languages for which we have no parallel data.
â€¢ Q: Can we use the multi-way encoder-decoder system
to translate Spanish into French?
English EnglishSpanish French46/70
47/70
48/70
Zero-shot MT
Firat et al. 2016 (two papers)
â€¢ Finetuning: what if we use a small amount of
parallel data in this setting?
â€¢ Q: Where would we get this data? Backtranslation
English EnglishSpanish French49/70
Zero-shot MT
Firat et al. 2016 (two papers)
â€¢ Finetuning: what if we use a small amount of
parallel data in this setting?
â€¢ Q: Where would we get this data? Backtranslation
English EnglishSpanish French
Spanish (MT)50/70
Zero-shot MT
Firat et al. 2016 (two papers)
â€¢ Finetuning: what if we use a small amount of
parallel data in this setting?
â€¢ Q: Where would we get this data? Backtranslation
English EnglishSpanish French
Spanish (MT)51/70
Zero-shot MT
Firat et al. 2016 (two papers)
â€¢ Finetuning: what if we use a small amount of
parallel data in this setting?52/70
Zero-shot MT
Firat et al. 2016 (two papers)
â€¢ Finetuning: what if we use a small amount of
parallel data in this setting?53/70
Simple Data Mixing
Do we really need separate encoders and decoders?
â€¦ simply feed in various language pairs:
Source Sent 1 (De) 2en versetzen Sie sich mal in meine Lage !
Target Sent 1 (En) put yourselves in my position .
Source Sent 2 (En) 2nl I flew on Air Force Two for eight years .
Target Sent 2 (Nl) ik heb acht jaar lang met de Air Force Two gevlogen .
â€¢ The model of the same size will learn both pairs.
â€¢ Hopefully benefiting from various similarities.
â€¢ Risk of catastrophic forgetting.
See Johnson et al. (2016) or Ha et al. (2017). 54/70
55/70
56/70
â€œLanguage Embeddingsâ€ from 927 BiblesEnglishmultilingual
NMT modelBible translations
in 927 languagesvector space
of language
embeddingslearning to translate
Portuguese - Swedish
(â€œzero-shot MTâ€)x1 x2 x3 x4
Ex
C
Z
Ey
y1 y2 y3
attention
encoder decoder
h1 h2 h3 h4trainlanguage
flags
input sentence
output sentence
Helsinki Neural
MT System
Tiedemann (2018) 57/70
â€œLanguage Embeddingsâ€ from 927 BiblesTrans-New Guinea
Otomanguean
Quechuan
Indo-European
Austronesian
Nilo-Saharan
Afro-Asiatic
Mayan
Niger-Congo
Creole
t-SNE of the language-embedding vectors, colored by language family. 58/70
Massively Multi-Lingual
Models
Available Data for ENâ†”100+ Langs
59/70
Translation Quality of Bilingual MT
60/70
Standard Transformer Model
61/70
Google Transformer Sizes
GPipe (Huang et al., 2019) introduces microbatches for faster training
of deep models across multiple GPUs.
Enc/Dec Depth FF Dim Heads Total Parameters GPUs Used
6 8192 16 400M 1 default
12 16384 32 1.3B 2 â€œwideâ€
24 8192 16 1.3B 4 â€œdeepâ€
32 16384 32 3.0B 8
64 16384 32 6.0B 16
â€¢ â€œDeepâ€ better than â€œwideâ€ on low-resource languages.
â€¢ Indicates better generalization.
â€¢ Further tricks needed to keep the training stable.
62/70
Massively Multilingual Models
63/70
Massive Massively Multilingual Models
64/70
Google-Sized Experiment
The recent 50 billion parameters Transformer needed further trick:
â€¢ sparsely-gated mixture of experts (Shazeer et al., 2017):
â‡’ BLEU on 100 langs re-gained and improved by 125x larger model.
https://ai.googleblog.com/2019/10/exploring-massively-multilingual.html
65/70
Domain Adapters to Recover Practical Sizes
â€¢ Bapna and Firat (2019) propose tiny tunable â€œadapterâ€ layers.
1. Pretrain on a large mixed-language corpus.
2. Inject adapter layers.
3. Finetune adapter layers for each of the target tasks.
66/70
Domain Adapters into English
67/70
Domain Adapters from English
68/70
Summary
â€¢ Transfer learning in NMT works.
â‡’ NMT can exploit more and less related data.
â€¢ Trivial Transfer: Parent just has to be larger.
â€¢ Even unrelated language pairs can help.
â€¢ Probably better initialization.
â€¢ Multi-source, multi-target, â€¦, flexible multi-lingual setups.
â€¢ Language families emerge in language token embedding.
â€¢ Model capacity is the bottleneck.
â€¢ Models 125x large for 100 languages in one model
allow gains on high-resource languages, too.
â€¢ With tiny adaptors instead of mixture of experts
model sizes can decrease again.
69/70
References
Ankur Bapna and Orhan Firat. 2019. Simple, scalable adaptation for neural machine translation. In Proceedings of the
2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP), pages 1538â€“1548, Hong Kong, China, November. Association for
Computational Linguistics.
Thanh-Le Ha, Jan Niehues, and Alexander H. Waibel. 2017. Effective strategies in zero-shot neural machine
translation. CoRR, abs/1711.07893.
Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam,
Quoc V Le, Yonghui Wu, and zhifeng Chen. 2019. Gpipe: Efficient training of giant neural networks using pipeline
parallelism. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'AlchÃ©-Buc, E. Fox, and R. Garnett, editors, Advances in
Neural Information Processing Systems 32, pages 103â€“112. Curran Associates, Inc.
Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda B.
ViÃ©gas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2016. Googleâ€™s multilingual neural
machine translation system: Enabling zero-shot translation. CoRR, abs/1611.04558.
Tom Kocmi and OndÅ™ej Bojar. 2017. Curriculum Learning and Minibatch Bucketing in Neural Machine Translation. In
Proceedings of Recent Advances in NLP (RANLP 2017).
Tom Kocmi and OndÅ™ej Bojar. 2018. Trivial Transfer Learning for Low-Resource Neural Machine Translation. In
Proceedings of the Third Conference on Machine Translation, Volume 1: Research Papers, volume 1, pages 244â€“252,
Stroudsburg, PA, USA. Association for Computational Linguistics, Association for Computational Linguistics.
Toan Q. Nguyen and David Chiang. 2017. Transfer learning across low-resource, related languages for neural machine
translation. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2:
Short Papers), pages 296â€“301. Asian Federation of Natural Language Processing.
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton, and Jeff Dean. 2017.70/70


Multi-Modal Translation
Speech and Vision
OndÅ™ej Bojar
May 7, 2020
NPFL087 Statistical Machine Translation
Charles University
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics unless otherwise stated
Outline
â€¢ Overview of Multi-Modal Translation.
â€¢ Speech Translation â‰ˆ ASR + MT.
â€¢ Problems at ASR-MT boundary.
â€¢ End-to-end SLT approaches.
â€¢ Visual information for MT.
Some pictures and tables from Sulubacak et al. (2019).
1/53
Overview of Multi-Modal MT
From survey by Sulubacak et al. (2019):MT TextText
2/53
Overview of Multi-Modal MT
From survey by Sulubacak et al. (2019):MT TextText
ASR
Speech
Language B
Language A
2/53
Overview of Multi-Modal MT
From survey by Sulubacak et al. (2019):MT TextText
ASR
Speech
Language B
Language A
TTS
Speech
2/53
Overview of Multi-Modal MT
From survey by Sulubacak et al. (2019):MT TextText
ASR
Speech
Language B
Language A
TTS
Speech
SLT
2/53
Overview of Multi-Modal MT
From survey by Sulubacak et al. (2019):MT TextText
ASR
Speech
Language B
Language A
TTS
Speech
SLT
S2S
2/53
Overview of Multi-Modal MT
From survey by Sulubacak et al. (2019):MT TextText
ASR
Speech
Language B
Language A
TTS
Speech
SLT
S2S
IGT
IC
IGT = image-guided
2/53
Overview of Multi-Modal MT
From survey by Sulubacak et al. (2019):MT TextText
ASR
Speech
Language B
Language A
TTS
Speech
SLT
S2S
IGT
IC
VGT
VD
IGT = image-guided, VGT = video-guided translation
2/53
Spoken Language
Translation
Basic Terms
â€¢ MT = Machine Translation = Text Translation
â€¢ Input are (mostly grammatically correct) individual sentences.
â€¢ Sentences may come in documents or not.
â€¢ (Document-level MT processes a sequence of sentences at once.)
3/53
Basic Terms
â€¢ MT = Machine Translation = Text Translation
â€¢ Input are (mostly grammatically correct) individual sentences.
â€¢ Sentences may come in documents or not.
â€¢ (Document-level MT processes a sequence of sentences at once.)
â€¢ Incremental MT
â€¢ MT of gradually growing input.
â€¢ MT decides whether to wait for more words or emit current word.
â€¢ Aims at stable output.
3/53
Basic Terms
â€¢ MT = Machine Translation = Text Translation
â€¢ Input are (mostly grammatically correct) individual sentences.
â€¢ Sentences may come in documents or not.
â€¢ (Document-level MT processes a sequence of sentences at once.)
â€¢ Incremental MT
â€¢ MT of gradually growing input.
â€¢ MT decides whether to wait for more words or emit current word.
â€¢ Aims at stable output.
â€¢ SLT = Spoken Language Translation
â€¢ Input is the sound in one language.
â€¢ Output is text (sometimes also speech).
â€¢ Sentences may or may not be assumed and produced.
â€¢ S2S = S2ST = Speech-to-Speech Translation
â€¢ Direct modelling, e.g. can aim to preserve voice or prosodics.
3/53
Spoken Language Translation
Cascaded ASR + MT
NN Prospects: ASR Surpassing Humans
â€¢ Switchboard conversational speech benchmark (2000).
â€¢ 40 phone calls between two random native English speakers.7x more datafirst DNNsequence losshuman level2012 2013 2014 2015 2016 2017
6
8
10
12
14
16
word
error
rate
Plot by https://awni.github.io/speech-recognition/
4/53
MT Surpassing Humans for News
Seg-Level Englishâ†’Czech 2018
Ave. % Ave. z System
1 84.4 0.667 CUNI-Transformer
2 79.8 0.521 uedin
78.6 0.483 Professional Translation
4 68.1 0.128 online-B
5 59.4 âˆ’0.178 online-A
6 54.1 âˆ’0.354 online-G
Doc-Aware Englishâ†’German 2019
Ave. Ave. z System
90.3 0.347 Facebook-FAIR
93.0 0.311 Microsoft-WMT19-sent-doc
92.6 0.296 Microsoft-WMT19-doc-level
90.3 0.240 Professional Translation
87.6 0.214 MSRA-MADL
88.7 0.213 UCAM
89.6 0.208 NEU
87.5 0.189 MLLP-UPV
87.5 0.130 eTranslation
86.8 0.119 dfki-nmt
84.2 0.094 online-B
â€¦ 10 more systems here â€¦
76.3 âˆ’0.400 online-X
43.3 âˆ’1.769 en-de-task
See lecture #1 for all caveats of MT evaluation.
5/53
SLT Pipeline
1. Run ASR.
2. Run MT.
6/53
SLT Pipeline
1. Run ASR Recognize lowercase words.
2. Run MT Translate sentences.
7/53
SLT Pipeline
1. Run ASR Recognize lowercase words.
2. Segment into sentences.
3. Run MT Translate sentences.
8/53
SLT Pipeline
1. Run ASR Recognize lowercase words.
2. Segment into sentences.
3. Consider how to handle uncertainty!
4. Run MT Translate sentences.
9/53
SLT Pipeline
1. Acquire sound.
2. Run ASR Recognize lowercase words.
3. Segment into sentences.
4. Consider how to handle uncertainty!
5. Run MT Translate sentences.
6. Present output.
10/53
SLT Pipeline When Deployed
1. Acquire sound.
2. Ship to ASR worker.
3. Run ASR Recognize lowercase words.
4. Ship to sentence segmenter.
5. Segment into sentences.
6. Ship to translation worker.
7. Consider how to handle uncertainty!
8. Run MT Translate sentences.
9. Ship to presentation worker.
10. Present output.
11/53
SLT Pipeline When Deployed
1. Acquire sound.
2. Ship to ASR worker.
3. Run ASR Recognize lowercase words.
4. Ship to sentence segmenter.
5. Segment into sentences.
6. Ship to translation worker.
7. Consider how to handle uncertainty!
8. Run MT Translate sentences.
9. Ship to presentation worker.
10. Present output.
In realtime!In realtime!
11/53
Overall Architecture in ELITR
â€¢ Components can run distributed, connected via â€œbi-socketsâ€.Mediator
Sound Input
ASR
Client
Workers
Dummy Output
Segmenter MT Presentation
Web
â€¢ Connections always open, reused across clients.
â€¢ TCP communication â‡’ relies on network capacity.
12/53
Spoken Language Translation
Network Issues
Failures Due to Setup
Over our test sessions, we saw:
â€¢ slow network at various steps,
â€¢ partially working misconfiguration.Mediator
Sound Input
ASR
Client
Workers
Dummy Output
Segmenter MT Presentation
Web
...network issues at
one of the partners
13/53
Failures Due to Setup
Over our test sessions, we saw:
â€¢ slow network at various steps,
â€¢ partially working misconfiguration.Mediator
Sound Input
ASR
Client
Workers
Dummy Output
Segmenter MT Presentation
Web
13/53
Failures Due to Setup
Over our test sessions, we saw:
â€¢ slow network at various steps,
â€¢ partially working misconfiguration.Mediator
Sound Input
ASR
Client
Workers
Dummy Output
Segmenter MT Presentation
Web
queuing of web...
update events
13/53
Failures Due to Setup
Over our test sessions, we saw:
â€¢ slow network at various steps,
â€¢ partially working misconfiguration.Mediator
Sound Input
ASR
Client
Workers
Dummy Output
Segmenter MT Presentation
Web
...running locally but
shipping sound twice
13/53
Spoken Language Translation
Sound Acquisition
Microphone Position
14/53
Headset Mic vs. Shirt Mic
A micro-test (just 3.5 minutes in total) with two microphones:
Word Error Rate Headset Shirt Diff
EN ASR 0.32 0.39 -0.07
CS ASR 0.14 0.17 -0.03
15/53
Microphone Distance and Other Errors
https://www.sweetwater.com/insync/5-ways-your-mic-technique-is-ruining-your-vocals/
16/53
Microphone Distance and Other Errors
https://www.sweetwater.com/insync/5-ways-your-mic-technique-is-ruining-your-vocals/
16/53
Microphone Distance and Other Errors
https://www.sweetwater.com/insync/5-ways-your-mic-technique-is-ruining-your-vocals/
16/53
Microphone Distance and Other Errors
https://www.sweetwater.com/insync/5-ways-your-mic-technique-is-ruining-your-vocals/
16/53
Volume Settings along the Pipeline
A number of volume controls is on the way:
â€¢ Wireless microphone output volume.
â€¢ Sound card input volume.
â€¢ Line/Mic Level. â€¢ Padding.
â€¢ Automatic clipping of too loud signal.
â‡’ You need to carefully â€˜trackâ€™ the signal step by step.
17/53
Spoken Language Translation
Realistic ASR Quality
ASR Challenges
Speaker intents You have a botel? Oh, yes. Weâ€™re situated in hearth
of ÄŒeskÃ© BudÄ›jovice.
Reality You have a bottle? Oh, yes. VeeR situated in haRd
of ÄŒeskÃ© BudÄ›jovice. + BACKGROUND NOISE
Unknowledgeable You have a bottle? Oh, yes. Weâ€™re situated in hearth
person hears of Che... WHICH CITY?
Noise-sensitive ASR âˆ… oh yes the the of âˆ…
Noise-resistent ASR you have somebody to Oh, yes, we are situated in hard
which is can we do?
Knowledgeable person You have a botel? Oh, yes, weâ€™re situated in hearth
/ Future ASR of ÄŒeskÃ© BudÄ›jovice.
â€¢ Non-â€œstandardâ€ pronunciation, background noise, OOV, named entities
18/53
ASR on Non-Native High-School Students
20
40
60
80
100
Google UEDIN KIT
WER
Recognized by all
20
40
60
80
100
Google UEDIN KIT
WER
All recordings
System: Google UEDIN KIT
19/53
ASR of Non-Natives in Noisy Environment
â€¢ Human error level: 4â€“6% WER (word error rate).
â€¢ Best neural nets are reportedly there, too.
â€¢ Our test of 90-second speeches of high-school students:
â€¢ Average WER: 40â€“50% KIT, 80â€“90% Google, UEDIN.
The best recognized segment:
Manual Google UEDIN KIT
why do you wear
those high heels ,
if you would wear
some sneakers ?
I know one really
good store , that
deals with the
sale of freetime
why do âˆ… where
does high heels if
you would wear
some sneakers I
no one really good
star that deals
with the sale of
freedown food to
â€™re ready where
tells us if you
would âˆ… sneaker
us I know won the
really good store
that deals with the
sale of freedown
food
why do are those
highs heels if you
would where some
sneakers i no one
really good story
that deals with the
sale of freetime
food to our 20/53
Spoken Language Translation
Realistic MT Quality
General Translation Errors, Domain Issues
ASR But it is much more difficult to ask if you do not have any clue.
MTde Aber es ist viel schwieringer zu fragen, ob Sie keine Vorstellung
davon haben.
MTcs Je vÅ¡ak mnohem tÄ›Å¾Å¡Ã­ ptÃ¡t se, zda nemÃ¡te ponÄ›tÃ­.
â€¢ â€œifâ€ should be translated as â€œwannâ€/â€œkdyÅ¾â€ in this context.
ASR You can be reported after some profanities.
MTcs MÅ¯Å¾ete bÃ½t hlÃ¡Å¡eni o nÄ›kterÃ½ch profesnÃ­ch vÄ›cech.
Gloss You can be reported due to some professional things.
21/53
ASR Errors Multiplied in MT
â€¢ Errors in ASR are mostly similar words.
â€¢ Reasonably easy for the user to recover from transcript errors.
â€¢ MT takes these wrong words as fully trustworthy.
â€¢ MT happily reorders the sentence to sound best, including wrong words.
â€¢ No information about ASR and MT confidence available!
ASR And the goal of my thesis is to fold.
MTcs A cÃ­lem mÃ© teorie je rozdrobit se.
Gloss And the goal of my theory is to fall apart.
Ref A cÃ­le mÃ¡ moje teze dva.
Gloss And there are two goals of my thesis.
22/53
Spoken Language Translation
ASR + MT Integration
ASR + MT Integration
â€¢ ASR emits string of lowercase words.
â€¢ MT expects individual correct sentences.
Options to bridge the gap:
1. Insert punctuation into ASR output â‡’ new step: Segmentation.
2. Change ASR to predict directly correct punctuation.
3. Fully end-to-end SLT.
23/53
Approaches to Segmentation
â€¢ Language-Model-based: LM score without and with punctuation:
P(some sneakers I know) â‰· P(some sneakers, I know) â‰·
â‰· P(some sneakers. I know) â‰· P(some sneakers? I know)
â€¢ Sequence-labelling:
â€¢ Label each word with punctuation that should follow it.
â€¢ Many techniques possible: HMM, CRF, LSTM, â€¦
â€¢ Machine-translation:
â€¢ Input: Text without punctuation.
â€¢ Output: Text with punctuation.
â€¢ Approaches: PBMT, NMT.
A critical decision whether to allow access to the sound:
â€¢ Delays, prosody, intonation are very informative. 24/53
Errors in Segmentation
â€¢ Errors in precision lead to confusing MT output:
Speaker â€¦all too wellâ€¦
ASR+Segm â€¦this approach does not generalize all too. Well,
so to somehow concludes that the whole talk.
â€¢ Errors in recall make too much content unstable, see below.
25/53
Spoken Language Translation
End-to-End SLT
Motivation for End-to-End SLT
Benefits:
â€¢ Uncertainty directly handled.
â€¢ Target-language considerations influence speech recognition.
â€¢ Potentially fewer NN parameters.
Drawbacks:
â€¢ Insufficient training data.
â€¢ Speech + transcript and parallel texts much more common than
speech + translation.
â€¢ 20â€“40x longer input sequences (sound timeframes vs. subwords).
â€¢ Difficult alignment problem within sentences/utterances.
â€¢ Non-golden utterance segmentation not yet much considered.
26/53
SLT Training TechniquesDecoder
Speech
Encoder
Standard
Decoder
Text
Encoder
Decoder
Decoder
Speech
Encoder
Target Source
Decoder
Speech
Encoder
Pretraining
Decoder
Speech
Encoder
Knowledge
distillation
Decoder
Speech
Encoder
Decoder
Text
Encoder
ASR pretrainingMT pretraining
TEACHER STUDENT
Multi-task
Learning
27/53
Proof-of-Concept End-to-End SLT (Berard et al., 2016)
â€¢ Synthetic French speech into English text (7 concatenative voices).
â€¢ MFCCs â†’ deep LSTM encoder â†’ attn â†’ deep LSTM decoder.(a) Machine translation alignment (b) Speech translation alignment
â€¢ End-to-end results not far from ASR+MT given synthetic input. 28/53
First Truly End-to-End SLT
BÃ©rard et al. (2018) presents the first truly end-to-end SLT:
â€¢ Speech encoder:
â€¢ 2 layers converting ğ‘›-dim input into ğ‘›â€²-dim.
â€¢ 2 layers of convolution
â€¢ 3-layer bidirectional LSTM
â€¢ Attention
â€¢ Char-level decoder
â€¢ Used either to predict English transcription (|ğ‘‰ | = 46),
â€¢ or French translation (|ğ‘‰ | = 167)
29/53
BÃ©rard et al. (2018) Pre-Training20000 40000 60000 80000
steps
30
40
50
60
70
WER (ASR)
14
15
16
17
18
19
20
21
BLEU (MT)
ASR mono
ASR multi
MT mono
MT multi
30/53
BÃ©rard et al. (2018) Resultsgreedy beam ensemble params
Test BLEU (million)
Cascaded 14.6 14.6 15.8 6.3 + 15.9
End-to-End 12.3 12.9
15.5â€  9.4Pre-trained 12.6 13.3
Multi-task 12.6 13.4
Table 4: AST results on Augmented LibriSpeech test. â€  com-
bines the end-to-end, pre-trained and multi-task models.
31/53
Recent End-to-End SLT Results (Sulubacak et al., 2019)Table 4: BLEU scores for SLT methods on Englishâ†’ French Augmented LibriSpeech/test.
All systems are end-to-end, except for the pipeline system marked with a dagger ( â€ ).
Approach BLEU â†‘ Training data Description
SLT (h) ASR (h) MT (sent)
BÂ´erard et al (2018) 13.4 100h CNN+LSTM. Multi-task.
Di Gangi et al (2019b) 13.8 236h CNN+Transformer.
Bahar et al (2019) 17.0 100h 130h 95k Pyramidal LSTM. Pretraining, augmentation.
Liu et al (2019) 17.0 100h Transformer. Knowledge distillation.
Inaguma et al (2019a) 17.3 472h CNN+LSTM. Multilingual.
Pino et al (2019) 21.7 100h 902h 29M CNN+Transformer. Pretraining, augmentation.
Pino et al (2019) â€  21.8 100h 902h 29M End-to-end ASR. CNN+LSTM.
32/53
Transformer Adapted for Speech Input Gangi et al. (2019)
33/53
Translatotron (Jia et al., 2019)log-mel spectrogram
(Spanish)
8-layer
Stacked
BLSTM
Encoder
concat
Speaker
Encoder
speaker
reference
utterance
Attention
Attention
Multihead
Attention
2Ã— LSTM Decoder
2Ã— LSTM Decoder
Spectrogram
Decoder
Vocoder
phonemes
(Spanish)
phonemes
(English)
linear freq
spectrogram
(English)
waveform
(English)
Auxiliary recognition tasks
â€¢ Speech transcripts still needed to train (but not at inference).
â€¢ Somewhat worse that SLT+TTS.
â€¢ Allows to transfer the voice across languages.
https://google-research.github.io/lingvo-lab/translatotron/
34/53
Spoken Language Translation
Presentation
The Importance of Presentation
â€¢ Presentation issues can kill the whole show.
â€¢ Bad font size may make output impossible to follow.
â€¢ Too much flicker, jumping text, â€¦
â€¢ Recent fully NN ASR operate on a moving window of say 8 seconds.
â€¢ The output is too unstable to follow, let alone if translated by MT.
â€¢ Presentation must be tested on stage.
â€¢ Sizing, visibility, â€¦ cannot be checked remotely.
35/53
Subtitle View
36/53
Paragraph View
37/53
ASR/Segmentation Updates (Cho et al., 2012; Cho et al., 2017)ASR
SEG1
MT1 ...Pixelen auf Ihrem Bildschirm. Zu jedem Zeitpunkt. Es ist auch eine sehr flexible Architektur...
pixels on your screen at any given moment it is also very flexible architecture of this is an entire book
ASR update which arrives laterunstable ASR outputstable ASR output
pixels on your screen. At any given moment. It is also very flexible architecture of...
unstable segmenter outputstable segmenter output
completed expected incoming
pixels on your screen. At any given moment. It is also very flexible. Architecture of this is an entire book.
unstable segmenter outputstable segmenter output
completed completed expected expected
SEG2
MT2 ...Pixelen auf Ihrem Bildschirm. Zu jedem Zeitpunkt. Sie ist auch sehr flexibel. Die architektur ist ein ganzes Buch.
should reset happen, this would be the new beginning
38/53
Cognitive Load, Overall Usability
â€¢ Users confirm that transcript and slides must be on the same
screen.
â€¢ Adding slide streaming/sharing to both Subtitle and Paragraph view.
â€¢ Overall usability:
â€¢ Often still bad, due to the cummulation of errors.
â€¢ Two foreign colleagues reported they could follow a Czech talk,
if fully focussed on the text.
â€¢ Desired settings differ from user to user:
â€¢ Those who understand source language will need
simultaneity over precision and stability.
â€¢ Those who cannot understand source need
stability and precision and are happy to wait for seconds.
39/53
Spoken Language Translation
Evaluation
Evaluating Spoken Language Translation
Three aspects o simultaneous (â€˜on-lineâ€™) SLT:
â€¢ Quality of the final translation.
â€¢ â€¦ equals standard MT quality estimates.
â€¢ Lag behind the source.
â€¢ Some lag is inevitable, e.g. waiting for the German verb.
â€¢ Flicker
â€¢ How many words are corrected?
40/53
Mismatch in SegmentingWisst ihr es? Es ist eine Katze, ist es das nicht? Ja, so ist es.
Isn't it? yes is it like that.
German Ref
English ASR
...
......
...
Do you know it's a cat,
â€¢ Consider Englishâ†’German SLT.
â€¢ No matter what the MT does with the recognized English,
segments wonâ€™t match.
41/53
Mismatch in SegmentingWisst ihr es? Es ist eine Katze, ist es das nicht? Ja, so ist es.
Isn't it? yes is it like that.
German Ref
English ASR
...
......
...
Do you know it's a cat,
Planned strategy:
â€¢ Follow reference segmentation.
â€¢ Find best matching hypothesis segmentation.
â€¢ a) Expand by full segments.
â€¢ b) Expand by a few words around the best-matching segment.
Or ignore the problem by force-segmenting into âˆ¼30s chunks.
41/53
Visual Information in MT
Motivation for Multi-Modal Translation (1/2)
Input A tennis player is getting ready.
Output A Tenista se pÅ™ipravuje.
42/53
Motivation for Multi-Modal Translation (1/2)
Input A tennis player is getting ready.
Output A Tenista se pÅ™ipravuje. â† male
Output B Tenistka se pÅ™ipravuje. â† female
42/53
Motivation for Multi-Modal Translation (1/2)
Input A tennis player is getting ready.
Output A Tenista se pÅ™ipravuje. â† male
Output B Tenistka se pÅ™ipravuje. â† female
42/53
Motivation for Multi-Modal Translation (2/2)
Hindi Visual Genome (Parida et al., 2019) provides 30k picture
descriptions from visualgenome.org, translated into Hindi.
1: Two lambs lying in the sun.
Hindi MT: à¤¦à¥‹ à¤­à¥‡à¥œ à¤•à¥‡ à¤¬Ä‹à¤šà¥‡ à¤¸à¥‚à¤°à¤œ à¤®È… à¤à¥‚à¤  à¤¬à¥‹à¤² à¤°à¤¹à¥‡ à¤¹Èˆ
Gloss: Two baby sheep are telling lies â€¦
Selected surrounding captions:
2. Sheep standing in the grass
3. Sheep with black face and legs
4. Sheep eating grass
5. Lamb sitting in grass.
43/53
Hindi Visual Genome Challenge Test Set
â€¢ A test set created by scanning the 3.15M unique strings for
ambiguous words.
â€¢ Only 19 words with multiple (automatic) translations were
identified:
Word Segment Count Word Segment Count
1 Stand 180 11 English 42
2 Court 179 12 Fair 41
3 Players 137 13 Fine 45
4 Cross 137 14 Press 35
5 Second 117 15 Forms 44
6 Block 116 16 Springs 30
7 Fast 73 17 Models 25
8 Date 56 18 Forces 9
9 Characters 70 19 Penalty 4
10 Stamp 60 Total 1400
44/53
Example from the â€œChallenge Test Setâ€
Street sign advising of penalty. The penalty box is white lined.
45/53
Attention to Source Words (?)
46/53
Attention to Source Image
47/53
Hierarchical Attention (LibovickÃ½ and Helcl, 2017)Source: a man sleeping in a green room on a
couch .
Reference: ein Mann schlÂ¨aft in einem grÂ¨unen
Raum auf einem Sofa .
Output with attention:ein
Mann
schlÃ¤ft
auf
einem
grÃ¼nen
Sofa
in
einem
grÃ¼nen
Raum
.
(1)
(2)
(3)
(1) source, (2) image, (3) sentinel
48/53
Recent Multi-Modal MT Results (Sulubacak et al., 2019)B L E U â†‘ M E T E O R â†‘ Ty p e D e scr ip tio n A r ch .
Elliott et al (2015) â€  9.7 (N/A) 24.7 (N/A) E,D Conditional LMs RNN
Caglayan et al (2016a) â€  29.3 (â†“4.6) 48.5 (â†“4.3) A Shared Attention RNN
Calixto et al (2016) â€  28.8 (N/A) 49.6 (N/A) A Separate Attention RNN
Huang et al (2016) â€  36.8 (â†‘2.0) 54.4 (â†‘2.3) IF Parallel RCNN-LSTMs RNN
Hitschler et al (2016) â€  34.3 (N/A) 56.0 (N/A) R Retrieval + Reranking SMT
Toyama et al (2016) 36.5 (â†‘1.6) 56.0 (â†‘0.7) L Variational RNN
Shah et al (2016) â€  34.8 (â†‘0.2) 56.7 (â†‘0.1) R Visual Reranking SMT
Caglayan et al (2016a) â€  36.2 (â€“ 0.0) 57.5 (â†‘0.1) R Visual Reranking SMT
Helcl and LibovickÂ´y (2017) 31.9 (â†“2.7) 49.4 (â†“2.3) A Hierarchical Attention RNN
Calixto and Liu (2017) 36.9 (â†‘3.2) 54.3 (â†‘2.0) I Input Prepend & Append RNN
Calixto et al (2017) 36.5 (â†‘2.8) 55.0 (â†‘2.7) A Gated Attention RNN
Calixto and Liu (2017) 37.3 (â†‘3.6) 55.1 (â†‘2.8) D Decoder Init. RNN
Elliott and KÂ´adÂ´ar (2017) 36.8 (â†‘1.3) 55.8 (â†‘1.8) T Imagination RNN
Caglayan et al (2017a) 38.2 (â†‘0.1) 57.6 (â†‘0.3) E,D Encoder Decoder Init. RNN
37.8 (â†“0.3) 57.7 (â†‘0.4) O Multiplicative Interaction RNN
Delbrouck and Dupont (2017b) 40.5 (N/A) 57.9 (N/A) A Encoder Attention + CBN RNN
Arslan et al (2018) 41.0 (â†‘2.4) 53.5 (â†“1.5) A Parallel Attention Transformer
Calixto et al (2018) 37.6 (â†‘2.6) 56.0 (â†‘1.1) L Variational RNN
Helcl